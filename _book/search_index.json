[["index.html", "Introduction to quantitative methods for economists About this book", " Introduction to quantitative methods for economists Alessandro Bramucci 09 Sep 2022 About this book This book brings together a collection of notes, exercises and practical applications that I have used over the years for my lectures in Quantitative Methods for Economists at the Department of Business and Economics of the Berlin School of Economics and Law (HWR) in Berlin, Germany. The course offers an introduction to basic econometrics to Master students in two study programs at the Institute for International Political Economy (IPE), International Economics and Political Economy of European Integration. This book is an attempt to systematize the contents discussed in class but also to make the materials freely available. The text is designed for all those who are approaching these topics for the first time (or after a long break) and in particular for those do not have a strong quantitative background. My hope is however that even advanced students may find the text interesting and instructive. Before getting into the heart of the presentation of the course contents (i.e.Â econometrics and the linear regression technique), the book takes a rather long tour that starts with an introduction to the statistical programming language R to then move on to the review of fundamental mathematical and statistical concepts. My personal impression from the classes taught in the past is that a good review of statistics and mathematics is always welcome by students. The text is divided into three parts. Part I, called Introduction to R, introduces the basics of the R language in particular for manipulating and cleaning data, creating graphs and writing simple functions. The examples proposed aim to be of interest to economics students. Part II, named Review of math and statistics, covers basic mathematical and statistical concepts, like basic algebra (with some brief mentions of linear algebra), calculus and simple optimization problems, descriptive and inferential statistics. Where possible, R is used to discuss examples and solve exercises. Part III, The linear regression, gets into the heart of the discussion presenting the least squares technique and regression analysis. Also here examples will be primarily based on economic data and models. Once again R will be our main working tool. This book is developed in my spare time. If you have advice or see mistakes please feel free to contact me via email bramucci[AT]hwr-berlin[DOT]de or to drop me a message on Twitter @AleBramucci. I will be happy to hear from you ! For some information regarding my projects, professional activities and recent publications you may also want to visit my personal webpage www.alessandrobramucci.com. Have fun reading the book! Few words of thanks I want to thank all the colleagues with whom I have worked during my years at the HWR Berlin and in particular my colleagues at the IPE institute. A special thanks goes to all my previous students in the course Quantitative Methods for Economists. Thanks to them (and to their patience) I was able to improve the materials of the course and correct imprecisions. Of course, any remaining errors are my sole responsibility. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["chapter1.html", "Chapter 1 Language essentials 1.1 Why R? 1.2 R and RStudio 1.3 Basic operations 1.4 Basic data structures 1.5 if else statement 1.6 For loops 1.7 Nested loops 1.8 Simple functions", " Chapter 1 Language essentials 1.1 Why R? R is an open-source statistical programming language used worldwide for statistical analysis and data science in a vast variety of fields. The R project was released to the public in 2000 and since then The R Foundation and the R Development Core Team take care of development of the language and of the release of new versions.1 Unlike many other computer programs for statistical and data analysis, in R you dont perform your operations by clicking. In R you work with a script, basically a text where the user write functions to perform the desired operations, such as computing averages, transforming data and so on. Although at first this seems very difficult especially for those who do not have experience with code based work, writing your own operations as if it were normal text has many advantages. This allows you not only to think carefully about the operations you are performing (rather than just clicking left and right) but also to ensure the reproducibility of your work as academic and scientific work demand. Another very good reason for using R is the large availability of R packages. R packages are collections of functions, data and other code that extend the capabilities of base R. Nowadays there are many R packages available to perform a wide variety of tasks, from data visualization, simple and sophisticated econometric analysis, web application development and much more. Not least, Rs ability to visualize data with publication quality graphs and to write data-driven texts make it (in my opinion) an ideal choice for economists. This does not mean that R is better than other programming languages widely used in statistical analysis like Python or Julia. From what I know R is very popular among economists and for our introductory quantitative methods course R is undoubtedly the right tool to learn. 1.2 R and RStudio Nowadays people do not work directly in R but use what is called an Integrated Development Environment (IDE, for short). An IDE is a software that allows to work with the programming language using a user-friendly and nice interface together with several additional features that make the coding faster and more enjoyable. RStudio is by far (again, in my opinion) the best IDE for R.2 It is simple to use and very intuitive. The RStudio Team has also developed a number of extremely amazing and useful packages like rmarkdown and shiny.3 This book for example is written entirely in RStudio using the powerful package bookdown. We are now going to describe very briefly the interface of RStudio. As you can see from Figure 1.1, RStudio is composed of 4 panes.4 Figure 1.1: The four panes in RStudio. In pane 1 we can see the R script (but it could also be another file, like an R Markdown file or a Python script). This is were we will type our code. From the script we can then execute the code. The output of our code will be printed in the console. In Figure 1.1 the console is shown in pane 2. You can perform operations directly from the console. The result will be displayed immediately in the console itself. This can be useful on certain occasions, such as when you need to calculate something on fly that does not necessarily has to be included in your main work. In all other occasions it is recommended to write the code in the script.5 In pane 3 we can see different tabs. In the environment tab is where you will see variables, functions and databases saved during the working session. In the history tab, you will see the list of all operations performed during the working session and from there you can execute your code again. Your code is automatically stored by RStudio and saved in a separate file (although this function can be also deactivated). We omit talking about the two remaining tabs. In pane 4 we can observe different tabs again. The files tab shows basically the files contained in your computer directories, like for example your project directory if you are working with an RStudio project.6 From there you can create new files, rename files, delete files and so on. In the plot pane you will see your graphs printed (like scatter plots, bar charts, etc.). In the help pane as you can easily guess you can get some very useful information about R and RStudio and the R language in general and in the Viewer tab more complex things are visualized, like plotly graphs.7 How to execute code? 1.3 Basic operations We now start by looking at some very simple operations that can be performed in R. To begin with, R can be used to solve arithmetic operations just like a normal calculator. 2 + 2 ## [1] 4 R includes standard mathematical functions such as the square root function, the exponential function and the logarithmic function. sqrt(25) ## [1] 5 exp(0) ## [1] 1 log(1) ## [1] 0 With a left pointing arrow (a small than sign followed by a minus sign) &lt;- we can assign a value to a variable. Here, for example, we assign the value 15 to the variable x. In this case, the variable x is saved in the environment (pane 4 in Figure 1.1. x &lt;- 15 To remove any variable from the environment, we can use the function rm() with the name of the variable that we wish to delete. Here, we delete the variable x that we have created above. rm(x) With the function c() we can assign a series of value to a variable. The letter c stands for something like concatenate. x &lt;- c(1, 2, 3) With the function ls, we can print in the console the list of the variables that we have so far created. ls() ## [1] &quot;x&quot; It is a good practice to start a new working session in R with an empty environment. This short piece of code remove all variables saved in the environment so far (not many in our case!). You could put something like this at the beginning of your R script so that each time you run the script previously created variables and functions are automatically deleted. Of course, it is up to you to figure out when it is appropriate to use such a function and when not. rm(list = ls()) R can also easily creates sequences of numbers. There are two ways. The first is by using the colon operator :. In the following example, we are creating a series of integers from 1 to 10. a &lt;- 1:10 The second way is by using the seq function. With the seq function you can specify the beginning (from), the end (to) and the increment (by) of the sequence of numbers.8 b &lt;- seq(from = 1, to = 10, by = 1) We can also generate random numbers from a normal distribution specifying mean and standard deviation (more on the normal distribution in Part II). The set.seed function allows us to generate the same numbers. This means that if you and I have the same seed, we will generate the same set of random numbers.9 set.seed(1234) rnorm(n = 10, mean = 0, sd = 1) ## [1] -1.2070657 0.2774292 1.0844412 -2.3456977 0.4291247 0.5060559 ## [7] -0.5747400 -0.5466319 -0.5644520 -0.8900378 A very important function in R is the length function. The length function comes in handy in a number of situations. Essentially, it tells us the number of elements in a vector (but also other types of objects). We can now use the length function with a sequence of numbers from 1 to 10 increasing by 0.1 generated using the seq function. How long is this vector? length(seq(from = 1, to = 10, by = 0.1)) ## [1] 91 R also includes all the fundamental types of operators like arithmetic, relational and logical operators. Arithmetic operators are what we remember from school, like + - * /. Power elevation is done with the ^ sign.10 5^2 ## [1] 25 Relational operators are used to compare values. They are kind of making a question to R where the response will be a true or false judgement. 100 &gt;= 100 ## [1] TRUE 99 &gt; 100 ## [1] FALSE With the double equal operator == we are asking R if two expressions are equal to each other.11 2 + 2 == 4 ## [1] TRUE The operator ! is the negation operator in R. It can be read as NOT. With the following expression we are asking R if 99 is NOT equal to 100. 99 != 100 ## [1] TRUE Here, we are asking if 1 is NOT equal to 1. !(1==1) ## [1] FALSE In R we also find the classical logical operators, like &amp; and |. The operator | asks if at least one of the two expressions is true. The operator | can be read as OR. The following expression asks if 1 is equal to 1 OR 2 is equal to 3. It will return true if at least one of the two expressions is correct. (1==1) | (2==3) ## [1] TRUE The operator &amp; (read AND) returns true when both expressions are true. The following expression asks if 1 is equal to 1 AND 2 is equal to 3. (1==1) &amp; (2==3) ## [1] FALSE Before concluding this section we want to briefly discuss what in R is called the recycling rule. This could help avoiding some bad surprises when working in R. What happens when we add two vectors together? If the two vectors have the same length, R will simply perform an the element-by-element sum. If the two vectors are a multiple of each other, the elements of the shorter vector will be recycled to match the length of the longer vector. a &lt;- c(1, 2) b &lt;- c(1, 2, 3, 4, 5, 6) a + b ## [1] 2 4 4 6 6 8 As you can see from the output above, R does not give us any warning messages about the recycling rule. On the contrary, when we sum two vectors where the shorter vector is not a multiple of the longer vector, R will give us a warning message but the recycling rule will be applied anyway. a &lt;- c(1, 2) b &lt;- c(1, 2, 3, 4, 5) a + b ## [1] 2 4 4 6 6 1.4 Basic data structures We now move to discuss the basic data structures in R. Vector, Matrix, Dataframe, List 1.5 if else statement The statistical language R includes the traditional if else statement. Let us make a simple example.12 The object x takes the value 10. The object y takes the value 2. If x is smaller or equal y, R prints the sentence \"x is smaller or equal y\" in the console, otherwise it prints \"x is larger than y\". Pretty straightforward. x &lt;- 10 y &lt;- 2 if (x &lt;= y) { print(&quot;x is smaller or equal y&quot;) } else { print(&quot;x is larger than y&quot;) } ## [1] &quot;x is larger than y&quot; An alternative version of the if else statement is the ifelse() function. It essentially does the same thing but in a vectorised fashion. Here, we apply the ifelse() function to a vector of random numbers created with rnorm. Then, we ask if each element in the vector x is smaller or equal to zero or larger than zero. The vector with the corresponding results will be printed in the console.13 x &lt;- rnorm(10, 0, 1) ifelse(x &lt;= 0, &quot;x is smaller or equal zero&quot;, &quot;x is larger than zero&quot;) ## [1] &quot;x is smaller or equal zero&quot; &quot;x is smaller or equal zero&quot; ## [3] &quot;x is smaller or equal zero&quot; &quot;x is larger than zero&quot; ## [5] &quot;x is larger than zero&quot; &quot;x is smaller or equal zero&quot; ## [7] &quot;x is smaller or equal zero&quot; &quot;x is smaller or equal zero&quot; ## [9] &quot;x is smaller or equal zero&quot; &quot;x is larger than zero&quot; 1.6 For loops To make it as short as possible, a loop is a piece of code that repeat operations over and over again, just as the name suggests. A for loop is a piece of code that tells the computer to perform a certain operation for every value or element in a variable (Grolemund 2014).14 An example is very much needed right now. x &lt;- 1:5 for (i in x) { print(&quot;Hello&quot;) } ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; What happened? The for loop has printed the word \"Hello\" five times. First of all, we have defined the variable x which is a sequence of integer numbers between 1 and 5. Then we tell R to print the word \"Hello\" for every i (i.e.Â every element) in x which is just a sequence of numbers of five elements. The same operations is then repeated five times. We could have done the same thing by looping over a character vector with five elements, like for example the first five days of the week. The results would have been the same. x &lt;- c(&quot;Monday&quot;, &quot;Tueasday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;) for (i in x) { print(&quot;Hello&quot;) } ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; R will automatically assign the numbers or character values to the index i in each run of the loop. This is very useful. The index i can then be used as a normal variable within the body of the loop to perform any type of operations, as math calculations and vector indexing. Here we raise every i from 1 to 5 to the second power. x &lt;- 1:5 for (i in x) { print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 In the next example, we loop over the character vector containing the names of the first five days of the week. We now want R to combine the sentence Today is with every day of the week stored in the vector. We use the function paste() to combine our sentence Today is (which is the same all the time) with each single value of the index i. The index i assumes in turn each single element of the character vector x. x &lt;- c(&quot;Monday&quot;, &quot;Tueasday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;) for (i in x) { print(paste(&quot;Today is&quot;, i)) } ## [1] &quot;Today is Monday&quot; ## [1] &quot;Today is Tueasday&quot; ## [1] &quot;Today is Wednesday&quot; ## [1] &quot;Today is Thursday&quot; ## [1] &quot;Today is Friday&quot; So far the output of the loop is printed in the console. It is much more useful instead if the results were saved for later use in some sort of database or placeholder. Let us use an example from the financial world. compounding interest preallocating space data_interests &lt;- numeric(100) interests &lt;- 0.01 data_interests[1] &lt;- 1 for (t in 2:length(data_interests)) { data_interests[t] &lt;- data_interests[t-1] * (1 + interests) } plot(x = 1:100, y = data_interests, xlim = c(1, length(data_interests)), ylim = c(1, max(data_interests)), type = &quot;l&quot;, col = &quot;black&quot;, main = &quot;Investments&quot;, xlab = &quot;Year&quot;, ylab = &quot;Euro&quot;) 1.7 Nested loops We can nest loops within other loops. Here we expand the previous example creating some alternative scenarios with different interest rates. The first for loop (index t) deals with repetitions over time, just as in the previous example. The second for loop, on the other hand, allows us to simulate different scenarios with different assumptions about the interest rate. As you can see, we have created a vector interest with three different rates of interest (1%, 2% and 3%). This time we will save our data in a matrix. Why? Because matrices have tow dimensions, as we have seen above. Our placeholder matrix is called data_interests. data_interests &lt;- matrix(nrow = 100, ncol = 3, byrow = TRUE) interests &lt;- c(0.01, 0.02, 0.03) data_interests[1,1] &lt;- 1 data_interests[1,2] &lt;- 1 data_interests[1,3] &lt;- 1 for (t in 2:nrow(data_interests)) { for (s in 1:ncol(data_interests)) { data_interests[t,s] &lt;- data_interests[t-1,s] * (1 + interests[s]) } } To make the code more elegant and concise, we can use a simple loop to include the three scenarios in the plot. First, we must open a plot without the inputs for the variable x and y. As you can see, we have included the word NULL instead of x and y.15 We then code the rest of the plot as we did above. What would be the alternative? The alternative would be to repeat the lines function three times, each time using the desired scenario as input, e.g.Â data_interests[,1] and so on (or to include the first scenario directly in the plot and then use two times the lines function). Note that the color will also depend on the index of the loop (l) and therefore on the scenario as we want a different color for each scenario (see col = l). In this case, the colors will picked by following the default order of colors palette in the plot() function.16 plot(NULL, xlim = c(1, nrow(data_interests)), ylim = c(1, max(data_interests)), main = &quot;Investments&quot;, xlab = &quot;Year&quot;, ylab = &quot;Euro&quot;) for (l in 1:ncol(data_interests)) { lines(data_interests[,l], type = &quot;l&quot;, col = l) } 1.8 Simple functions Small but important digression. The word function is a reserved word. A reserved word is a word that you cannot use to give names to your objects. You may find a list of reserved words in the R Documentation. Take a look here if you want.17 my_fun &lt;- function(x, y) x^y my_fun(x = 5, y = 2) ## [1] 25 Putting everything together Let us now try to write a function that put together all the things we did in the previous paragraphs, namely creating the placeholders, running the loop and generating a plot. my_fun &lt;- function(time, scenario1, scenario2, scenario3) { data_interests &lt;- matrix(nrow = time, ncol = 3, byrow = TRUE) # Here we put the inputs of our function interests &lt;- c(scenario1, scenario2, scenario3) data_interests[1,1] &lt;- 1 data_interests[1,2] &lt;- 1 data_interests[1,3] &lt;- 1 for (t in 2:nrow(data_interests)) { for (s in 1:ncol(data_interests)) { data_interests[t,s] &lt;- data_interests[t-1,s] * (1 + interests[s]) } } plot(NULL, xlim = c(1, nrow(data_interests)), ylim = c(1, max(data_interests)), main = &quot;Investments&quot;, xlab = &quot;Year&quot;, ylab = &quot;Euro&quot;) for (l in 1:ncol(data_interests)) { lines(data_interests[,l], type = &quot;l&quot;, col = l) } } my_fun(time = 5, scenario1 = 0.01, scenario2 = 0.02, scenario3 = 0.03) Pretty cool, isnt it? References "],["chapter2.html", "Chapter 2 Data extraction and manipulation 2.1 Conditional selection", " Chapter 2 Data extraction and manipulation In this chapter, we want to learn how to use R, and in particular the functions of what is also called base R to work with data18. To practice we will use a dataset contained in the package wooldridge. This R package contains all the databases used in the famous econometrics textbook Introductory Econometrics: A Modern Approach by Jeffrey M. Wooldridge (2013). All information on the datasets available in the package, such as variable names, data sources, etc., can be found in the related R package documentation. We start by installing the desired package. install.packages(&quot;wooldridge&quot;) After having installed the package, we must load it into the library. library(wooldridge) To unload the package from the library, we can use the following code. We dont need this now. We want to continue using the package, of course! detach(&quot;package:wooldridge&quot;, unload = TRUE) In RStudio this procedure is even easier. To install a package, simply select the desired package from the drop-down menu, which is accessed from the Packages tab in the panel that is (usually) located in the bottom right-hand corner. This (very simple) procedure is summarised in figure 2.1. knitr::include_graphics(&quot;images/image1.png&quot;) Figure 2.1: How to install a package in RStudio. Once the package is installed, we have to load it into the library. In this case, we simply check the box of the desired package just as shown in figure 2.2. knitr::include_graphics(&quot;images/image2.png&quot;) Figure 2.2: Load the package to the library from RStudio. R is different from Excel in several respects. One in particular is clear. With Excel, one works mainly by looking at the data. In R, on the other hand, one works mainly by looking at the codes. However, it is necessary to look at the data from time to time, for example after having modified existing variables or having created new ones. The dataset we use in this exercise is called wage1. We can take a glance at the data using the View function. View(wage1) Let us start by seeing how many columns and rows our dataset contains. dim(wage1) ## [1] 526 24 Indexing when you need to address a particular element into a vector, for example the wage of worker nr. 5 wage1$educ[5] ## [1] 12 If we want data for more than one worker #wage1$educ[c(2,3,5)] Data from worker one to worker five #wage1$educ[1:5] If we want to modify one particular observation #wage1$educ[5] &lt;- NA Negative indexing allow to show all data except those specified in parenthesis #wage1$educ[-c(1:520)] Negative indexing allow us to drop specified rows and columns. Drop the first row in the dataset #wage1[-1,] Sometimes we may have to delete some variables from our dataset. To delete a column, we must use the negative sign in front of the variable that we wish to delete. In this case, we are eliminating the first column, #wage1[,-1] We can also delete columns using variable names. In this case, we delete the variable containing the years of education (which is the first variable in the dataset). #wage1[2] Converting dollars to euros and add the new column variable to the dataset. wage1$wage_EUR &lt;- wage1$wage * 0.86 We can compare the variable wage, originally expressed in dollars, and the new variable wage_EUR. Here we take a look at the first five rows. wage1[1:5, c(&quot;wage&quot;, &quot;wage_EUR&quot;)] ## wage wage_EUR ## 1 3.10 2.6660 ## 2 3.24 2.7864 ## 3 3.00 2.5800 ## 4 6.00 5.1600 ## 5 5.30 4.5580 2.1 Conditional selection To use when you need to extract some data that satisfy certain criteria Workers that have more than 15 years of education Workers that have between 15 AND 18 year of education Workers that have between 15 OR 18 year of education A more interesting research question is to ask the hourly wage of workers that have 15 or more years of education Hourly wage of workers with more than 15 years of education and 5 years of experience Look at what happens when we type educ&gt;15. We can exploit the fact that TRUE=1 and FALSE=0 to calculate the percentage of workers that have more than 15 years of education wage1$educ[wage1$educ &gt; 17] wage1$educ[wage1$educ &gt;= 15 &amp; wage1$educ &lt;= 18] wage1$educ[wage1$educ &gt;= 17 | wage1$educ &lt;= 2] wage1$wage[wage1$educ &gt;= 15] wage1$wage[wage1$educ &gt;= 15 &amp; wage1$exper &gt;= 5] sum(wage1$educ &gt; 15) / sum(wage1$educ &gt;= 0) * 100 References "],["chapter3.html", "Chapter 3 Code 3", " Chapter 3 Code 3 dplyr ggplot2 growth contributions "],["chapter4.html", "Chapter 4 Basic math elements 4.1 Derivative rules 4.2 Matrix operations", " Chapter 4 Basic math elements 4.1 Derivative rules The rules for sums and differences Given $ f(x) = g(x) h(x) $, where \\(g(x)\\) and \\(h(x)\\) are both differentiable functions, the derivative of a sum or difference of two functions is given by: \\[\\begin{equation} f&#39;(x) = g&#39;(x) \\pm h&#39;(x) \\end{equation}\\] The product rule Given \\(f(x) = g(x) \\cdot h(x)\\), where \\(g(x)\\) and \\(h(x)\\) are both differentiable functions, the derivative is given by: \\[\\begin{equation} f&#39;(x) = h(x) \\cdot g&#39;(x) + h&#39;(x) \\cdot g(x) \\end{equation}\\] The quotient rule Given \\(f(x) = \\frac{g(x)}{h(x)}\\), where \\(g(x)\\) and \\(h(x)\\) are both differentiable functions and \\(h(x)\\neq0\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = \\dfrac{h(x) \\cdot g&#39;(x) - h&#39;(x) \\cdot g(x)}{[h(x)]^2} \\end{equation}\\] The generalized power function rule Given \\(f(x) = [g(x)]^n\\), where \\(g(x)\\) is a differentiable functions and n is any real number, the derivative is given by: \\[\\begin{equation} f&#39;(x) = n[g(x)]^{n-1} \\cdot g&#39;(x) \\end{equation}\\] The chain rule Given \\(f(x) = h(g(x))\\), where \\(f\\) is a function of a function where \\(h\\) is in turn function of , the derivative is given by: \\[\\begin{equation} f&#39;(x) = h&#39;(g(x)) \\cdot g&#39;(x) \\end{equation}\\] Additional rules Given \\(f(x) = e^x\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = e^x \\end{equation}\\] Given \\(f(x) = ln(x)\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = \\frac{1}{x} \\end{equation}\\] Given \\(f(x) = sin(x)\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = cos(x) \\end{equation}\\] Given \\(f(x) = cos(x)\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = -sin(x) \\end{equation}\\] 4.2 Matrix operations R can perform standard matrix algebra operations. We can use matrix algebra functions in R to solve our problem from class. \\[ 7x + 5y - 3z = 16 \\\\ 3x -5y +2z = -8 \\\\ 5x +3y -7z = 0 \\] First, we rewrite the system using matrix and vector notation: \\[ \\mathbf{A} = \\left[\\begin{array} {rrr} 7 &amp; 5 &amp; -3 \\\\ 3 &amp; -5 &amp; 2 \\\\ 5 &amp; 3 &amp; -7 \\\\ \\end{array}\\right] \\mathbf{b} = \\left[\\begin{array} {rrr} x \\\\ y \\\\ z \\\\ \\end{array}\\right] \\mathbf{r} = \\left[\\begin{array} {rrr} 16 \\\\ -8 \\\\ 0 \\\\ \\end{array}\\right] \\] In order to obtain the result vector b, we have to rearrange the model performing some simple matrix algebra operations. \\[ \\mathbf{A}^{-1}\\mathbf{A}\\mathbf{b} = \\mathbf{A}^{-1}\\mathbf{r} \\\\ \\text{remember that} \\space \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I} \\\\ \\mathbf{b} = \\mathbf{A}^{-1}\\mathbf{r} \\] We are now ready to solve our system of equations using R: data &lt;- c(7, 5, -3, 3, -5, 2, 5, 3, -7) A &lt;- matrix(data, nrow = 3, ncol = 3, byrow = TRUE) r &lt;- c(16, -8, 0) b &lt;- solve(A) %*% r b ## [,1] ## [1,] 1 ## [2,] 3 ## [3,] 2 "],["chapter5.html", "Chapter 5 Fundamental concepts in statistics 5.1 Mean and median 5.2 Boxplot 5.3 Expected value 5.4 Variance 5.5 Correlation 5.6 Covariance 5.7 Expected value, variance and covariance rules", " Chapter 5 Fundamental concepts in statistics 5.1 Mean and median x &lt;- c(3,5,2,6,5,9,5,2,8,5) mean(x) ## [1] 5 median(x) ## [1] 5 ## Find the weighted arithmetic mean # Values of z z &lt;- c(70,90,85) # Weights wt &lt;- c(1,1,3) weighted.mean(z, wt) ## [1] 83 A percentile is a point in a distribution at which or below which a given proportion of data is found. The k-th percentile divides the data in a way that k-percent of the data lie below the percentile and (100 - k)-percent lie above the percentile. It is also common to hear about quantiles. Quantiles is a more generic terms which indicates values partitioning data in equally spaced groups. Specific types of quantiles are percentiles (see above), deciles, quartiles, etc. a &lt;- c(1,2,3,3,3,4,5,5,6,8,8,9,10,12,12,13,14,15,16,18) # 1) Find the values existing at the Q_1, Q_2, Q_3 and Q_4, where Q stands for quartile quantile(a, probs = c(0.25, 0.50, 0.75, 1)) ## 25% 50% 75% 100% ## 3.75 8.00 12.25 18.00 # Do we obtain the same results using the nearest rank method? # n = P / 100 * N # n &lt;- ordinal rank # P &lt;- percentile of interest # N &lt;- number of values in the set # Q_1, Q_2, Q_3 and Q_4 P &lt;- c(0.25, 0.50, 0.75, 1) * 100 N &lt;- length(a) # Ranks of the values at Q_1, Q_2, Q_3 and Q_4 n &lt;- P / 100 * N n ## [1] 5 10 15 20 # The values at the th percentile a[n] ## [1] 3 8 12 18 # 2) the percentile of value 4 and value 15 percentile &lt;- ecdf(a) percentile(4)*100 ## [1] 30 percentile(15)*100 ## [1] 90 # Let&#39;s use again the nearest rank method instead of the in-built ecdf function # P = n / N * 100 # Let&#39;s first find the rank (n) of the value 4 and 15 by looking at the original set P_new &lt;- c(6, 18) / N * 100 P_new ## [1] 30 90 5.2 Boxplot boxplot(a, horizontal = TRUE) # Clear the workspace rm(list = ls()) # Outliers in boxplots # Data a &lt;- c(1,2,3,3,3,4,5,5,6,8,8,9,10,12,12,13,14,15,16,18) b &lt;- c(1,2,3,3,3,4,5,5,6,8,8,9,10,12,12,13,14,15,16,30) # Plot the boxplot of a and b boxplot(a, b, names = c(&quot;a&quot;, &quot;b&quot;), horizontal = TRUE) # We define the boundaries for the identification of an outlier in the boxplot lower_limit &lt;- quantile(b, prob = 0.25) - 1.5 * IQR(b) lower_limit ## 25% ## -9 upper_limit &lt;- quantile(b, prob = 0.75) + 1.5 * IQR(b) upper_limit ## 75% ## 25 5.3 Expected value # Clean the workspace rm(list = ls()) # 2) x &lt;- c(6,5,6,3,1,5) # Sample mean mean(x) ## [1] 4.333333 # Expected value die &lt;- 1:6 E_die &lt;- sum(die * rep(1/6,6)) E_die ## [1] 3.5 # 3) pays &lt;- c(0, 10, 0, 40, 0, -20) E_pays &lt;- sum(pays * rep(1/6,6)) # Expected pay of the game E_pays ## [1] 5 # 4) tickets &lt;- 100000 pays_lottery &lt;- c(5, 25, 100, 10000) probabilities &lt;- c(200/tickets, 20/tickets, 5/tickets, 1/tickets) E_pays_lottery &lt;- sum(pays_lottery * probabilities) # Fair price for a ticket E_pays_lottery ## [1] 0.12 Simulation of the expected value of the fair die/dice. This exercise is taken from the book Hands-On Programming with R by Garrett Grolemund. library(ggplot2) roll_function &lt;- function(nr_of_die) { die &lt;- 1:6 roll &lt;- sample(die, size = nr_of_die, replace = TRUE, prob = rep(1/6, 6)) sum_roll &lt;- sum(roll) return(sum_roll) } throws &lt;- replicate(10000, roll_function(nr_of_die = 2)) qplot(throws, binwidth = 1, ylab = &quot;Frequency&quot;, xlab = &quot;Numbers&quot;) + geom_vline(aes(xintercept = mean(throws))) + geom_text(aes(y = 500, x = mean(throws), label = mean(throws))) + theme_minimal() 5.4 Variance Find variance and standard deviation b &lt;- c(2, 5, 3, 1, 6, 3) var(b) ## [1] 3.466667 sd(b) ## [1] 1.861899 sd(b) == sqrt(var(b)) ## [1] TRUE # 2) Add 10 to data b_plus_10 &lt;- b + 10 var(b_plus_10) ## [1] 3.466667 sd(b_plus_10) ## [1] 1.861899 # 3) Multiply data by 10 b_times_10 &lt;- b * 10 var(b_times_10) ## [1] 346.6667 sd(b_times_10) ## [1] 18.61899 5.5 Correlation With this short exercise we want to see how the Pearson correlation coefficient is a measure of linear correlation between two variables. x &lt;- runif(100, min = 0, max = 10) y &lt;- sin(x) + rnorm(100, mean = 0, sd = 0.5) plot(x,y) abline(lm(y ~ x)) cor(x, y) ## [1] -0.02519416 # The Pearson correlation coefficient is also symmetric cor(x, y) == cor(x, y) ## [1] TRUE cor.test(x, y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = -0.24949, df = 98, p-value = 0.8035 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2205210 0.1720755 ## sample estimates: ## cor ## -0.02519416 a &lt;- runif(100, min = 0, max = 10) b &lt;- a + rnorm(100, mean = 0, sd = 1) plot(a,b) abline(lm(b ~ a)) cor(a, b) ## [1] 0.9452564 cor.test(a, b) ## ## Pearson&#39;s product-moment correlation ## ## data: a and b ## t = 28.675, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9195708 0.9628976 ## sample estimates: ## cor ## 0.9452564 Programming challenge. Program a function that takes two inputs, x and y, and returns the correlation coefficient between x and y. set.seed(12345) x &lt;- rnorm(100, mean = 0, sd = 1) y &lt;- rnorm(100, mean = 0, sd = 5) my_corr_function &lt;- function(a, b) { numerator &lt;- sum((x - mean(x)) * (y - mean(y))) denominator &lt;- sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2)) correl &lt;- numerator / denominator return(correl) } my_corr_function(a = x, b = y) ## [1] 0.1042097 cor(x,y) ## [1] 0.1042097 5.6 Covariance Find covariance and Pearsons correlation coefficient between the variables \\(X\\) and \\(Y\\). X &lt;- c(2, 1, 3) Y &lt;- c(10, 30, 50) cov(X, Y) ## [1] 10 cor(X, Y) ## [1] 0.5 5.7 Expected value, variance and covariance rules Expected value The of a (discrete) random variable is the arithmetic mean of that variable where each value is weighted by its probability. It can also be thought as the long-run average for any random variable over an indefinite number of trials. For a discrete random variable X having the possible values \\(x_1, \\dots, x_N\\) the expectation of X is defined as: \\[ E(X) = x_1P(X=x_1) + \\dots + x_NP(X=x_N) = \\sum\\limits_{i=1}^{N} x_iP(X = x_i) = \\sum\\limits_{i=1}^{n} x_ip_{i}\\] The expected value of a random variable can be also understood as the population mean, \\(E(X) = \\mu_x\\) or simply \\(\\mu\\). With equals weights, the formula for the expected value will be equal to the formula for the arithmetic average. Similarly, the expected value of functions of discrete random variables is given by the following: \\[ E\\{g(X)\\} = g(x_1)p_1 + \\dots + g(x_N)p_N = \\sum\\limits_{i=1}^{N} g(x_i)p_i\\] Rule 1 The expected value of a constant, for example , is that constant \\[\\begin{align} E(b) = b \\end{align}\\] This rule can be easily understood following the rules of the summation operator. \\[\\begin{align} E(b) &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}b \\nonumber \\\\ &amp; = \\dfrac{1}{N}Nb \\nonumber \\\\ &amp; = b \\nonumber \\end{align}\\] Rule 2 If \\(X\\) is a random variable and \\(b\\) is a constant then, \\[\\begin{align} E(bX) = bE(X) \\end{align}\\] Again, using the rules of the summation operator and substituting back the population mean \\(\\mu_x\\) (remember that \\(E(X) = \\mu_x\\)): \\[\\begin{align} E(bX) &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}x_{i}b \\nonumber\\\\ &amp; = b\\dfrac{1}{N}\\sum_{i=1}^{N}x_{i} \\nonumber\\\\ &amp; = b\\mu_x \\nonumber \\\\ &amp; = bE(X) \\nonumber \\end{align}\\] Rule 3 The expected value of the sum of several variables is the sum of the expected values. If X, Y, and Z are three random variables, then, \\[\\begin{align} E(X + Y + Z) = E(X) + E(Y) + E(Z) \\end{align}\\] The rules of the sigma operator can be applied also to this case. \\[\\begin{align} E(X + Y + Z) &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}(x_{i} + y_{i} + z_{i}) \\nonumber\\\\ &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}x_{i} + \\dfrac{1}{N}\\sum_{i=1}^{N}y_{i} + \\dfrac{1}{N}\\sum_{i=1}^{N}z_{i} \\nonumber\\\\ &amp; = \\mu_x + \\mu_y + \\mu_z \\nonumber\\\\ &amp; = E(X) + E(Y) + E(Z) \\nonumber \\end{align}\\] Note also that \\(E(X^2) \\neq [E(X)]^2\\) and that for non-linear functions \\(E[g(X)] \\neq g[E(X)]\\). and can be seen with simple numerical examples. This is left as an exercise. Covariance The population between two random variables X and Y \\(cov(X,Y)\\) is defined as the expected value of the product of the deviation of the variables from their respective means. \\[\\begin{align} cov(X,Y) &amp; = E\\{(X-\\mu_x)(Y-\\mu_y)\\} \\\\ &amp; = E[XY - \\mu_xY - \\mu_yX + \\mu_y\\mu_x]\\nonumber\\\\ &amp; = E(XY) - \\mu_xE(Y) - \\mu_y(X) + E(\\mu_x\\mu_y)\\nonumber\\\\ &amp; = E(XY) - \\mu_x\\mu_y - \\mu_y\\mu_x + \\mu_x\\mu_y\\nonumber\\\\ &amp; = E(XY) - \\mu_x\\mu_y\\nonumber\\\\ &amp; = E(XY) - E(X)E(Y)\\nonumber \\end{align}\\] On the contrary, two random variables \\(X\\) and \\(Y\\) are said to be independent if \\(cov(X,Y) = 0\\). Only in this case, \\[\\begin{align} E(XY) = E(X)E(Y) \\end{align}\\] Rule 1 If \\(Y = V + W\\), then, \\[\\begin{align} cov(X, Y) = cov(X, V) + cov(X, W) \\end{align}\\] PROOF for Rule 1: \\[\\begin{align} cov(X,Y) &amp; = E\\{(X - \\mu_x)(Y - \\mu_y)\\} \\nonumber \\\\ &amp; = E\\{(X - \\mu_x)([V + W] - [\\mu_v + \\mu_w])\\} \\nonumber \\\\ &amp; = E\\{(X - \\mu_x)(V - \\mu_v) + (X - \\mu_x)(W - \\mu_w)\\}\\nonumber \\\\ &amp; = cov(X, V) + cov(X, W)\\nonumber \\end{align}\\] Rule 2 If \\(Y = bZ\\), where is a constant and a random variable, then, \\[\\begin{align} cov(X, Y) = b\\cdot cov(X, Z) \\end{align}\\] PROOF for Rule 2 \\[\\begin{align} cov(X,Y) &amp; = E\\{(X - \\mu_x)(Y - \\mu_y)\\}\\nonumber \\\\ &amp; = E\\{(X - \\mu_x) (bZ - b\\mu_z)\\}\\nonumber \\\\ &amp; = bE\\{(X - \\mu_x) (Z - \\mu_z)\\}\\nonumber \\\\ &amp; = bcov(X,Z)\\nonumber \\end{align}\\] Rule 3 If \\(Y = b\\), where is a constant, then, \\[\\begin{align} cov(X, Y) = 0 \\end{align}\\] PROOF for Rule 3: \\[\\begin{align} cov(X,Y) &amp; = E\\{(X - \\mu_x)(Y - \\mu_y)\\}\\nonumber \\\\ &amp; = E\\{(X - \\mu_x) (b - b)\\}\\nonumber \\\\ &amp; = E\\{0\\}\\nonumber \\end{align}\\] Variance The population of a random variable \\(X\\) can be understood as a measure of the dispersion of its probability distribution. It is defined as the expected (or average) squared deviation of its values from the mean. \\[\\begin{align} var(X) &amp; = E\\{(X - \\mu_x)^2\\}\\nonumber \\\\ &amp; = E(X^2 - 2\\mu_xX + \\mu_{X}^{2})\\nonumber \\\\ &amp; = E(X^2) + E(-2\\mu_xX) + E(\\mu_{X}^{2})\\nonumber \\\\ &amp; = E(X^2) - 2\\mu_xE(X) + \\mu_{X}^{2}\\nonumber \\\\ &amp; = E(X^2) - 2\\mu_x\\mu_x + \\mu_{X}^{2}\\nonumber \\\\ &amp; = E(X^2) - \\mu_x^{2}\\nonumber \\end{align}\\] We can also think of a random variable \\(X\\) composed of two entities, the population mean \\(\\mu_x\\) and a disturbance term or random component \\(u\\). \\[\\begin{align} X = \\mu_x + u \\hspace{1cm}\\text{from which it follows that}\\hspace{1cm} u = X - \\mu_x \\end{align}\\] We can show that the expected value of \\(u\\) is zero and that the variance of X is the same as the variance of \\(u\\). In other words, the variance of X solely depends on the variance of \\(u\\) and not of its mean (!). \\[\\begin{align} E(u) = E(X - \\mu_x) = E(X) + E(- \\mu_x) = \\mu_x - \\mu_x = 0 \\end{align}\\] We know that, \\[\\begin{align} var(X) = E\\{(X - \\mu_x)^2\\} = E(u^2) \\end{align}\\] and, \\[\\begin{align} var(u) = E\\{(u - \\text{mean of}\\hspace{0.1cm} u)^2 \\} = E\\{(u - 0)^2 \\} = E(u^2) \\end{align}\\] therefore, It is also useful and pretty straight forward to note that the variance of a random variable \\(X\\) can be thought of the covariance of \\(X\\) with itself: \\[\\begin{align} var(X) &amp; = E\\{(X - \\mu_x)^2\\}\\nonumber \\\\ &amp; = E\\{(X - \\mu_x)(X - \\mu_x)\\}\\nonumber \\\\ &amp; = cov(X, X)\\nonumber \\end{align}\\] Rule 1 If \\(Y = V + W\\), then, \\[\\begin{align} var(Y) = var(V) + var(W) + 2cov(V, W) \\end{align}\\] PROOF for Rule 1: \\[\\begin{align} var(Y) &amp; = cov(Y,Y) \\nonumber \\\\ &amp; = cov(Y,[V + W]) \\nonumber \\\\ &amp; = cov(Y,V) + cov(Y,W) \\nonumber \\\\ &amp; = cov(V,[V + W]) + cov([V + W],W) \\nonumber \\\\ &amp; = cov(V,V) + cov(W,V) + cov(V,W) + cov(W,W) \\nonumber \\\\ &amp; = var(V) + var(W) + 2cov(V, W) \\nonumber \\\\ \\end{align}\\] Rule 2 If \\(Y = bZ\\), where \\(b\\) is a constant, then, \\[\\begin{align} var(Y) = b^2var(Z) \\end{align}\\] PROOF for Rule 2: \\[\\begin{align} var(Y) &amp; = cov(Y,Y)\\nonumber \\\\ &amp; = cov(bZ,Y)\\nonumber \\\\ &amp; = bcov(Z,Y)\\nonumber \\\\ &amp; = bcov(Z,bZ)\\nonumber \\\\ &amp; = b^2cov(Z,Z)\\nonumber \\\\ &amp; = b^2var(Z)\\nonumber \\end{align}\\] Rule 3 If \\(Y = b\\), where \\(b\\) is a constant, then, \\[\\begin{align} var(Y) = 0 \\end{align}\\] PROOF for Rule 3: \\[\\begin{align} var(Y) &amp; = cov(b,b)\\nonumber \\\\ &amp; = 0 \\nonumber \\end{align}\\] Rule 4 If \\(Y = V + b\\), where \\(b\\) is a constant, then, \\[\\begin{align} var(Y) = var(V) \\end{align}\\] PROOF for Rule 4: \\[\\begin{align} var(Y) &amp; = var(V + b) \\nonumber \\\\ &amp; = var(V) + var(b) + 2cov(V,b)\\nonumber \\\\ &amp; = var(V) \\nonumber \\\\ \\end{align}\\] "],["chapter6.html", "Chapter 6 Introduction to statistical inference 6.1 The normal distribution24 6.2 The standard normal distribution", " Chapter 6 Introduction to statistical inference The basics of hypothesis testing19 In descriptive statistics, we work with a sample of data obtained from a larger population and we are interested in understanding the characteristics of the sample. In inferential statistics, we use the sample to try to obtain conclusions (more precisely said, to make inference) regarding the characteristics of the population from which the sample is obtained. For example, having a sample available we might be interested in inferring whether the sample mean (sample statistics, \\(\\bar{x}\\)) is representative of the population mean (population parameter, \\(\\mu\\)). That is, we are interested in generalising the information obtained from the sample to the entire population. Of course it is easy to imagine that our conclusion will be subject to error as we only have one sample available. With only one sample available, it is very unlikely that the sample mean will be identical to the population mean. So in our inference process we have to take this sampling error into account. Hypothesis testing is a widely used statistical procedure for making inference from a sample to a population.20 The test starts with two statements. The null hypothesis, denoted \\(H_0\\). The null hypothesis describes the condition that is assumed to be true at the time. It is often compared to the situation in court where an accused person is assumed to be innocent until there is enough evidence to find him guilty. The second statement is the alternative hypothesis, denote \\(H_1\\). The null hypothesis and the alternative hypothesis are mutually exclusive. The alternative hypothesis is the one that is favoured if enough evidence is found. Many times we have a theory that suggests which values to specify for the null hypothesis and the alternative hypothesis. Hypothesis testing is a means of testing the statistical (not practical) validity of our theory. \\[H_0: \\mu = \\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\] How do we decide between the null hypothesis and the alternative hypothesis? We assume that we have a normally distributed random variable \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Let us further assume that the population mean \\(\\mu\\) (which we do not know) is a value equal to \\(\\mu_0\\) (maybe because theory or experience tells us so). In a first sample obtained from our random variable \\(X\\), we compute the mean (\\(\\bar{x}\\)) and obtain a value close to the hypothesized value \\(\\mu_0\\). Since we know that data are obtained from a normally distributed \\(X\\), we know that the probability of obtaining a value close to the mean is actually quite high. In a second sample, a much larger value of \\(\\bar{x}\\) is obtained. In this case, we know that obtaining values much larger than the mean is less probable (in other words, we are in the tales of the distribution). What should we conclude with such a high value now? Does the sample results confirm or contradict the null hypothesis, i.e.Â the hypothesized value for the mean of the population? We can continue to believe that the null hypothesis is correct and that we have just an unlucky sample from which we have obtained a very high mean but we are somehow convinced that the hypothesized value is still correct (\\(H_0: \\mu = \\mu_0\\)) and the sample was just a bit off. Or we can convince ourselves that the sample result contradicts the assumed value \\(\\mu_0\\) and that we were wrong with the null hypothesis (therefore, \\(H_1: \\mu \\neq \\mu_0\\)). In the rule, the null hypothesis is rejected if the probability of obtaining such an extreme value is smaller than an arbitrarily defined probability. There are two kinds of mistakes that can be made. A Type I error and a Type II error. When we reject a hypothesis that is actually true, we are committing a Type I error. When we are not rejecting (or accepting, but this is not the right terminology) a hypothesis that should be rejected, we are committing a Type II error. Since we are basing our decision on a sample, and there is always uncertainty in the sampling process, we are more or less sure that there is some uncertainty attached to our test conclusion. In hypothesis testing we decide in advance what kind of error we want to make. The procedure is to decide in advance to commit to a certain Type I error, e.g.Â we decide in advance to tolerate a Type I error in 5% of the cases. This value is called significance level and is traditionally indicated with the greek letter \\(\\alpha\\).21 Example The mean lifetime of a sample of 100 light bulbs is computed to be 1570 hours with a standard deviation of 120 hours. If \\(\\mu\\) is the mean lifetime of all the light bulbs produced, test the hypothesis that the population mean is \\(\\mu = 1600\\) hours against the alternative that \\(\\mu \\neq 1600\\) using a significance level of 5%. Find the p-value of the test and build a 95% confidence interval. \\[H_0: \\mu = 1600\\] \\[H_1: \\mu \\neq \\space 1600\\] We need to construct our test statistic to perform the test. In practice, wee need to transform the computed mean obtained from the sample of light bulbs into a statistic that follow a standard normal distribution with zero mean and unit variance. \\[z_{test} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{1570 - 1600}{12} = -2.5\\] n &lt;- 100 sample_mean &lt;- 1570 sample_sigma &lt;- 120 mu &lt;- 1600 alpha5p &lt;- 0.05 z_test &lt;- abs((sample_mean - mu) / (sample_sigma / sqrt(n))) z_test ## [1] 2.5 z_crit_5p &lt;- qnorm(alpha5p/2, mean = 0, sd = 1, lower.tail = FALSE) Since in this exercise we are interested in performing a two-sided test (look a the formulation of the null and alternative hypothesis), we take the absolute value of our \\(z\\) test. We can reject \\(H_0\\) at the 5% significance level (\\(\\alpha\\) = 5%) if, \\[|z_{test}| &gt; z_{crit}\\] z_test &gt; z_crit_5p ## [1] TRUE We can reject \\(H_0\\) at the 5% significance level. In the following graph, in green we can see the rejection region (the two green shaded areas add up to 5%) while with the yellow line it is indicated the value of our \\(z\\) test in both tails of the distribution. As we said, since the value of our \\(z\\) test falls within the rejection region, we can reject \\(H_0\\) at the 5% significance level. What do we see highlighted in black in the graph above? The sum of the two black areas represent our p-value for the z score that we have just calculated. We can think of the p-value as the smallest significance level at which we still reject the null hypothesis (or the largest significance level at which the null hypothesis cannot be rejected). How large is the p-value for our z test? pvalue &lt;- pnorm(z_test, mean = 0, sd = 1, lower.tail = FALSE) * 2 pvalue*100 ## [1] 1.241933 How can we interpret the p-value? In our exercise, the p-value of 1.24% represents the probability that a mean lifetime of less than 1570 or more than 1630 hours would occur by chance if \\(H_0\\) were actually true. A rather small probability. How did we obtain the values 1570 and 1630? We have used (half) of the p-value to calculate the quantiles of a normal distribution with mean \\(mu = 1600\\) and standard deviation \\(120/\\sqrt{100}\\). qnorm(pvalue/2, mean = mu, sd = (sample_sigma / sqrt(n)), lower.tail = FALSE) ## [1] 1630 qnorm(pvalue/2, mean = mu, sd = (sample_sigma / sqrt(n)), lower.tail = TRUE) ## [1] 1570 (pnorm(1570, mean = mu, sd = (sample_sigma / sqrt(n)), lower.tail = TRUE) + pnorm(1630, mean = mu, sd = (sample_sigma / sqrt(n)), lower.tail = FALSE)) * 100 ## [1] 1.241933 Since we see that the p-value is larger than 1%, we already know that we will fail to reject the null hypothesis at the 1% significance level (\\(\\alpha\\) = 1%) . Shouldnt we have used a t test rather than a \\(z\\) test as suggested by professional statisticians? Probably yes. Since we do not know the population standard deviation (the standard deviation of all light bulbs), a t test sounds more appropriate. However, since our sample is relatively large (\\(n = 100\\)), much larger than the commonly suggested rule of thumb (\\(n &gt; 30\\)), we will practically obtain (almost) the same result using the normal distribution and the t distribution.22 To conclude, we need to construct the 95% confidence interval (\\(100 - \\alpha\\), where \\(\\alpha\\) = 5%). \\[(\\bar{x} + z_{\\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] We must be careful with the signs in the formula above. Once we look up the critical value \\(z_{\\alpha / 2}\\) (or we compute it using R) we will see that the quantity is actually negative. If we include the negative sign in the formula for the confidence interval, it would not be wrong, but it can create some confusion. This is why it is better to use the formula below. In this case, we have to include the negative sign in the formula because \\(z_{1 - \\alpha / 2}\\) is going to be a positive number and this is the version that we are going to implement in R. \\[(\\bar{x} - z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] sample_mean + c(-1, +1) * z_crit_5p * (sample_sigma / sqrt(n)) ## [1] 1546.48 1593.52 The hypothesized value for the mean lifetime of all the bulbs produced was 1600 working hours. Since this particular value falls outside the confidence interval constructed around the sample mean, we can reject \\(H_0\\). With the confidence interval test the null hypothesis is rejected if and only if the hypothesized value falls outside the confidence interval. The \\(z\\) test (or \\(t\\) test) and the confidence interval test are basically an elaboration of one another and provides always the same test decision.23 6.1 The normal distribution24 The normal distribution is a common probability distribution in statistics and econometrics (it is just one of many distributions). The normal distribution fits a number of natural and social phenomena. When a phenomenon (a random variable) has a normal distribution, its probability density function (for short, PDF) assumes the well-known bell-shaped curve. The normal distribution is sometimes called the Gaussian distribution or the Gauss curve in honor of the famous mathematician Carl-Friedrich Gauss.25 Shape and position of the normal distribution are entirely determined by mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) of the normally distributed random variable. This is written as: \\[X \\sim Normal(\\mu, \\sigma)\\] For example, we see that the two normal distributions shown in the following graph have the same mean but different standard deviations. The mean determines the location of the normal distribution in the horizontal axis. The majority of the body is located around the mean to which correspond the peak in the distribution. The standard deviation determines the shape of the curve. In practice, it determines how far the values of the variable are from the mean. This means that a higher mean shifts the curve to the right without changing its shape. Similarly, a higher standard deviation widens the body of the curve without shifting its position on the horizontal axis. The normal distribution has a number of interesting and useful properties. First of all, it is symmetrical with respect to the mean, from which it follows that half of the values are distributed half to the right and half to the left of the mean. Knowing the mean and standard deviation of a certain event or random variable, the normal distribution allows us to calculate the probability that the event will assume a certain value or range of values. Roughly speaking, this correspond to the area below the curve. In reality, this is done using the cumulative distribution function (CDF) which is nothing more than the integral of the PDF. The following figure shows the relationship between PDF (left) and CDF (right) of a normally distributed random variable with mean 0 and standard deviation 1. 6.2 The standard normal distribution A special case of normal distribution is the standard normal distribution where the mean is equal to 0 and the standard deviation is equal to 1 (this is actually what we used in the previous exercise but we had not yet used this term). Let us now see how it is possible to standardise a variable. This is a very important procedure that we will see again later when we talk about hypothesis testing. In the following section we will instead use it to calculate the so-called \\(z\\) scores. \\[Z = \\frac{X - \\mu}{\\sigma}\\] Rewriting \\(Z\\) as \\(aX + b\\), where \\(a = (1/\\sigma)\\) and \\(b = -(\\mu/\\sigma)\\) and using the properties of expectation and variance we can see that: \\[E(Z) = aE(X) + b = (\\mu/\\sigma) - (\\mu/\\sigma) = 0\\] \\[Var(Z) = a^2Var(X) = (\\sigma^2/\\sigma^2) = 1\\] What does that mean? It means that if we subtract the mean from a variable (\\(X\\)) and divide it by the standard deviation we will have a standardised variable (\\(Z\\)) that has a mean of zero and standard deviation of 1. Exercises We are given the following set of numbers: \\(6, 2 , 8, 7, 5\\). Transform the set into standard scores and check that mean and standard deviation of the transformed set are respectively 0 and 1. x &lt;- c(6, 2 , 8, 7, 5) mean_x &lt;- mean(x) sd_x &lt;- sd(x) z &lt;- (x - mean_x)/sd_x mean(z) ## [1] 1.387779e-16 sd(z) ## [1] 1 Let us assume that the random variable \\(X\\) is a normally distributed random variable with mean (\\(\\mu\\)) equal to 5 and population standard deviation (\\(\\sigma\\)) equal to 4. In short, \\(Normal ~ (5,4)\\). Calculate the probabilities that our random variable \\(X\\) assume a value smaller than 6, \\(P(X \\leq 6)\\), using the table of the standard normal probabilities or R (much better!). If we did not have R available we would have to find the \\(z\\) score corresponding to the value of interest, 6 in this case, and look in the table of standard normal probabilities (the area below the curve) the probability that our random variable assumes a value smaller than that.26 mu_x &lt;- 5 sigma_x &lt;- 4 z &lt;- (6 - mu_x) / sigma_x z ## [1] 0.25 \\[z = \\frac{6 - 5}{4} = 0.25\\] Our \\(z\\) value of interest is 0.25. The probability that the variable \\(X\\) takes on a value less than 6 is given by the area under the normal curve to the left of \\(z = 0.25\\). This value is equal to: pnorm(z, mean = 0, sd = 1, lower.tail = TRUE)*100 ## [1] 59.87063 We can achieve the same result by using the lower.tail = FALSE option. In this case we get the white area in the graph below and will have to subtract this quantity from 1 or 100%, i.e.Â the whole area under the curve. 100 - pnorm(z, mean = 0, sd = 1, lower.tail = FALSE)*100 ## [1] 59.87063 In the graph below, the area marked in green indicates the probability that the independent variable \\(X\\) takes on a value less than 6 given mean and population standard deviation of 5 and 4, respectively. If we have software at our disposal we do not have to use tables. In this case there is no need to calculate the \\(z\\) score. The result (and the graph) will be exactly the same with the important difference that now the values reported in the horizontal axis will be the values of \\(X\\) and not the standardized scores. ## [1] 59.87063 The test scores for a class of students (this is the population) are normally distributed with mean (\\(\\mu\\)) equal to 75 points and standard deviation (\\(\\sigma\\)) equal to 10 points. What is the probability that a students scores above 80 points? Calculate the following probabilities: Given \\(X \\sim Normal(3,4)\\), find \\(P(X \\leq 1)\\) Given \\(X \\sim Normal(4,0)\\), find \\(P(2 &lt; X \\leq 6)\\) The discussion presented here is based on Wooldridge, J. Introductory Econometrics: A Modern Approach (Chapter 4 and Appendix C) and Dougherty, C. Introduction to Econometrics (Review chapter). This procedure is part of the branch of statistics called parametric statistics. In fact, it is assumed that the population from which the sample is obtained follows some kind of distribution (\\(t\\) distribution, normal distribution, \\(F\\) distribution) to which reference will be made to compare the plausibility of the sample statistics. Another branch of statistics is called non-parametric and does not assume any kind of distribution. Again, \\(\\alpha\\) is the probability of rejecting a true null hypothesis. Traditionally in econometrics, we use three levels of significance 10%, 5% and 1%. If we want to see why this is the case, take a look here. For common misunderstandings about the confidence interval, see here. The discussion presented here and in particular the proof of mean and standard deviation of the the standardized random variable rely on Wooldridge, J. Introductory Econometrics: A Modern Approach (Appendix C). At first glance, many phenomena do not appear to follow a normal distribution. However, after a logarithmic transformation they assume a (log)normal distribution. Since the normal distribution is continuous, \\(P(Z &lt; z) = P(Z \\leq z)\\). "],["chapter7.html", "Chapter 7 Regression analysis 7.1 Simple regression model 7.2 Multiple regression model 7.3 Regression assumptions", " Chapter 7 Regression analysis 7.1 Simple regression model Text and the derivation above rely on the suggested literature: Wooldridge, J. Introductory Econometrics: A Modern Approach (Chapter 2) and Dougherty, C. Introduction to Econometrics (Chapter 1). We are given the following set of values: We want to find the line that minimizes the sum of squared residuals, i.e.Â the squared distance between the observed value and the line. In a simple bivariate case as this one, we have to find a slope (\\(\\hat{\\beta_1}\\)) and an intercept (\\(\\hat{\\beta_0}\\)) for the line so that the sum of squared residuals is as small as possible. \\[\\begin{align} y_i &amp; = \\hat{y_i} + \\hat{u_i} \\\\ \\hat{y_i} &amp; = \\hat{\\beta_0} + \\hat{\\beta_1}x_i \\\\ y_i &amp; = \\hat{\\beta_0} + \\hat{\\beta_1}x_i + \\hat{u_i} \\\\ \\hat{u_i} &amp; = y_i - \\hat{y_i} \\\\ \\hat{u_i} &amp; = y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}x_i) \\end{align}\\] We have now a function that we want to minimize with respect to \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). \\[\\begin{align} f(\\hat{\\beta_0}, \\hat{\\beta_1}) = \\sum_{n=1}^{3} \\hat{u_i}^2 = \\sum_{n=1}^{3} (y_i - \\hat{y_i})^2 = \\sum_{n=1}^{3} (y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}x_i))^2 = \\sum_{n=1}^{3} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)^2 \\end{align}\\] We now take the derivative of (6) with respect to \\(\\hat{\\beta_0}\\) and set it equal to 0. We then do the same thing for \\(\\hat{\\beta_1}\\). These are the so-called . \\[\\begin{align} \\dfrac{\\partial f(\\hat{\\beta_0}, \\hat{\\beta_1})}{\\partial \\hat{\\beta_0}} &amp; = -2\\sum_{n=1}^{3} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0 \\\\ \\dfrac{\\partial f(\\hat{\\beta_0}, \\hat{\\beta_1})}{\\partial \\hat{\\beta_1}} &amp; = -2\\sum_{n=1}^{3} x_i (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0 \\end{align}\\] x &lt;- c(-3, 3, 5) # Variable x y &lt;- c(5, 6, 2) # Variable y # Plot plot(x, y, # First variable on the h. axis then the variable on the v. axis xlim=c(-4,6), # Length of the x-axis ylim=c(-4, 10)) # Length of the y-axis abline(v=0) # Line for the vertical axis abline(h=0) # Line for the horizotal axis # Fitting the line to the data abline(lm(y ~ x), # Slope and intercept are calculated automatically using the lm() function col = &quot;red&quot;) # Color of the line # Line for the mean of y abline(h = mean(y), col = &quot;green&quot;, lty = 3) legend(&quot;bottomright&quot;, # Position of the legend title = &quot;LEGEND&quot;, # Title of the legend expression(hat(y[i]) == hat(beta)[&quot;0&quot;] + hat(beta)[&quot;1&quot;]%.%x[i], bar(y)), # Names of the variables col = c(&quot;red&quot;, &quot;green&quot;), # Colors of the lines lty = c(1, 3), # Type of lines cex = 1, # Dimesions horiz = TRUE, # Option horizontal bg = &quot;transparent&quot;, # Background bty = &quot;n&quot;) # No box around the legend Slope coefficient beta_1 &lt;- cov(y, x) / var(x) Intercept beta_0 &lt;- mean(y) - beta_1 * mean(x) We calculate the fitted values y_hat &lt;- beta_0 + beta_1 * (x) y_hat ## [1] 5.5 4.0 3.5 We calculate the residuals u_hat &lt;- y - y_hat u_hat ## [1] -0.5 2.0 -1.5 We calculate the residual sum of squares (SSR) SSR &lt;- sum(u_hat^2) SSR ## [1] 6.5 We calculate the explained sum of squares (SSE) SSE &lt;- sum((y_hat-mean(y))^2) SSE ## [1] 2.166667 We can calculate the sum of square total (SST) SST &lt;- SSR + SSE SST ## [1] 8.666667 Lets check var(y)*(length(y)-1) ## [1] 8.666667 We can finally calculate the R2 R2 &lt;- SSE / SST R2 ## [1] 0.25 Alternatively R2 &lt;- 1 - SSR/SST R2 ## [1] 0.25 We can also calculate the root mean squared error (RMSE); Degrees of freedom correction = 2 RMSE &lt;- sqrt(SSR/1) RMSE ## [1] 2.54951 We can now compare our work with the output of the lm() function model1 &lt;- lm(y ~ x) stargazer(model1, type = &quot;html&quot;, dep.var.labels = c(&quot;y&quot;), #column.labels = c(&quot;&quot;), covariate.labels = c(&quot;x&quot;), colnames = FALSE, model.numbers = FALSE, keep.stat = c(&quot;n&quot;, &quot;rsq&quot;, &quot;ser&quot;)) Dependent variable: y x -0.250 (0.433) Constant 4.750 (1.639) Observations 3 R2 0.250 Residual Std. Error 2.550 (df = 1) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 x &lt;- c(-3, 3, 5) # Variable x y &lt;- c(5, 6, 2) # Variable y # We can fit a quadratic model; remember the general equation of the parabola f(x) = c + b*x + a*x^2 model2 &lt;- lm(y ~ x + I(x^2)) model2 ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Coefficients: ## (Intercept) x I(x^2) ## 7.9375 0.1667 -0.2708 beta_0 &lt;- unname(coef(model2)[&quot;(Intercept)&quot;]) beta_1 &lt;- unname(coef(model2)[&quot;x&quot;]) beta_2 &lt;- unname(coef(model2)[&quot;I(x^2)&quot;]) # We use the estimated parameters to plot a parabola mod &lt;- function(x) {x^2 * beta_2 + x * beta_1 + beta_0} plot(x, y, xlim = c(-10, 10), ylim = c(-10, 10)) curve(mod, from = -10, to = 10, add = TRUE) abline(v = 0) # Line for the vertical axis abline(h = 0) # Line for the horizontal axis points(-3, 5, col = &quot;black&quot;, pch = 19, cex = 1) # Mark the point in black points(3, 6, col = &quot;black&quot;, pch = 19, cex = 1) # Mark the point in black points(5, 2, col = &quot;black&quot;, pch = 19, cex = 1) # Mark the point in black # We can use the estimated coefficients to simulate some new data and then fit a parabola again # and a linear model. We then compare the R2. varx &lt;- -10:10 vary &lt;- beta_0 + rnorm(21, 0, 2) + (varx) * beta_1 + (varx)^2 * beta_2 # Estimating the new parameters (they are very similar) model3 &lt;- lm(vary ~ varx + I(varx^2)) # We calculate the predicted values plot(varx, vary, xlim = c(-10, 10), ylim = c(-10, 10)) vary_new &lt;- predict(model3) matlines(varx, vary_new) abline(h = 0, lty = 3) abline(v = 0, lty = 3) # We mark the original observations. They are very very close to the fitted model points(-3, 5, col=&quot;black&quot;, pch = 19, cex = 1) points(3, 6, col=&quot;black&quot;, pch = 19, cex = 1) points(5, 2, col=&quot;black&quot;, pch = 19, cex = 1) # We can now try to fit a linear model model4 &lt;- lm(vary ~ varx) # Overlap the linear model to the previous plot abline(model4) Which model has the largest \\(R^2\\)? stargazer(model4, model3, type = &quot;html&quot;, dep.var.labels = c(&quot;y&quot;), column.labels = c(&quot;1&quot;, &quot;2&quot;), covariate.labels = c(&quot;x&quot;, &quot;x&quot;), colnames = FALSE, model.numbers = FALSE, keep.stat = c(&quot;n&quot;, &quot;rsq&quot;, &quot;ser&quot;)) Dependent variable: y 1 2 x 0.162 0.162* (0.360) (0.080) x -0.284*** (0.015) Constant -2.140 8.281*** (2.182) (0.724) Observations 21 21 R2 0.010 0.954 Residual Std. Error 9.999 (df = 19) 2.208 (df = 18) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 7.2 Multiple regression model 7.3 Regression assumptions "],["chapter8.html", "Chapter 8 Inference 8.1 The basics of hypothesis testing27 8.2 The t test in the regression 8.3 F statistic for overall significance of a regression", " Chapter 8 Inference 8.1 The basics of hypothesis testing27 In descriptive statistics, we work with a sample of data obtained from a larger population and we are interested in understanding the characteristics of the sample. In inferential statistics, we use the sample to try to obtain conclusions (more precisely said, to make inference) regarding the characteristics of the population from which the sample is obtained. For example, having a sample available we might be interested in inferring whether the sample mean (sample statistics, \\(\\bar{x}\\)) is representative of the population mean (population parameter, \\(\\mu\\)). That is, we are interested in generalising the information obtained from the sample to the entire population. Of course it is easy to imagine that our conclusion will be subject to error as we only have one sample available. With only one sample available, it is very unlikely that the sample mean will be identical to the population mean. So in our inference process we have to take this sampling error into account. Hypothesis testing is a widely used statistical procedure for making inference from a sample to a population.28 The test starts with two statements. The null hypothesis, denoted \\(H_0\\). The null hypothesis describes the condition that is assumed to be true at the time. It is often compared to the situation in court where an accused person is assumed to be innocent until there is enough evidence to find him guilty. The second statement is the alternative hypothesis, denote \\(H_1\\). The null hypothesis and the alternative hypothesis are mutually exclusive. The alternative hypothesis is the one that is favoured if enough evidence is found. Many times we have a theory that suggests which values to specify for the null hypothesis and the alternative hypothesis. Hypothesis testing is a means of testing the statistical (not practical) validity of our theory. \\[H_0: \\mu = \\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\] How do we decide between the null hypothesis and the alternative hypothesis? We assume that we have a normally distributed random variable \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Let us further assume that the population mean \\(\\mu\\) (which we do not know) is a value equal to \\(\\mu_0\\) (maybe because theory or experience tells us so). In a first sample obtained from our random variable \\(X\\), we compute the mean (\\(\\bar{x}\\)) and obtain a value close to the hypothesized value \\(\\mu_0\\). Since we know that data are obtained from a normally distributed \\(X\\), we know that the probability of obtaining a value close to the mean is actually quite high. In a second sample, a much larger value of \\(\\bar{x}\\) is obtained. In this case, we know that obtaining values much larger than the mean is less probable (in other words, we are in the tales of the distribution). What should we conclude with such a high value now? Does the sample results confirm or contradict the null hypothesis, i.e.Â the hypothesized value for the mean of the population? We can continue to believe that the null hypothesis is correct and that we have just an unlucky sample from which we have obtained a very high mean but we are somehow convinced that the hypothesized value is still correct (\\(H_0: \\mu = \\mu_0\\)) and the sample was just a bit off. Or we can convince ourselves that the sample result contradicts the assumed value \\(\\mu_0\\) and that we were wrong with the null hypothesis (therefore, \\(H_1: \\mu \\neq \\mu_0\\)). In the rule, the null hypothesis is rejected if the probability of obtaining such an extreme value is smaller than an arbitrarily defined probability. There are two kinds of mistakes that can be made. A Type I error and a Type II error. When we reject a hypothesis that is actually true, we are committing a Type I error. When we are not rejecting (or accepting, but this is not the right terminology) a hypothesis that should be rejected, we are committing a Type II error. Since we are basing our decision on a sample, and there is always uncertainty in the sampling process, we are more or less sure that there is some uncertainty attached to our test conclusion. In hypothesis testing we decide in advance what kind of error we want to make. The procedure is to decide in advance to commit to a certain Type I error, e.g.Â we decide in advance to tolerate a Type I error in 5% of the cases. This value is called significance level and is traditionally indicated with the greek letter \\(\\alpha\\).29 Example The mean lifetime of a sample of 100 light bulbs is computed to be 1570 hours with a standard deviation of 120 hours. If \\(\\mu\\) is the mean lifetime of all the light bulbs produced, test the hypothesis that the population mean is \\(\\mu = 1600\\) hours against the alternative that \\(\\mu \\neq 1600\\) using a significance level of 5%. Find the p-value of the test and build a 95% confidence interval. \\[H_0: \\mu = 1600\\] \\[H_1: \\mu \\neq \\space 1600\\] We need to construct our test statistic to perform the test. In practice, wee need to transform the computed mean obtained from the sample of light bulbs into a statistic that follow a standard normal distribution with zero mean and unit variance. \\[z_{test} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{1570 - 1600}{12} = -2.5\\] n &lt;- 100 sample_mean &lt;- 1570 sample_sigma &lt;- 120 mu &lt;- 1600 alpha5p &lt;- 0.05 z_test &lt;- abs((sample_mean - mu) / (sample_sigma / sqrt(n))) z_test ## [1] 2.5 z_crit_5p &lt;- qnorm(alpha5p/2, mean = 0, sd = 1, lower.tail = FALSE) Since in this exercise we are interested in performing a two-sided test (look a the formulation of the null and alternative hypothesis), we take the absolute value of our \\(z\\) test. We can reject \\(H_0\\) at the 5% significance level (\\(\\alpha\\) = 5%) if, \\[|z_{test}| &gt; z_{crit}\\] z_test &gt; z_crit_5p ## [1] TRUE We can reject \\(H_0\\) at the 5% significance level. In the following graph, in green we can see the rejection region (the two green shaded areas add up to 5%) while with the yellow line it is indicated the value of our \\(z\\) test in both tails of the distribution. As we said, since the value of our \\(z\\) test falls within the rejection region, we can reject \\(H_0\\) at the 5% significance level. What do we see highlighted in black in the graph above? The sum of the two black areas represent our p-value for the z score that we have just calculated. We can think of the p-value as the smallest significance level at which we still reject the null hypothesis (or the largest significance level at which the null hypothesis cannot be rejected). How large is the p-value for our z test? pvalue &lt;- pnorm(z_test, mean = 0, sd = 1, lower.tail = FALSE) * 2 pvalue*100 ## [1] 1.241933 How can we interpret the p-value? In our exercise, the p-value of 1.24% represents the probability that a mean lifetime of less than 1570 or more than 1630 hours would occur by chance if \\(H_0\\) were actually true. A rather small probability. How did we obtain the values 1570 and 1630? We have used (half) of the p-value to calculate the quantiles of a normal distribution with mean \\(mu = 1600\\) and standard deviation \\(120/\\sqrt{100}\\). qnorm(pvalue/2, mean = mu, sd = (sample_sigma / sqrt(n)), lower.tail = FALSE) ## [1] 1630 qnorm(pvalue/2, mean = mu, sd = (sample_sigma / sqrt(n)), lower.tail = TRUE) ## [1] 1570 (pnorm(1570, mean = mu, sd = (sample_sigma / sqrt(n)), lower.tail = TRUE) + pnorm(1630, mean = mu, sd = (sample_sigma / sqrt(n)), lower.tail = FALSE)) * 100 ## [1] 1.241933 Since we see that the p-value is larger than 1%, we already know that we will fail to reject the null hypothesis at the 1% significance level (\\(\\alpha\\) = 1%) . Shouldnt we have used a t test rather than a \\(z\\) test as suggested by professional statisticians? Probably yes. Since we do not know the population standard deviation (the standard deviation of all light bulbs), a t test sounds more appropriate. However, since our sample is relatively large (\\(n = 100\\)), much larger than the commonly suggested rule of thumb (\\(n &gt; 30\\)), we will practically obtain (almost) the same result using the normal distribution and the t distribution.30 To conclude, we need to construct the 95% confidence interval (\\(100 - \\alpha\\), where \\(\\alpha\\) = 5%). \\[(\\bar{x} + z_{\\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] We must be careful with the signs in the formula above. Once we look up the critical value \\(z_{\\alpha / 2}\\) (or we compute it using R) we will see that the quantity is actually negative. If we include the negative sign in the formula for the confidence interval, it would not be wrong, but it can create some confusion. This is why it is better to use the formula below. In this case, we have to include the negative sign in the formula because \\(z_{1 - \\alpha / 2}\\) is going to be a positive number and this is the version that we are going to implement in R. \\[(\\bar{x} - z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] sample_mean + c(-1, +1) * z_crit_5p * (sample_sigma / sqrt(n)) ## [1] 1546.48 1593.52 The hypothesized value for the mean lifetime of all the bulbs produced was 1600 working hours. Since this particular value falls outside the confidence interval constructed around the sample mean, we can reject \\(H_0\\). With the confidence interval test the null hypothesis is rejected if and only if the hypothesized value falls outside the confidence interval. The \\(z\\) test (or \\(t\\) test) and the confidence interval test are basically an elaboration of one another and provides always the same test decision.31 8.2 The t test in the regression We are now going to perform the test of the hypothesis on the population parameters of a simple bivariate linear regression model. The good news is that this procedure is done automatically by all statistical packages that run regressions. In our case, we are going to replicate the results provided with the output of the \\(lm\\) function in R. First, we perform a two-sided t test for the intercept parameter (\\(\\beta_0\\)). We want to test the null hypothesis (\\(H_0\\)) that \\(\\beta_0 = 0\\) against the alternative hypothesis (\\(H_1\\)) that \\(\\beta_0 \\neq 0\\). This is the kind of hypothesis test that is usually performed by linear regression function in software packages. Since we are interested in a two-sided test, we will take the absolute value of our \\(t\\) statistic. We want to perform the test at the 5% significance level. We need to find the critical value for a 5% significance level with 524 degrees of freedom. We reject \\(H_0\\) if \\(|t_{\\hat{\\beta}_0}| &gt; t_{crit}\\) otherwise we will fail to reject \\(H_0\\). alpha5p &lt;- 0.05 t_stat_beta_0 &lt;- abs(sum_reg1$coefficients[1,3]) t_crit_5p &lt;- qt(alpha5p/2, df = reg1$df.residual, lower.tail = FALSE) t_stat_beta_0 &gt; t_crit_5p ## [1] FALSE We fail to reject \\(H_0\\) at the 5% significance level. Consequently, we will fail to reject the \\(H_0\\) for every significance level smaller than 5%. Which one is the smallest significance level at which we still reject \\(H_0\\)? We need to calculate the (in)famous p-value. In this case, it is given by \\(P(|T| &gt; |t_{\\hat{\\beta}_0}|)\\).32 We can use the function pt to calculate the area below the t distribution at the right of our \\(t_{\\hat{\\beta}_0}\\). Dont forget to multiply this quantity by two as we need the p-value for a two-sided test. pt(t_stat_beta_0, df = reg1$df.residual, lower.tail = FALSE) * 2 * 100 ## [1] 18.70735 We have a relatively large sample. We know that the t distribution converges to the standard normal distribution as the number of observation in the sample approaches infinity (meaning that the quantiles of the t distribution and the standard normal distribution are the same). Therefore, we can obtain (almost) the same result using the function pnorm. pnorm(t_stat_beta_0, lower.tail = FALSE) * 2 * 100 ## [1] 18.64969 We can check that we will actually reject \\(H_0\\) for a significance level larger than our p-value. Lets try to see whats happening when the significance level is 20%.33 As before, we reject \\(H_0\\) if \\(|t_{\\hat{\\beta}_0}| &gt; t_{crit}\\) otherwise we will fail to reject \\(H_0\\). alpha20p &lt;- 0.20 t_crit_20p &lt;- qt(alpha20p/2, df = reg1$df.residual, lower.tail = FALSE) t_stat_beta_0 &gt; t_crit_20p ## [1] TRUE Given the result of the test, we reject \\(H_0\\) at the 20% significance level. We can also build a 95% confidence interval for the intercept:34 beta_0 &lt;- sum_reg1$coefficients[1,1] se_beta_0 &lt;- sum_reg1$coefficients[1,2] ci_lower &lt;- beta_0 - t_crit_5p * se_beta_0 ci_lower ## [1] -2.250472 ci_upper &lt;- beta_0 + t_crit_5p * se_beta_0 ci_upper ## [1] 0.4407687 Since the value of 0 is contained in the confidence interval, we fail to reject \\(H_0\\) at 5% significance level. The \\(H_0\\) is rejected if and only if the value 0 is not contained in the 95% confidence interval. There is no need to repeat all these steps above every time. The confint function allows us to quickly calculate the confidence interval. confint(reg1, parm = 1, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) -2.250472 0.4407687 Now, we perform a two-sided t test for the slope parameter. We want to test the null hypothesis (\\(H_0\\)) that \\(\\beta_1 = 0\\) against the alternative hypothesis (\\(H_1\\)) \\(\\beta_1 \\neq 0\\). Since we are interested in a two-sided test, we will take the absolute value of our t statistic. As we know, we reject \\(H_0\\) if \\(|t_{\\hat{\\beta}_1}| &gt; t_{crit}\\), otherwise, we fail to reject \\(H_0\\). t_stat_beta_1 &lt;- abs(sum_reg1$coefficients[2,3]) t_stat_beta_1 &gt; t_crit_5p ## [1] TRUE What is the conclusion of the test? We can reject \\(H_0\\) that \\(\\beta_1 = 0\\) at the 5% significance level. Can we reject \\(H_0\\) also at the 1% significance level? We must first calculate the critical value for a 1% significance level with 524 degrees of freedom. alpha1p &lt;- 0.01 t_crit_1p &lt;- qt(alpha1p/2, df = reg1$df.residual, lower.tail = FALSE) t_stat_beta_1 &gt; t_crit_1p ## [1] TRUE We can reject \\(H_0\\) that \\(\\beta_1 = 0\\) also at the 1% significance level. We can also build a 95% confidence interval for the slope parameter. beta_1 &lt;- sum_reg1$coefficients[2,1] se_beta_1 &lt;- sum_reg1$coefficients[2,2] ci_lower &lt;- beta_1 - t_crit_5p * se_beta_1 ci_lower ## [1] 0.4367534 ci_upper &lt;- beta_1 + t_crit_5p * se_beta_1 ci_upper ## [1] 0.6459651 Since the value of 0 is not contained in the confidence interval, we reject \\(H_0\\) at 5% significance level. Also here we can check our result using the confint function. confint(reg1, parm = 2, level = 0.95) ## 2.5 % 97.5 % ## educ 0.4367534 0.6459651 8.3 F statistic for overall significance of a regression The F test is used to test whether a group of variables has no effect on the dependent variable. In this sense, the test allows to test if the parameters of a set (or at the limit all) the independent variables are jointly significance. Obviously it is the theory or intuition that tells us to operate such a test on a given group of variables. It is often the case that the F test is performed on all independent variables in a model. It is then said that the test is for the overall significance of the regression. In this exercise, to understand how the F test works in practice, we will replicate the F test provided by the regression function in R (as by any other statistical software packages). This is precisely a test for overall joint significance of the regression. We estimate the following model: \\[wage = \\beta_0 + \\beta_1educ + \\beta_2exper + \\beta_3tenure + u\\] We formulate the following joint null hypothesis (\\(H_0\\)) stating that the regressors have jointly no effect on the dependent variable: \\[H_0 : \\beta_1 = \\beta_2 = \\beta_3 = 0\\] The alternative hypothesis (\\(H_1\\)) is: \\[H_1 : H_0 \\space \\text{is not true}\\] The formula for the F statistic (or F ratio), where \\(q\\) is the number of restrictions (in this example we are imposing three restrictions), and \\(n-k-1\\) is the number of degrees of freedom of the unrestricted model, is defined by:35 \\[F = \\frac{SSR_r - SSR_{ur}}{SSR_{ur}} * \\frac{(n-k-1)}{q}\\] First, we estimate the unrestricted model. With this term, we mean the entire or complete model: reg1 &lt;- lm(wage1$wage ~ wage1$educ + wage1$exper + wage1$tenure) We can now calculate the sum of squared residual (SSR) of the unrestricted model: SSR_ur &lt;- sum(reg1$residuals^2) We then estimate the restricted model. The restricted model has clearly less parameters than the unrestricted model. Since we are performing an F test for the overall significance of the regression, we must regress the dependent variable wage on just an intercept. In R, this is done by including only a 1 after the tilde sign in the lm function. reg2 &lt;- lm(wage1$wage ~ 1) We can now calculate the SSR of the restricted model. SSR_r &lt;- sum(reg2$residuals^2) We report the results in a single table created using the stargazer package. stargazer(reg1, reg2, type = &quot;latex&quot;, header = FALSE, title = &quot;F test for the overall significance of the regression.&quot;, keep.stat = c(&quot;n&quot;, &quot;rsq&quot;, &quot;f&quot;)) Finally, we can calculate the F statistic and its corresponding p-value. We compare the value of our F statistic (and its p-value) with the value provided by R (see the last row of the first column in the table above). df_ur &lt;- reg1$df.residual # 522 df_r &lt;- reg2$df.residual # 525 q &lt;- df_r - df_ur # 3 F_test &lt;- (SSR_r - SSR_ur) / SSR_ur * df_ur / q F_test ## [1] 76.87317 pval &lt;- pf(F_test, q, df_ur, lower.tail = FALSE) pval ## [1] 3.405862e-41 We choose a significance level (\\(\\alpha\\)) of 1% and calculate the corresponding critical value in the F distribution. qf(0.01, df = 3, df2 = 522, lower.tail = FALSE) ## [1] 3.819327 What is the conclusion of the test? We can observe that our F value is clearly larger the critical value for the chosen significance level of 1%. Our p-value is also very very small, certainly smaller than the significance level of 1%. We can therefore soundly reject the null hypothesis that the variables are not jointly significant. We can also create the graph of the F distribution. In green we mark the rejection region for the significance level that we have choosen. The discussion presented here is based on Wooldridge, J. Introductory Econometrics: A Modern Approach (Chapter 4 and Appendix C) and Dougherty, C. Introduction to Econometrics (Review chapter). This procedure is part of the branch of statistics called parametric statistics. In fact, it is assumed that the population from which the sample is obtained follows some kind of distribution (\\(t\\) distribution, normal distribution, \\(F\\) distribution) to which reference will be made to compare the plausibility of the sample statistics. Another branch of statistics is called non-parametric and does not assume any kind of distribution. Again, \\(\\alpha\\) is the probability of rejecting a true null hypothesis. Traditionally in econometrics, we use three levels of significance 10%, 5% and 1%. If we want to see why this is the case, take a look here. For common misunderstandings about the confidence interval, see here. See Wooldridge p.Â 126. Such a large significance level is never used in practice! See Wooldridge p.Â 130. The number of degrees of freedom of the unrestricted model is given by \\(n-k-1\\) where \\(n\\) is the number of observations, \\(k\\) is the number of independent variables and \\(1\\) stands for the coefficient of the intercept. "],["chapter9.html", "Chapter 9 Applications 9.1 Linear growth model 9.2 The consumption function", " Chapter 9 Applications 9.1 Linear growth model Time Series: Real Gross Domestic Product of USA (seasonally not adjusted) - Observations: from 1960 to 2007 - Units: Billions of dollars - Data: https://fred.stlouisfed.org/ - Source: Gujarati, D. Econometrics by Example Data description Time Series: Real Gross Domestic Product of USA (seasonally not adjusted) Observations: from 1960 to 2007 Units: Billions of dollars Data: https://fred.stlouisfed.org/series/GDPCA Level-level model # Loading the data RGDP &lt;- read.csv(&quot;GDPCA.csv&quot;, sep= &quot;;&quot;, header = TRUE) # Inspecting the data head(RGDP, 5L) ## DATE TIME GDPCA ## 1 1960 1 3108.707 ## 2 1961 2 3188.123 ## 3 1962 3 3383.085 ## 4 1963 4 3530.412 ## 5 1964 5 3734.043 # The model linear_trend &lt;- lm(RGDP$GDPCA ~ RGDP$TIME) library(stargazer) # Loading the data library(readr) RGDP &lt;- read_delim(&quot;C:/Users/bramucci/Documents/Intro_to_quant_methods/GDPCA.csv&quot;, delim = &quot;;&quot;, escape_double = FALSE, trim_ws = TRUE) # Inspecting the data head(RGDP, 5L) ## # A tibble: 5 x 3 ## DATE TIME GDPCA ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1960 1 3109. ## 2 1961 2 3188. ## 3 1962 3 3383. ## 4 1963 4 3530. ## 5 1964 5 3734. # The model linear_trend &lt;- lm(RGDP$GDPCA ~ RGDP$TIME) # Plot plot(RGDP$TIME, RGDP$GDPCA, type = &quot;l&quot;, col = &quot;blue&quot;, main = &quot;The linear trend model&quot;, xlab = &quot;Time&quot;, ylab = &quot;Real GDP&quot;, ylim = c(0, max(RGDP$GDPCA)), xaxs = &quot;i&quot;, yaxs = &quot;i&quot;) abline(lm(RGDP$GDPCA ~ RGDP$TIME)) stargazer(linear_trend, type = &quot;text&quot;, keep.stat = c(&quot;n&quot;, &quot;rsq&quot;)) ## ## ======================================== ## Dependent variable: ## --------------------------- ## GDPCA ## ---------------------------------------- ## TIME 242.900*** ## (6.697) ## ## Constant 1,953.991*** ## (188.490) ## ## ---------------------------------------- ## Observations 48 ## R2 0.966 ## ======================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 The slope coefficient gives us absolute change in real GDP per unit of time (year). These results show that over the period 19602007, real GDP in the USA increased on average by about $243 billion per year. Log-level model We can also estimate the rate of growth of real GDP using logs. We know that: \\[RGDP_t = RGDP_{1960}*(1 + r)^t\\] Using logs we can rewrite the function above as: \\[ln(RGDP_t) = ln(RGDP_{1960}) + ln(1 + r)*t\\] Now letting \\(\\beta_0 = ln(RGDP_{1960})\\) and \\(\\beta_1 = ln(1 + r)\\) we can rewrite the model as: \\[ln(RGDP_t) = \\beta_0 + \\beta_1*t + u_t\\] RGDP$lnGDPCA &lt;- log(RGDP$GDPCA) log_linear_trend &lt;- lm(RGDP$lnGDPCA ~ RGDP$TIME) RGDP$lnGDPCA &lt;- log(RGDP$GDPCA) log_linear_trend &lt;- lm(RGDP$lnGDPCA ~ RGDP$TIME) plot(RGDP$TIME, RGDP$lnGDPCA, type = &quot;l&quot;, col = &quot;red&quot;, main = &quot;The log-linear trend model&quot;, xlab = &quot;Time&quot;, ylab = &quot;Log real GDP&quot;, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;) abline(lm(RGDP$lnGDPCA ~ RGDP$TIME)) stargazer(log_linear_trend, type = &quot;text&quot;, keep.stat = c(&quot;n&quot;, &quot;rsq&quot;)) ## ## ======================================== ## Dependent variable: ## --------------------------- ## lnGDPCA ## ---------------------------------------- ## TIME 0.032*** ## (0.0003) ## ## Constant 8.088*** ## (0.009) ## ## ---------------------------------------- ## Observations 48 ## R2 0.995 ## ======================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Results show that real GDP in the USA has been increasing at a rate of 3.22% per year. If we exponentiate the slope coefficient (\\(e^{\\beta_0}\\)) we obtain approximately the value of real GDP at the starting period (1960). exp(8.0881267) ## [1] 3255.583 head(RGDP, 1L) ## # A tibble: 1 x 4 ## DATE TIME GDPCA lnGDPCA ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1960 1 3109. 8.04 The coefficient \\(\\beta_2\\) gives us the instantaneous rate of growth. We can also calculate the compound rate of growth \\(r\\). (exp(0.0322421) - 1)*100 ## [1] 3.276751 Quadratic regression model (level-level) We can now consider the following polynomial (i.e.Â quadratic) model: \\[ RGDP_t = \\beta_0 + \\beta_1*t + \\beta_2*t^2 + u_t \\] The fact that \\(t\\) and \\(t^2\\) are correlated does not raise the problem of collinearity. One of the assumption of the multiple linear regression model is that there is no perfect linear relationship between the regressors. This is because the two variables are not a linear function of each other but rather a quadratic function. quadratic_trend &lt;- lm(RGDP$GDPCA ~ RGDP$TIME + I(RGDP$TIME^2)) quadratic_trend &lt;- lm(RGDP$GDPCA ~ RGDP$TIME + I(RGDP$TIME^2)) plot(RGDP$TIME, RGDP$GDPCA, type = &quot;l&quot;, col = &quot;black&quot;, main = &quot;The quadratic level-level trend model&quot;, xlab = &quot;Time&quot;, ylab = &quot;Real GDP&quot;, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;) lines(RGDP$TIME, quadratic_trend$fitted.values, col = &quot;green&quot;) stargazer(quadratic_trend, type = &quot;text&quot;, keep.stat = c(&quot;n&quot;, &quot;rsq&quot;)) ## ## ======================================== ## Dependent variable: ## --------------------------- ## GDPCA ## ---------------------------------------- ## TIME 72.987*** ## (9.011) ## ## TIME2) 3.468*** ## (0.178) ## ## Constant 3,369.931*** ## (95.719) ## ## ---------------------------------------- ## Observations 48 ## R2 0.996 ## ======================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 How can we interpret these results? In the multiple linear regression context, we interpret the partial effect of one regressor by differentiating the function with respect to that variable. In a linear model in levels, what we usually obtain is the coefficient (one of the betas) that multiply that variable. By doing so we are treating the other variables as constants. In this example, we have to differentiate with respect to \\(t\\) and we cannot treat \\(t^2\\) as a constant. \\[ \\frac{dRGDP}{dt} = \\beta_1 + 2 * \\beta_2 * t \\] \\[ \\frac{dRGDP}{dt} = 72.9873 + 2 * 3.4676 * t \\] We have obtained the rate of change in real GDP with respect to time. The rate of change in real GDP with respect to time is a positive function of time. As time increases, real GDP is increasing at an increasing rate. We can also notice that the rate of change in real GDP depends on time at which the rate of change is measured as opposed to the linear level-level model from above. Quadratic regression model (log-level) We can now consider the same model as before, but with the log of real GDP: \\[ ln(RGDP_t) = \\beta_0 + \\beta_1*t + \\beta_2*t^2 + u_t \\] quadratic_log_trend &lt;- lm(RGDP$lnGDPCA ~ RGDP$TIME + I(RGDP$TIME^2)) quadratic_log_trend &lt;- lm(RGDP$lnGDPCA ~ RGDP$TIME + I(RGDP$TIME^2)) plot(RGDP$TIME, RGDP$lnGDPCA, type = &quot;l&quot;, col = &quot;black&quot;, main = &quot;The quadratic log-level trend model&quot;, xlab = &quot;Time&quot;, ylab = &quot;Log real GDP&quot;, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;) lines(RGDP$TIME, quadratic_log_trend$fitted.values, col = &quot;purple&quot;) stargazer(quadratic_log_trend, type = &quot;text&quot;, keep.stat = c(&quot;n&quot;, &quot;rsq&quot;)) ## ## ======================================== ## Dependent variable: ## --------------------------- ## lnGDPCA ## ---------------------------------------- ## TIME 0.036*** ## (0.001) ## ## TIME2) -0.0001*** ## (0.00002) ## ## Constant 8.057*** ## (0.013) ## ## ---------------------------------------- ## Observations 48 ## R2 0.996 ## ======================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 What is the interpretation of what we have just obtained? \\[ \\frac{d ln(RGDP)}{dt} = \\beta_1 + 2 * \\beta_2 * t\\] \\[ \\frac{d ln(RGDP)}{dt} = 0.036 - 2 * 0.00007736 * t\\] On the left hand side of the derivative, we have the rate of growth in real GDP. From the estimated coefficients we see that real GDP is increasing at a decreasing rate. 9.2 The consumption function Estimation of the Keynesian consumption function In this brief exercise we are going to estimate a Keynesian aggregate consumption function for the United States from 1960 to 2019. We will regress private consumption (at constant prices) on real GDP. The slope of the regression will be what in the economic literature is called the marginal propensity to consume (MPC for short). What is MPC? MPC is that value that tells us how much consumption will increase as income increases. Theory tells us that the MPC is a value between 0 and 1. When income increases by 1$, consumption will increase by a value less than the initial increase of 1$ but greater than 0. From our Macroeconomics lessons we remember that the Keynesian consumption function is represented by the following equation, where \\(C\\) indicates the aggregate household consumption, \\(Y\\) the national income (both expressed in billions of constant dollars), \\(c_Y\\) is the MPC and \\(c_{aut}\\), autonomous consumption, i.e.Â the part of consumption that does not depend on income. \\[C = c_{aut} + c_Y Y\\] We first get the data using the rdbnomics package and then we will produce a graph of the time series from 1960 to 2019 with the ggplot2 package. Both time series are expressed in billions of 2015 dollars. # Private final consumption expenditure at 2015 prices df_cons &lt;- rdb(ids = &quot;AMECO/OCPH/USA.1.1.0.0.OCPH&quot;) %&gt;% select(original_period, value) %&gt;% rename(Year = original_period, cons = value) %&gt;% filter(Year &gt;= 1960 &amp; Year &lt;= 2019) # Gross domestic product at 2015 reference levels [OVGD] df_rGDP &lt;- rdb(ids = &quot;AMECO/OVGD/USA.1.1.0.0.OVGD&quot;) %&gt;% select(original_period, value) %&gt;% rename(Year = original_period, rGDP = value) %&gt;% filter(Year &gt;= 1960 &amp; Year &lt;= 2019) # Joining data df_final &lt;- df_cons %&gt;% left_join(df_rGDP, by = c(&quot;Year&quot;)) %&gt;% mutate(ldcons = c(NA, diff(log(cons))), ldrGDP = c(NA, diff(log(rGDP)))) # Preparing the data for the graphs df_graph_level &lt;- df_final %&gt;% pivot_longer(!Year, names_to = &quot;Variable&quot;, values_to = &quot;Value&quot;) %&gt;% filter(Variable %in% c(&quot;rGDP&quot;, &quot;cons&quot;)) %&gt;% arrange(Variable) We are now ready to perform the linear regression. We will also produce the scatter plot of the data to get the graphical intuition of the regression. From the linear regression, we obtained the MPC. If income increases by 1$, consumption will increase by approximately 70 cents on average. Once the MPC is obtained, we can calculate the value of the Keynesian multiplier (\\(m\\)) as learned in Macroeconomics classes. \\[m = \\frac{1}{1 - c_Y} = \\frac{1}{1 - 0.7} = 3.33\\] However, this regression has a number of problems from an econometric perspective.36 The two series have a tendency to grow over time and the variables might seem highly correlated to us because they both have the same tendency to increase with time. This could lead to the conclusion that there is a correlation when in fact there is not (not the case here). This problem is known as spurious regression. We need to see if the independent variable is correlated with the error term. This is certainly an undesirable behavior. The residuals must be completely random and contain no predictive power. We can observe a clear pattern between residuals and income. Our coefficients are biased. If we were in a cross-sectional context, we would have to figure out which variable is missing from the model and where possible include it. Or it could be that the model is misspecified. To get to the point, econometricians would say that our two series follow a unit root process. The series are highly persistent over time and contain a, in our case positive, time trend.37 Wanting to simplify a lot we can say that our time series are not stationary and must be transformed before being used in a regression. A quick fix that works is to use logarithmic differences.38 We can see that the estimate in the slope has changed slightly. This time it is somewhat higher at 0.77. However, the interpretation of the model has changed. This time we estimated an elasticity. How can we calculate the MPC having estimated the elasticity of consumption with respect to income? From courses in Microeconomics we remind that elasticity is nothing more than the ratio of two percentage rates and that on the demand curve elasticity varies depending on where you measure it.39 In our case, our elasticity parameter (just called \\(\\epsilon_Y\\) for simplicity) is given by: \\[ \\epsilon_Y = \\frac{\\frac{\\partial C}{C}}{\\frac{\\partial Y}{Y}} = \\frac{\\partial C}{\\partial Y} \\frac{Y}{C}\\] Rearranging the terms, we get: \\[\\frac{\\partial C}{\\partial Y} = \\epsilon_Y \\frac{C}{Y} \\] The elasticity parameter is equal to 0.77 while the C/Y term that we will calculate as the average over the entire period from 1960 to 2019 is equal to 0.64. The marginal effect, the MPC, calculated on the avare over the entire period is therefore 0.4928. How do the residuals perform this time? There are actually a whole host of econometric issues that we have left out but that need to be properly addressed when estimating a model. Is there serial correlation in the residuals and if so what is the consequence? Is there heteroschedasticity in the residuals and if so what problems could it cause? There are tests appropriately developed by statisticians to identify these problems and solutions to fix them. All of these things will be the subject of the course in the following semester. The value of \\(R^2\\) almost equal to one must immediately raise doubts. The two concepts, trending behaviour and persistent behaviour, should not be confused. Please refer to the Wooldridge textbook (Chapter 11). Differencing will removes the linear trend. For a review of the elasticity concept in economics, see here. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
