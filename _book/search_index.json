[["index.html", "Introduction to quantitative methods for economists About this book Thanks", " Introduction to quantitative methods for economists Alessandro Bramucci 22 Jun 2022 About this book This book brings together a collection of notes, exercises and practical applications that I have used over the years for my lectures in Quantitative Methods for Economists at the at the Berlin School of Economics and Law (HWR) in Berlin, Germany. Thanks This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["chapter1.html", "Chapter 1 Code 1", " Chapter 1 Code 1 "],["chapter2.html", "Chapter 2 Data extraction and manipulation using base R 2.1 Conditional selection", " Chapter 2 Data extraction and manipulation using base R To begin with, lets see how to install a new package in R. We are going to install the wooldridge package which contains all the datasets used in the book Introductory Econometrics by Jeffrey M. Wooldridge. Additional information regarding the datasets can be found in the following document. Now, let us see how to unload a package from the library. The quickest way is to untick the checkbox of the package you want to remove in the Packages window. The following code is automatically executed in the console. There is no need to copy it or run it again. Another, perhaps more radical, way to unload all packages is to restart the RStudio session. In this case, the keyboard shortcut CTRL + SHIFT + F10 can be used. Unloading a package After loading the dataset, in our case contained in the wooldridge package, we can take a look at the data. It is important to take a look at the data, for example after having modified existing variables or having created new ones. The dataset we use in this exercise is wage1. We can take a glance at the data using the View function. Indexing when you need to address a particular element into a vector, for example the wage of worker nr. 5 ## [1] 12 If we want data for more than one worker ## [1] 12 11 12 Data from worker one to worker five ## [1] 11 12 11 8 12 If we want to modify one particular observation Negative indexing allow to show all data except those specified in parenthesis ## [1] 12 16 10 15 16 14 Negative indexing allow us to drop specified rows and columns. Drop the first row in the dataset ## wage educ exper tenure nonwhite female married numdep smsa northcen south ## 2 3.24 12 22 2 0 1 1 3 1 0 0 ## 3 3.00 11 2 0 0 0 0 2 0 0 0 ## 4 6.00 8 44 28 0 0 1 0 1 0 0 ## 5 5.30 NA 7 2 0 0 1 1 0 0 0 ## 6 8.75 16 9 8 0 0 1 0 1 0 0 ## 7 11.25 18 15 7 0 0 0 0 1 0 0 ## 8 5.00 12 5 3 0 1 0 0 1 0 0 ## 9 3.60 12 26 4 0 1 0 2 1 0 0 ## 10 18.18 17 22 21 0 0 1 0 1 0 0 ## 11 6.25 16 8 2 0 1 0 0 1 0 0 ## 12 8.13 13 3 0 0 1 0 0 1 0 0 ## 13 8.77 12 15 0 0 0 1 2 1 0 0 ## 14 5.50 12 18 3 0 0 0 0 1 0 0 ## 15 22.20 12 31 15 0 0 1 1 1 0 0 ## 16 17.33 16 14 0 0 0 1 1 1 0 0 ## 17 7.50 12 10 0 0 1 1 0 1 0 0 ## 18 10.63 13 16 10 0 1 0 0 1 0 0 ## 19 3.60 12 13 0 0 1 1 3 1 0 0 ## 20 4.50 12 36 6 0 1 1 0 1 0 0 ## 21 6.88 12 11 4 0 1 0 0 1 0 0 ## 22 8.48 12 29 13 0 0 1 3 1 0 0 ## 23 6.33 16 9 9 0 1 0 0 1 0 0 ## 24 0.53 12 3 1 0 1 0 0 1 0 0 ## 25 6.00 11 37 8 1 1 0 0 1 0 0 ## 26 9.56 16 3 3 1 0 1 1 1 0 0 ## 27 7.78 16 11 10 0 0 1 1 1 0 0 ## 28 12.50 16 31 0 0 0 1 0 1 0 0 ## 29 12.50 15 30 0 0 0 1 2 1 0 0 ## 30 3.25 8 9 1 0 1 1 2 1 0 0 ## 31 13.00 14 23 5 0 0 1 2 1 0 0 ## 32 4.50 14 2 5 0 1 0 2 1 0 0 ## 33 9.68 13 16 16 0 1 0 0 1 0 0 ## 34 5.00 12 7 3 0 1 0 0 1 0 0 ## 35 4.68 12 3 0 0 1 0 0 1 0 0 ## 36 4.27 16 22 4 1 1 1 1 1 0 0 ## 37 6.15 12 15 6 1 1 1 1 1 0 0 ## 38 3.51 4 39 15 0 0 1 5 1 0 0 ## 39 3.00 14 3 3 0 1 1 0 1 0 0 ## 40 6.25 12 11 0 0 1 0 0 1 0 0 ## 41 7.81 12 3 0 0 1 1 0 1 0 0 ## 42 10.00 12 20 5 1 1 1 4 1 0 0 ## 43 4.50 14 16 0 0 1 1 1 1 0 0 ## 44 4.00 11 45 12 0 1 1 0 1 0 0 ## 45 6.38 13 11 4 0 1 0 0 1 0 0 ## 46 13.70 15 20 13 0 0 1 2 1 0 0 ## 47 1.67 10 1 0 0 0 0 1 1 0 0 ## 48 2.93 12 36 2 0 1 1 1 1 0 0 ## 49 3.65 14 9 2 0 0 0 0 1 0 0 ## 50 2.90 12 15 1 0 1 1 2 1 0 0 ## 51 1.63 12 18 0 0 1 0 2 0 0 0 ## 52 8.60 16 3 2 0 1 0 0 1 0 0 ## 53 5.00 12 15 5 0 0 1 1 1 0 0 ## 54 6.00 12 7 7 0 0 1 0 1 0 0 ## 55 2.50 12 2 0 0 0 0 2 0 0 0 ## 56 3.25 15 3 0 0 0 0 1 0 0 0 ## 57 3.40 16 1 1 0 1 0 1 0 0 0 ## 58 10.00 8 13 0 1 0 0 0 1 0 0 ## 59 21.63 18 8 8 0 1 0 0 1 0 0 ## 60 4.38 16 7 0 0 0 0 0 1 0 0 ## 61 11.71 13 40 20 0 1 0 0 1 0 0 ## 62 12.39 14 42 5 0 0 0 0 1 0 0 ## 63 6.25 10 36 8 0 0 0 0 1 0 0 ## 64 3.71 10 13 0 1 1 0 4 1 0 0 ## 65 7.78 14 9 3 0 0 0 0 1 0 0 ## 66 19.98 14 26 23 0 0 1 2 1 0 0 ## 67 6.25 16 7 4 1 1 1 3 1 0 0 ## 68 10.00 12 25 3 0 0 1 3 1 0 0 ## 69 5.71 16 10 5 0 0 1 1 1 0 0 ## 70 2.00 12 3 2 0 1 0 4 1 0 0 ## 71 5.71 16 3 0 0 1 0 0 1 0 0 ## 72 13.08 17 17 2 1 0 1 3 1 0 0 ## 73 4.91 12 17 8 0 0 1 2 1 0 0 ## 74 2.91 12 20 34 0 1 1 2 1 0 0 ## 75 3.75 12 7 0 0 1 1 0 1 0 0 ## 76 11.90 13 24 19 0 0 1 2 1 0 0 ## 77 4.00 12 28 0 0 1 1 1 1 0 0 ## 78 3.10 12 2 1 0 1 0 1 1 0 0 ## 79 8.45 12 19 13 0 0 1 4 1 0 0 ## 80 7.14 18 13 0 0 0 1 2 1 0 0 ## 81 4.50 9 22 5 0 0 1 4 1 0 0 ## 82 4.65 16 3 1 0 0 0 0 1 0 0 ## 83 2.90 10 4 0 0 1 0 1 1 0 0 ## 84 6.67 12 7 5 0 0 0 0 1 0 0 ## 85 3.50 12 6 2 1 1 0 1 1 0 0 ## 86 3.26 12 13 3 0 1 1 1 1 0 0 ## 87 3.25 12 14 0 0 1 1 1 1 0 0 ## 88 8.00 12 14 4 0 1 0 1 1 0 0 ## 89 9.85 8 40 24 1 0 1 2 1 0 0 ## 90 7.50 12 11 7 0 0 1 1 1 0 0 ## 91 5.91 12 14 6 0 0 1 2 0 0 0 ## 92 11.76 14 40 39 0 0 1 0 1 0 0 ## 93 3.00 12 1 0 0 1 0 2 1 0 0 ## 94 4.81 12 2 0 0 1 0 0 1 0 0 ## 95 6.50 12 4 1 0 1 0 0 1 0 0 ## 96 4.00 9 19 1 0 1 1 1 1 0 0 ## 97 3.50 13 1 0 0 0 0 1 1 0 0 ## 98 13.16 12 34 22 0 0 1 0 1 0 0 ## 99 4.25 14 5 2 0 0 1 2 1 0 0 ## 100 3.50 12 3 0 0 0 0 0 1 0 0 ## 101 5.13 15 6 6 0 1 0 0 1 0 0 ## 102 3.75 12 14 0 0 1 1 3 1 0 0 ## 103 4.50 12 35 12 0 1 1 0 1 0 0 ## 104 7.63 12 8 4 0 1 0 0 1 0 0 ## 105 15.00 14 7 7 1 0 1 1 1 0 0 ## 106 6.85 15 11 3 0 1 1 2 1 0 0 ## 107 13.33 12 14 11 0 0 1 2 1 0 0 ## 108 6.67 12 35 10 0 0 0 0 1 0 0 ## 109 2.53 12 46 0 0 1 0 0 1 0 0 ## 110 9.80 17 7 0 0 0 1 0 1 0 0 ## 111 3.37 11 45 12 0 0 1 0 1 0 0 ## 112 24.98 18 29 25 0 0 1 0 1 0 0 ## 113 5.40 12 6 3 0 0 1 0 1 0 0 ## 114 6.11 14 15 0 0 0 1 2 1 0 0 ## 115 4.20 14 33 16 0 1 1 0 1 0 0 ## 116 3.75 10 15 0 0 0 0 0 1 0 0 ## 117 3.50 14 5 0 0 1 1 0 0 0 1 ## 118 3.64 12 7 2 0 0 0 0 1 0 1 ## 119 3.80 15 6 1 0 0 1 1 1 0 1 ## 120 3.00 8 33 12 0 1 1 3 0 0 1 ## 121 5.00 16 2 1 0 1 1 0 0 0 1 ## 122 4.63 14 4 0 0 1 1 2 1 0 1 ## 123 3.00 15 1 0 0 0 0 0 1 0 1 ## 124 3.20 12 29 0 0 1 0 1 1 0 1 ## 125 3.91 18 17 3 0 0 1 2 0 0 1 ## 126 6.43 16 17 3 0 1 1 2 0 0 1 ## 127 5.48 10 36 3 0 1 1 0 1 0 1 ## 128 1.50 8 31 30 0 0 0 0 0 0 1 ## 129 2.90 10 23 2 0 1 0 2 0 0 1 ## 130 5.00 11 13 1 0 0 1 0 0 0 1 ## 131 8.92 18 3 3 0 0 1 0 1 0 1 ## 132 5.00 15 15 0 0 0 0 0 1 0 1 ## 133 3.52 12 48 1 0 0 1 0 0 0 1 ## 134 2.90 11 6 0 0 1 0 1 0 0 1 ## 135 4.50 12 12 0 0 0 1 3 1 0 1 ## 136 2.25 12 5 0 0 1 0 2 1 0 1 ## 137 5.00 14 19 5 0 0 1 4 1 0 1 ## 138 10.00 16 9 3 1 0 1 2 1 0 1 ## 139 3.75 2 39 13 0 0 1 0 1 0 1 ## 140 10.00 14 28 11 0 1 1 0 1 0 1 ## 141 10.95 16 23 20 0 0 1 4 1 0 1 ## 142 7.90 12 2 0 0 0 0 0 1 0 1 ## 143 4.72 12 15 1 0 1 0 0 1 0 1 ## 144 5.84 13 5 0 0 1 1 1 1 0 1 ## 145 3.83 12 18 2 0 1 0 3 1 0 1 ## 146 3.20 15 2 2 0 1 0 2 1 0 1 ## 147 2.00 10 3 0 1 1 0 5 1 0 1 ## 148 4.50 12 31 4 0 1 1 3 1 0 1 ## 149 11.55 16 20 5 0 1 1 3 1 0 1 ## 150 2.14 13 34 15 1 1 1 0 1 0 1 ## 151 2.38 9 5 0 1 0 0 5 1 0 1 ## 152 3.75 12 11 0 0 0 0 1 1 0 1 ## 153 5.52 13 31 3 0 0 0 0 1 0 1 ## 154 6.50 12 8 5 0 1 1 0 1 0 1 ## 155 3.10 12 2 2 0 1 0 1 1 0 1 ## 156 10.00 14 18 5 0 0 1 2 1 0 1 ## 157 6.63 16 3 0 0 0 1 0 1 0 1 ## 158 10.00 16 3 2 0 0 1 0 1 0 1 ## 159 2.31 9 4 1 0 1 0 5 0 1 0 ## 160 6.88 18 4 4 0 0 0 0 1 1 0 ## 161 2.83 10 1 0 0 0 0 4 1 1 0 ## 162 3.13 10 1 0 0 1 0 1 1 1 0 ## 163 8.00 13 28 5 0 0 1 1 1 1 0 ## 164 4.50 12 47 4 0 1 1 0 1 1 0 ## 165 8.65 18 13 1 0 1 0 0 1 1 0 ## 166 2.00 13 2 6 0 1 0 0 1 1 0 ## 167 4.75 12 48 2 0 1 1 0 1 1 0 ## 168 6.25 13 6 5 0 1 1 1 1 1 0 ## 169 6.00 13 8 0 0 0 1 2 1 1 0 ## 170 15.38 13 25 21 0 0 1 2 1 1 0 ## 171 14.58 18 13 7 0 1 0 0 1 1 0 ## 172 12.50 12 8 1 0 0 0 0 1 1 0 ## 173 5.25 12 19 10 0 1 1 2 1 1 0 ## 174 2.17 13 1 4 0 1 0 1 1 1 0 ## 175 7.14 12 43 5 0 1 0 0 1 1 0 ## 176 6.22 12 19 9 0 1 1 1 1 1 0 ## 177 9.00 12 11 5 0 1 1 0 1 1 0 ## 178 10.00 14 43 4 0 0 1 0 1 1 0 ## 179 5.77 10 44 3 0 0 1 0 1 1 0 ## 180 4.00 12 22 11 0 1 1 2 1 1 0 ## 181 8.75 16 3 2 0 0 1 1 1 1 0 ## 182 6.53 16 3 2 0 1 0 0 1 1 0 ## 183 7.60 12 41 11 0 1 0 0 1 1 0 ## 184 5.00 14 5 0 0 0 0 0 1 1 0 ## 185 5.00 12 14 11 0 1 0 0 1 1 0 ## 186 21.86 12 24 16 0 0 1 3 1 1 0 ## 187 8.64 12 28 8 0 0 1 0 1 1 0 ## 188 3.30 12 25 8 0 0 1 1 1 1 0 ## 189 4.44 12 3 0 0 0 0 0 1 1 0 ## 190 4.55 12 11 0 0 0 1 0 0 1 0 ## 191 3.50 12 7 6 1 1 1 0 0 1 0 ## 192 6.25 16 9 2 0 0 1 1 0 1 0 ## 193 3.85 16 5 0 0 0 1 0 1 1 0 ## 194 6.18 14 9 3 0 1 1 0 1 1 0 ## 195 2.91 11 1 0 0 1 0 3 1 1 0 ## 196 6.25 16 2 1 1 1 0 0 1 1 0 ## 197 6.25 12 13 0 0 1 1 0 1 1 0 ## 198 9.05 12 10 2 0 0 1 3 1 1 0 ## 199 10.00 17 5 3 0 0 1 0 1 1 0 ## 200 11.11 12 30 8 0 0 1 0 1 1 0 ## 201 6.88 12 31 19 0 0 1 3 1 1 0 ## 202 8.75 16 1 2 1 0 0 0 1 1 0 ## 203 10.00 8 9 0 0 0 1 0 1 1 0 ## 204 3.05 12 10 0 0 1 1 2 1 1 0 ## 205 3.00 12 38 0 0 1 1 0 0 1 0 ## 206 5.80 12 19 6 0 1 1 2 0 1 0 ## 207 4.10 16 5 0 0 1 1 0 1 1 0 ## 208 8.00 12 26 2 0 0 1 1 1 1 0 ## 209 6.15 12 35 12 0 1 0 0 1 1 0 ## 210 2.70 9 2 0 0 1 0 1 1 1 0 ## 211 2.75 13 1 2 0 1 0 1 1 1 0 ## 212 3.00 16 19 10 0 1 1 0 1 1 0 ## 213 3.00 14 3 2 0 1 0 0 1 1 0 ## 214 7.36 8 36 24 0 0 1 3 1 1 0 ## 215 7.50 14 29 24 0 0 1 1 1 1 0 ## 216 3.50 13 1 2 0 0 0 3 1 1 0 ## 217 8.10 12 38 3 0 1 1 1 1 1 0 ## 218 3.75 18 1 2 0 0 0 1 1 1 0 ## 219 3.25 9 29 0 1 0 1 1 1 1 0 ## 220 5.83 8 36 15 0 1 1 0 0 0 0 ## 221 3.50 8 4 0 0 0 0 3 1 0 0 ## 222 3.33 12 45 4 0 1 1 0 0 0 0 ## 223 4.00 14 22 3 0 1 0 0 1 0 0 ## 224 3.50 12 20 4 0 1 1 2 0 0 0 ## 225 6.25 16 5 0 0 0 1 0 1 0 0 ## 226 2.95 8 15 2 1 1 1 1 1 0 0 ## 227 5.71 13 10 2 0 1 1 0 1 0 0 ## 228 3.00 9 3 0 0 1 0 0 1 0 0 ## 229 22.86 16 16 7 0 0 1 2 1 0 0 ## 230 9.00 12 38 1 0 0 1 0 1 0 0 ## 231 8.33 15 33 26 0 0 1 1 1 0 0 ## 232 3.00 11 2 0 0 0 0 0 1 0 0 ## 233 5.75 14 6 5 0 0 1 0 1 0 0 ## 234 6.76 12 19 3 1 0 1 2 1 0 0 ## 235 10.00 12 29 0 0 0 1 2 1 0 0 ## 236 3.00 12 2 0 0 0 0 0 1 0 0 ## 237 3.50 18 3 1 0 1 0 0 1 0 0 ## 238 3.25 12 4 0 0 0 0 0 1 0 0 ## 239 4.00 12 10 1 1 1 0 0 1 0 0 ## 240 2.92 12 4 0 0 1 0 1 0 0 1 ## 241 3.06 12 14 10 1 1 0 3 0 0 1 ## 242 3.20 12 15 5 0 1 1 1 0 0 1 ## 243 4.75 12 19 0 0 0 1 3 0 0 1 ## 244 3.00 14 17 0 0 1 1 4 1 0 1 ## 245 18.16 16 29 7 0 0 1 1 1 0 1 ## 246 3.50 12 2 0 0 1 0 2 1 0 1 ## 247 4.11 14 5 0 0 0 0 0 0 0 1 ## 248 1.96 11 38 3 1 1 0 0 0 0 1 ## 249 4.29 12 3 0 0 1 0 0 0 0 1 ## 250 3.00 10 47 0 0 0 0 0 0 0 1 ## 251 6.45 12 7 6 0 0 1 0 0 0 1 ## 252 5.20 6 47 13 1 0 1 0 1 0 1 ## 253 4.50 13 23 2 1 1 0 0 1 0 1 ## 254 3.88 12 12 3 0 0 1 3 1 0 1 ## 255 3.45 10 11 0 0 1 0 2 1 0 1 ## 256 10.91 12 25 23 0 0 0 0 1 0 1 ## 257 4.10 14 6 0 0 1 1 0 1 0 1 ## 258 3.00 13 3 1 0 0 0 0 1 0 1 ## 259 5.90 12 14 7 1 0 1 2 1 0 1 ## 260 18.00 18 13 0 0 1 0 0 1 0 1 ## 261 4.00 12 9 0 0 0 0 0 1 0 1 ## 262 3.00 12 1 0 1 0 0 0 1 0 1 ## 263 3.55 12 6 0 0 1 1 1 0 0 0 ## 264 3.00 12 11 1 0 1 1 2 0 0 0 ## 265 8.75 12 47 44 0 0 1 0 0 0 0 ## 266 2.90 8 49 6 0 1 0 1 0 0 0 ## 267 6.26 13 37 17 0 1 1 0 1 0 0 ## 268 3.50 13 2 0 0 1 0 0 1 0 0 ## 269 4.60 14 7 0 0 1 1 0 1 0 0 ## 270 6.00 12 22 8 0 0 0 2 1 0 0 ## 271 2.89 10 8 0 0 1 0 1 1 0 0 ## 272 5.58 16 1 1 0 0 1 0 1 0 0 ## 273 4.00 12 43 6 0 1 1 0 0 0 0 ## 274 6.00 16 2 2 0 0 0 0 1 0 0 ## 275 4.50 12 2 1 0 1 0 2 1 0 0 ## 276 2.92 14 1 3 0 0 0 0 1 0 0 ## 277 4.33 18 1 0 0 0 0 0 1 0 0 ## 278 18.89 17 26 20 0 0 1 2 1 0 0 ## 279 4.28 13 1 1 0 1 0 2 1 0 0 ## 280 4.57 14 37 7 0 1 1 0 1 0 0 ## 281 6.25 15 12 4 0 1 1 2 1 0 0 ## 282 2.95 14 41 23 1 0 1 0 0 1 0 ## 283 8.75 12 24 1 0 0 0 0 1 1 0 ## 284 8.50 8 38 26 0 0 0 0 1 1 0 ## 285 3.75 12 18 0 0 1 1 4 1 1 0 ## 286 3.15 12 26 1 0 0 1 0 1 1 0 ## 287 5.00 8 45 2 0 0 0 0 0 1 0 ## 288 6.46 12 27 0 0 0 1 3 0 1 0 ## 289 2.00 9 2 0 0 0 0 3 0 1 0 ## 290 4.79 12 41 8 0 0 1 0 1 1 0 ## 291 5.78 16 11 4 0 0 1 2 1 1 0 ## 292 3.18 12 5 0 0 1 1 0 1 1 0 ## 293 4.68 16 3 1 0 1 0 0 1 1 0 ## 294 4.10 12 3 2 0 1 0 0 1 1 0 ## 295 2.91 12 4 0 0 0 1 0 1 0 1 ## 296 6.00 13 21 13 0 0 1 4 0 0 1 ## 297 3.60 10 34 26 0 1 1 0 0 0 1 ## 298 3.95 6 49 6 0 0 1 6 0 0 1 ## 299 7.00 12 6 5 1 0 1 1 0 0 1 ## 300 3.00 12 26 9 0 1 1 0 0 0 1 ## 301 6.08 16 9 0 0 0 0 0 0 0 1 ## 302 8.63 12 23 9 0 0 1 1 0 0 1 ## 303 3.00 8 33 2 0 0 1 0 0 0 1 ## 304 3.75 12 5 2 0 1 1 1 0 0 1 ## 305 2.90 6 49 7 0 0 1 0 0 0 1 ## 306 3.00 4 48 0 1 0 1 0 0 0 1 ## 307 6.25 11 35 31 0 0 1 0 1 0 1 ## 308 3.50 11 23 2 1 1 0 2 0 0 1 ## 309 3.00 7 26 1 0 1 0 3 0 0 1 ## 310 3.24 12 16 0 0 1 1 2 1 0 1 ## 311 8.02 18 23 3 0 1 1 1 1 0 1 ## 312 3.33 12 36 8 0 1 1 1 1 0 1 ## 313 5.25 16 4 0 0 0 0 0 1 0 1 ## 314 6.25 12 10 0 0 0 1 3 1 0 1 ## 315 3.50 14 18 2 0 0 1 1 0 1 0 ## 316 2.95 12 3 1 0 0 0 1 0 1 0 ## 317 3.00 10 7 0 0 1 1 2 0 1 0 ## 318 4.69 10 7 7 0 0 0 0 0 1 0 ## 319 3.73 9 33 2 0 1 0 1 0 1 0 ## 320 4.00 10 34 12 0 0 1 0 0 1 0 ## 321 4.00 12 8 0 0 1 1 2 0 1 0 ## 322 2.90 12 17 1 0 1 1 2 0 1 0 ## 323 3.05 12 2 0 0 1 0 1 0 1 0 ## 324 5.05 10 5 0 0 1 0 0 1 1 0 ## 325 13.95 16 41 16 0 0 1 0 1 1 0 ## 326 18.16 16 35 28 0 0 1 1 1 1 0 ## 327 6.25 16 11 4 0 0 0 0 1 1 0 ## 328 5.25 12 4 0 1 0 0 0 1 1 0 ## 329 4.79 12 12 3 0 1 1 3 1 1 0 ## 330 3.35 7 35 0 0 1 0 1 1 0 1 ## 331 3.00 8 33 0 0 0 1 1 1 0 1 ## 332 8.43 16 8 6 0 0 1 0 1 0 1 ## 333 5.70 16 2 0 0 0 0 0 1 0 1 ## 334 11.98 18 8 10 0 0 1 2 1 0 1 ## 335 3.50 13 29 1 0 1 1 0 0 0 1 ## 336 4.24 10 14 5 1 0 1 1 0 0 1 ## 337 7.00 16 26 3 0 1 1 1 1 0 1 ## 338 6.00 14 11 3 0 1 1 2 1 0 1 ## 339 12.22 16 10 2 0 0 0 0 1 0 1 ## 340 4.50 12 13 0 0 0 1 0 1 0 1 ## 341 3.00 9 23 20 0 1 1 2 0 0 1 ## 342 2.90 11 1 2 1 0 0 2 0 0 1 ## 343 15.00 11 35 31 0 0 1 0 0 0 1 ## 344 4.00 12 5 2 0 0 1 0 0 0 1 ## 345 5.25 11 13 11 0 0 1 2 0 0 1 ## 346 4.00 12 22 3 0 0 1 3 0 0 1 ## 347 3.30 12 21 9 0 1 1 2 0 0 1 ## 348 5.05 12 19 0 0 1 1 3 0 0 1 ## 349 3.58 12 13 0 0 0 0 0 1 0 1 ## 350 5.00 14 15 5 0 0 1 2 0 0 1 ## 351 4.57 14 3 0 0 0 1 0 0 0 1 ## 352 12.50 18 6 2 0 0 0 0 1 0 1 ## 353 3.45 12 6 5 0 1 1 0 1 0 1 ## 354 4.63 12 16 1 0 1 0 0 1 0 1 ## 355 10.00 12 31 2 0 0 1 0 1 0 1 ## 356 2.92 11 1 0 0 1 0 1 0 1 0 ## 357 4.51 12 5 2 0 0 0 0 1 1 0 ## 358 6.50 17 3 0 0 0 1 0 1 1 0 ## 359 7.50 16 11 0 0 0 1 0 1 1 0 ## 360 3.54 13 6 7 0 1 1 0 0 1 0 ## 361 4.20 13 11 3 0 1 1 2 1 1 0 ## 362 3.51 12 7 2 0 1 1 0 1 1 0 ## 363 4.50 14 5 0 0 0 1 0 1 1 0 ## 364 3.35 14 5 4 0 1 1 0 1 1 0 ## 365 2.91 11 2 2 0 0 0 2 1 1 0 ## 366 5.25 10 44 7 0 0 1 1 1 1 0 ## 367 4.05 8 44 25 0 0 1 0 1 1 0 ## 368 3.75 14 13 0 0 1 1 3 1 1 0 ## 369 3.40 12 26 15 0 1 1 1 1 1 0 ## 370 3.00 10 2 1 0 1 0 1 1 0 1 ## 371 6.29 17 10 3 0 0 1 1 1 0 1 ## 372 2.54 9 2 0 0 1 0 1 1 0 1 ## 373 4.50 12 35 0 0 1 1 1 1 0 1 ## 374 3.13 12 6 5 0 1 1 1 1 0 1 ## 375 6.36 14 8 1 0 0 1 0 1 0 1 ## 376 4.68 16 1 0 0 0 0 0 1 0 1 ## 377 6.80 12 14 10 0 0 1 2 0 0 1 ## 378 8.53 10 14 6 0 0 1 0 0 0 1 ## 379 4.17 0 22 10 0 1 0 0 0 0 1 ## 380 3.75 14 8 4 0 1 1 0 0 0 1 ## 381 11.10 15 1 4 0 1 0 2 0 0 1 ## 382 3.26 16 15 5 1 1 1 2 1 0 1 ## 383 9.13 12 14 12 0 0 1 3 1 0 1 ## 384 4.50 11 37 10 0 0 1 2 1 0 1 ## 385 3.00 11 1 1 0 1 0 1 1 0 1 ## 386 8.75 12 4 4 0 0 1 0 1 0 1 ## 387 4.14 13 29 0 0 1 1 0 1 0 1 ## 388 2.87 12 45 8 0 1 1 0 0 1 0 ## 389 3.35 13 22 0 0 1 0 2 0 1 0 ## 390 6.08 16 42 10 0 0 0 0 0 1 0 ## 391 3.00 15 9 0 1 0 0 0 0 1 0 ## 392 4.20 16 8 0 0 1 1 1 1 1 0 ## 393 5.60 15 31 15 0 1 0 2 1 1 0 ## 394 10.00 12 24 24 0 0 1 0 1 1 0 ## 395 12.50 18 16 5 0 0 1 1 1 1 0 ## 396 3.76 6 6 0 0 0 0 4 0 0 1 ## 397 3.10 6 14 0 1 1 0 5 1 0 1 ## 398 4.29 12 47 25 0 1 1 2 1 0 1 ## 399 10.92 12 34 5 0 0 1 0 0 0 1 ## 400 7.50 16 6 2 0 1 1 1 1 0 1 ## 401 4.05 9 7 4 0 1 1 1 1 0 1 ## 402 4.65 12 27 2 0 0 1 3 1 0 1 ## 403 5.00 11 24 5 0 0 1 0 1 0 1 ## 404 2.90 10 18 0 0 1 1 2 0 0 1 ## 405 8.00 12 12 3 1 1 0 1 0 0 1 ## 406 8.43 8 27 3 0 0 1 3 1 0 1 ## 407 2.92 9 49 0 0 1 0 0 1 0 1 ## 408 6.25 17 4 0 0 1 0 0 1 0 0 ## 409 6.25 16 24 2 0 1 1 2 1 0 0 ## 410 5.11 11 3 0 0 0 0 0 1 0 0 ## 411 4.00 10 2 0 0 1 0 1 1 0 0 ## 412 4.44 8 29 11 0 0 1 3 0 0 1 ## 413 6.88 13 34 21 0 0 1 0 0 0 1 ## 414 5.43 14 10 3 1 0 1 1 1 0 1 ## 415 3.00 13 5 0 0 1 0 1 1 0 1 ## 416 2.90 11 2 0 0 1 0 2 1 0 1 ## 417 6.25 7 39 21 1 0 1 0 1 0 1 ## 418 4.34 16 5 2 1 1 0 0 1 0 1 ## 419 3.25 12 14 2 0 1 1 2 0 0 1 ## 420 7.26 13 8 2 0 0 0 0 1 0 1 ## 421 6.35 14 10 1 0 1 1 2 1 0 1 ## 422 5.63 16 2 2 0 0 1 0 1 0 0 ## 423 8.75 14 9 3 0 0 1 1 1 0 0 ## 424 3.20 11 1 0 0 0 0 0 0 0 0 ## 425 3.00 8 45 1 0 1 1 0 0 0 0 ## 426 3.00 14 33 3 0 1 1 0 1 0 0 ## 427 12.50 17 21 18 0 0 1 3 1 0 0 ## 428 2.88 10 2 0 0 1 0 3 1 0 0 ## 429 3.35 12 9 1 0 0 1 0 1 1 0 ## 430 6.50 12 33 2 0 0 1 0 0 1 0 ## 431 10.38 18 16 2 1 0 1 1 1 1 0 ## 432 4.50 14 10 0 0 0 1 0 1 1 0 ## 433 10.00 18 9 8 0 0 0 0 0 1 0 ## 434 3.81 12 8 1 0 1 1 2 0 1 0 ## 435 8.80 16 9 1 0 0 1 0 0 0 1 ## 436 9.42 14 23 0 0 1 1 2 0 0 1 ## 437 6.33 12 23 8 0 0 1 2 0 0 1 ## 438 4.00 9 22 18 1 0 0 0 0 0 1 ## 439 2.90 12 37 0 0 1 1 0 0 0 1 ## 440 20.00 12 22 4 0 0 1 1 0 0 1 ## 441 11.25 17 28 25 0 0 1 1 1 0 1 ## 442 3.50 12 14 0 1 1 0 2 1 0 1 ## 443 6.00 15 19 4 0 1 1 2 1 0 1 ## 444 14.38 17 10 9 1 0 1 1 0 0 1 ## 445 6.36 16 25 0 0 0 1 1 1 0 1 ## 446 3.55 12 21 0 0 1 1 1 0 0 1 ## 447 3.00 15 32 0 0 0 1 0 1 0 1 ## 448 4.50 16 21 10 0 0 1 1 1 0 1 ## 449 6.63 12 36 0 0 1 0 0 1 0 1 ## 450 9.30 15 2 2 0 0 1 0 1 0 1 ## 451 3.00 12 11 0 0 1 1 2 1 0 1 ## 452 3.25 12 40 2 0 1 0 0 0 1 0 ## 453 1.50 12 11 1 0 1 1 2 0 1 0 ## 454 5.90 12 9 7 0 1 1 1 1 1 0 ## 455 8.00 16 23 4 0 0 0 0 1 1 0 ## 456 2.90 11 1 0 0 1 0 1 1 1 0 ## 457 3.29 14 30 13 0 0 0 0 1 1 0 ## 458 6.50 14 41 33 0 0 1 0 1 1 0 ## 459 4.00 13 6 0 0 1 0 1 1 1 0 ## 460 6.00 14 11 0 0 0 1 0 0 1 0 ## 461 4.08 12 43 17 0 1 1 0 1 1 0 ## 462 3.75 12 39 2 0 1 1 0 1 1 0 ## 463 3.05 8 50 24 0 1 0 0 0 0 1 ## 464 3.50 12 26 20 0 1 1 0 0 0 1 ## 465 2.92 3 51 30 1 0 0 0 1 0 1 ## 466 4.50 11 3 9 0 0 0 0 1 0 1 ## 467 3.35 15 3 1 0 1 1 2 1 0 1 ## 468 5.95 11 15 9 1 0 1 1 0 0 1 ## 469 8.00 12 17 6 0 0 1 2 0 0 1 ## 470 3.00 4 36 0 0 0 1 1 0 0 1 ## 471 5.00 9 31 9 1 0 1 6 0 0 1 ## 472 5.50 12 9 4 0 0 1 1 0 0 1 ## 473 2.65 12 42 10 0 1 1 0 1 0 0 ## 474 3.00 11 3 0 0 1 0 2 1 0 0 ## 475 4.50 12 37 14 0 1 1 0 0 0 0 ## 476 17.50 16 23 22 0 0 1 3 1 0 0 ## 477 8.18 13 21 5 0 0 0 0 1 0 0 ## 478 9.09 15 11 12 0 0 1 1 1 0 0 ## 479 11.82 16 35 13 0 0 1 0 1 0 0 ## 480 3.25 12 42 0 0 1 1 0 1 0 0 ## 481 4.50 12 3 0 0 0 0 0 1 0 0 ## 482 4.50 12 13 0 0 0 1 0 1 0 0 ## 483 3.71 9 14 7 0 0 1 2 0 0 1 ## 484 6.50 10 14 11 0 0 1 3 0 0 1 ## 485 2.90 12 39 1 0 1 1 0 0 0 1 ## 486 5.60 11 11 8 0 0 1 2 0 0 1 ## 487 2.23 8 28 3 0 0 1 4 0 0 1 ## 488 5.00 6 18 0 0 1 1 3 1 0 0 ## 489 8.33 16 6 2 0 0 1 0 1 0 0 ## 490 2.90 12 26 1 0 1 0 4 1 0 1 ## 491 6.25 12 21 6 0 0 1 3 1 0 1 ## 492 4.55 16 34 2 0 0 1 0 0 0 1 ## 493 3.28 12 17 2 0 1 0 0 0 0 1 ## 494 2.30 10 2 0 0 1 0 4 1 1 0 ## 495 3.30 13 5 0 0 1 0 0 1 1 0 ## 496 3.15 13 1 0 0 1 0 4 1 1 0 ## 497 12.50 14 40 30 0 0 1 0 1 1 0 ## 498 5.15 16 39 21 0 1 0 0 1 0 0 ## 499 3.13 10 1 1 0 0 0 2 1 0 0 ## 500 7.25 12 14 5 0 0 1 3 1 0 0 ## 501 2.90 12 2 2 0 1 0 0 1 0 0 ## 502 1.75 11 2 1 0 0 0 2 1 0 0 ## 503 2.89 0 42 0 0 1 1 2 0 0 0 ## 504 2.90 5 34 0 0 1 1 5 0 0 0 ## 505 17.71 16 10 3 0 0 1 1 1 0 0 ## 506 6.25 16 4 3 0 0 1 0 0 0 0 ## 507 2.60 9 4 0 0 0 0 1 0 0 0 ## 508 6.63 15 21 3 0 1 1 2 0 0 0 ## 509 3.50 12 31 3 1 1 0 1 1 0 0 ## 510 6.50 12 20 14 0 0 1 3 1 0 0 ## 511 3.00 12 36 1 1 1 0 0 1 0 0 ## 512 4.38 13 7 0 1 0 1 1 1 0 0 ## 513 10.00 12 15 0 0 0 0 1 0 0 0 ## 514 4.95 7 25 17 0 0 1 5 0 0 0 ## 515 9.00 17 7 0 0 1 1 2 0 0 0 ## 516 1.43 12 17 0 0 1 1 2 0 0 0 ## 517 3.08 12 3 1 0 0 0 0 0 0 0 ## 518 9.33 14 12 11 0 0 1 3 0 0 0 ## 519 7.50 12 18 5 0 0 1 2 0 0 0 ## 520 4.75 13 47 1 0 0 1 0 0 0 0 ## 521 5.65 12 2 0 0 0 0 0 0 0 0 ## 522 15.00 16 14 2 0 1 1 2 0 0 0 ## 523 2.27 10 2 0 0 1 0 3 0 0 0 ## 524 4.67 15 13 18 0 0 1 3 0 0 0 ## 525 11.56 16 5 1 0 0 1 0 0 0 0 ## 526 3.50 14 5 4 1 1 0 2 0 0 0 ## west construc ndurman trcommpu trade services profserv profocc clerocc ## 2 1 0 0 0 0 1 0 0 0 ## 3 1 0 0 0 1 0 0 0 0 ## 4 1 0 0 0 0 0 0 0 1 ## 5 1 0 0 0 0 0 0 0 0 ## 6 1 0 0 0 0 0 1 1 0 ## 7 1 0 0 0 1 0 0 1 0 ## 8 1 0 0 0 0 0 0 1 0 ## 9 1 0 0 0 1 0 0 1 0 ## 10 1 0 0 0 0 0 0 1 0 ## 11 1 0 0 0 1 0 0 1 0 ## 12 1 0 0 0 0 0 1 0 1 ## 13 1 0 0 0 0 0 0 0 0 ## 14 1 0 0 0 0 0 0 0 0 ## 15 1 0 0 0 0 0 0 1 0 ## 16 1 0 0 0 0 0 1 1 0 ## 17 1 0 0 0 0 0 0 0 1 ## 18 1 0 0 0 0 0 1 1 0 ## 19 1 0 0 0 0 0 0 0 0 ## 20 1 0 0 0 0 0 1 0 0 ## 21 1 0 0 0 0 0 1 1 0 ## 22 1 0 0 0 0 0 0 0 0 ## 23 1 0 0 0 0 0 0 1 0 ## 24 1 0 0 0 0 1 0 0 0 ## 25 1 0 0 0 0 1 0 0 0 ## 26 1 0 1 0 0 0 0 1 0 ## 27 1 0 0 1 0 0 0 0 0 ## 28 1 0 0 0 0 0 0 1 0 ## 29 1 0 0 0 0 0 0 1 0 ## 30 1 0 0 0 0 0 0 0 0 ## 31 1 0 0 0 0 0 1 1 0 ## 32 1 0 0 0 1 0 0 0 0 ## 33 1 0 0 0 0 0 0 1 0 ## 34 1 0 0 0 0 0 0 0 1 ## 35 1 0 0 0 0 1 0 0 1 ## 36 1 0 0 0 1 0 0 1 0 ## 37 1 0 0 0 0 1 0 0 0 ## 38 1 0 0 0 1 0 0 0 0 ## 39 1 0 0 0 0 0 1 0 1 ## 40 1 0 0 0 0 0 1 0 1 ## 41 1 0 0 0 0 1 0 0 0 ## 42 1 0 0 0 0 0 1 0 1 ## 43 1 0 0 0 0 0 1 0 1 ## 44 1 0 0 0 0 0 1 0 0 ## 45 1 0 0 0 0 0 0 0 0 ## 46 1 0 0 0 0 0 1 1 0 ## 47 1 0 0 0 0 1 0 0 0 ## 48 1 0 0 0 1 0 0 1 0 ## 49 1 0 0 0 1 0 0 1 0 ## 50 1 0 0 0 0 1 0 0 0 ## 51 0 0 0 0 0 1 0 0 0 ## 52 0 0 0 0 0 0 1 1 0 ## 53 0 0 0 0 1 0 0 0 1 ## 54 0 0 0 0 0 0 0 0 0 ## 55 0 0 0 0 0 0 0 0 0 ## 56 0 0 0 0 1 0 0 0 0 ## 57 0 0 0 0 1 0 0 0 0 ## 58 0 0 0 0 1 0 0 0 0 ## 59 0 0 0 0 0 0 1 1 0 ## 60 0 0 0 0 1 0 0 0 0 ## 61 0 0 1 0 0 0 0 1 0 ## 62 0 0 0 0 0 0 1 1 0 ## 63 0 0 0 0 1 0 0 0 0 ## 64 0 0 0 0 0 0 1 0 1 ## 65 0 0 0 0 0 0 1 1 0 ## 66 0 0 0 0 0 0 0 1 0 ## 67 0 0 0 0 0 0 1 1 0 ## 68 0 0 0 0 0 0 1 1 0 ## 69 0 0 0 0 0 0 1 1 0 ## 70 0 0 0 0 0 1 0 0 0 ## 71 0 0 0 0 1 0 0 0 1 ## 72 0 0 0 0 0 0 1 1 0 ## 73 0 0 0 0 0 0 0 0 0 ## 74 0 0 0 0 0 1 0 0 1 ## 75 0 1 0 0 0 0 0 0 1 ## 76 0 0 0 0 0 0 1 1 0 ## 77 0 0 0 0 0 0 1 0 1 ## 78 0 0 0 0 1 0 0 0 1 ## 79 0 0 0 0 0 0 0 0 0 ## 80 0 0 0 0 0 0 1 1 0 ## 81 0 0 0 0 0 0 0 0 0 ## 82 0 0 1 0 0 0 0 0 0 ## 83 0 0 0 0 0 0 1 0 0 ## 84 0 0 0 0 0 0 0 1 0 ## 85 0 0 0 0 0 0 1 0 0 ## 86 0 0 0 0 1 0 0 1 0 ## 87 0 0 0 0 0 0 1 0 1 ## 88 0 0 0 0 0 0 1 1 0 ## 89 0 0 0 0 0 0 0 0 0 ## 90 0 0 0 0 0 0 0 0 0 ## 91 0 0 0 1 0 0 0 0 0 ## 92 0 0 0 1 0 0 0 1 0 ## 93 0 0 0 0 1 0 0 1 0 ## 94 0 0 0 0 0 0 1 0 0 ## 95 0 0 0 0 0 0 1 1 0 ## 96 0 0 0 0 0 1 0 0 0 ## 97 0 0 1 0 0 0 0 1 0 ## 98 0 0 0 0 1 0 0 1 0 ## 99 0 0 0 0 1 0 0 0 0 ## 100 0 0 0 0 1 0 0 0 0 ## 101 0 0 1 0 0 0 0 1 0 ## 102 0 0 0 0 0 1 0 0 0 ## 103 0 0 0 0 0 0 1 0 1 ## 104 0 0 0 1 0 0 0 0 1 ## 105 0 0 0 0 0 0 0 0 0 ## 106 0 0 0 0 0 0 1 1 0 ## 107 0 0 0 0 1 0 0 1 0 ## 108 0 0 0 0 0 0 0 0 0 ## 109 0 0 0 0 1 0 0 1 0 ## 110 0 0 0 0 0 1 0 1 0 ## 111 0 0 0 0 1 0 0 1 0 ## 112 0 0 0 0 0 0 0 1 0 ## 113 0 0 0 0 0 1 0 0 0 ## 114 0 0 0 0 1 0 0 1 0 ## 115 0 0 0 0 0 0 1 1 0 ## 116 0 0 0 0 1 0 0 0 0 ## 117 0 0 0 0 0 0 1 0 1 ## 118 0 0 0 0 0 0 1 0 0 ## 119 0 0 0 0 1 0 0 0 0 ## 120 0 0 1 0 0 0 0 0 0 ## 121 0 0 0 0 1 0 0 1 0 ## 122 0 0 0 0 0 0 1 0 0 ## 123 0 0 0 0 1 0 0 1 0 ## 124 0 0 0 0 0 0 0 0 1 ## 125 0 0 0 0 0 0 1 1 0 ## 126 0 0 0 0 0 0 1 1 0 ## 127 0 0 0 0 0 0 0 0 0 ## 128 0 0 0 0 1 0 0 0 0 ## 129 0 0 0 0 0 0 1 0 0 ## 130 0 0 0 0 1 0 0 0 0 ## 131 0 0 0 0 1 0 0 1 0 ## 132 0 0 0 0 1 0 0 1 0 ## 133 0 0 0 0 0 0 1 0 0 ## 134 0 0 1 0 0 0 0 0 0 ## 135 0 0 0 0 1 0 0 0 0 ## 136 0 0 0 0 1 0 0 0 0 ## 137 0 0 0 0 1 0 0 0 0 ## 138 0 0 0 0 0 0 0 1 0 ## 139 0 0 0 0 0 0 1 0 0 ## 140 0 0 1 0 0 0 0 1 0 ## 141 0 0 1 0 0 0 0 1 0 ## 142 0 1 0 0 0 0 0 0 0 ## 143 0 0 0 0 0 0 0 0 0 ## 144 0 0 0 0 0 0 1 0 1 ## 145 0 0 0 0 0 0 0 0 1 ## 146 0 0 0 1 0 0 0 0 0 ## 147 0 0 0 0 1 0 0 0 0 ## 148 0 0 0 0 0 0 1 0 1 ## 149 0 1 0 0 0 0 0 0 1 ## 150 0 0 0 0 0 1 0 0 0 ## 151 0 0 0 0 0 0 0 0 0 ## 152 0 0 0 0 0 0 0 0 0 ## 153 0 0 0 0 1 0 0 0 0 ## 154 0 0 0 1 0 0 0 0 1 ## 155 0 0 1 0 0 0 0 0 0 ## 156 0 0 0 0 0 0 0 1 0 ## 157 0 0 0 0 0 0 1 1 0 ## 158 0 0 0 1 0 0 0 1 0 ## 159 0 0 0 0 1 0 0 0 0 ## 160 0 0 0 0 0 0 1 1 0 ## 161 0 0 0 0 1 0 0 0 0 ## 162 0 0 0 0 1 0 0 1 0 ## 163 0 0 0 0 0 1 0 1 0 ## 164 0 0 0 0 1 0 0 0 1 ## 165 0 0 1 0 0 0 0 1 0 ## 166 0 0 0 0 0 1 0 0 0 ## 167 0 0 0 0 0 0 1 0 0 ## 168 0 0 0 0 0 0 1 1 0 ## 169 0 0 0 0 0 0 1 0 0 ## 170 0 0 0 0 0 0 1 1 0 ## 171 0 0 0 0 0 0 1 1 0 ## 172 0 0 0 0 0 0 0 0 0 ## 173 0 0 0 1 0 0 0 0 1 ## 174 0 0 1 0 0 0 0 0 0 ## 175 0 0 0 0 0 0 0 0 0 ## 176 0 0 0 0 0 0 1 0 1 ## 177 0 0 0 0 0 0 1 1 0 ## 178 0 0 0 0 0 0 1 1 0 ## 179 0 0 0 0 0 0 1 1 0 ## 180 0 0 1 0 0 0 0 0 0 ## 181 0 0 1 0 0 0 0 1 0 ## 182 0 0 0 0 0 0 1 1 0 ## 183 0 0 0 0 0 0 1 1 0 ## 184 0 0 0 0 1 0 0 0 0 ## 185 0 0 0 0 0 0 1 0 0 ## 186 0 0 0 0 1 0 0 1 0 ## 187 0 0 1 0 0 0 0 1 0 ## 188 0 0 0 0 1 0 0 0 0 ## 189 0 0 0 0 1 0 0 1 0 ## 190 0 0 0 0 0 0 0 0 0 ## 191 0 0 0 0 0 0 0 0 0 ## 192 0 0 0 0 0 0 1 1 0 ## 193 0 0 0 0 1 0 0 0 0 ## 194 0 0 0 0 0 0 1 1 0 ## 195 0 0 0 0 0 1 0 1 0 ## 196 0 0 0 0 1 0 0 1 0 ## 197 0 0 0 0 0 0 1 0 1 ## 198 0 1 0 0 0 0 0 1 0 ## 199 0 0 0 0 0 0 0 1 0 ## 200 0 0 0 0 1 0 0 1 0 ## 201 0 0 0 0 0 0 1 0 0 ## 202 0 0 0 0 0 0 0 1 0 ## 203 0 1 0 0 0 0 0 0 0 ## 204 0 0 0 0 1 0 0 0 0 ## 205 0 0 0 0 0 1 0 0 1 ## 206 0 0 1 0 0 0 0 0 0 ## 207 0 0 0 0 0 0 1 1 0 ## 208 0 0 0 0 0 0 0 0 0 ## 209 0 0 0 0 1 0 0 0 0 ## 210 0 0 0 0 1 0 0 0 1 ## 211 0 0 0 0 0 0 1 0 0 ## 212 0 0 0 0 1 0 0 0 1 ## 213 0 0 0 0 0 0 1 0 0 ## 214 0 0 0 0 0 0 0 0 0 ## 215 0 0 0 0 0 0 0 1 0 ## 216 0 0 0 0 1 0 0 0 0 ## 217 0 0 0 0 0 0 1 1 0 ## 218 0 0 0 0 0 0 1 1 0 ## 219 0 0 0 0 0 0 0 0 0 ## 220 0 0 0 0 0 0 1 0 1 ## 221 0 0 0 0 1 0 0 0 0 ## 222 0 0 0 0 1 0 0 0 0 ## 223 0 1 0 0 0 0 0 0 1 ## 224 0 0 0 0 0 0 1 0 1 ## 225 0 0 1 0 0 0 0 1 0 ## 226 0 0 1 0 0 0 0 0 0 ## 227 0 0 0 0 1 0 0 1 0 ## 228 0 0 0 0 0 1 0 0 0 ## 229 0 0 0 0 0 0 0 1 0 ## 230 0 0 0 0 0 0 1 1 0 ## 231 0 0 0 0 0 0 1 1 0 ## 232 0 0 0 0 1 0 0 0 0 ## 233 0 0 0 0 1 0 0 1 0 ## 234 0 0 1 0 0 0 0 0 0 ## 235 0 0 0 0 0 0 1 0 1 ## 236 0 1 0 0 0 0 0 0 0 ## 237 0 0 0 1 0 0 0 1 0 ## 238 0 0 0 0 0 1 0 0 0 ## 239 0 0 0 0 0 0 1 0 1 ## 240 0 0 0 0 1 0 0 1 0 ## 241 0 0 0 0 1 0 0 0 0 ## 242 0 0 0 0 0 0 1 1 0 ## 243 0 0 0 0 0 0 0 0 0 ## 244 0 0 0 0 0 1 0 0 1 ## 245 0 0 1 0 0 0 0 1 0 ## 246 0 0 0 1 0 0 0 0 1 ## 247 0 0 0 0 0 0 1 1 0 ## 248 0 0 0 0 0 1 0 0 0 ## 249 0 0 0 0 1 0 0 0 0 ## 250 0 0 0 0 0 1 0 0 0 ## 251 0 0 0 0 1 0 0 0 0 ## 252 0 1 0 0 0 0 0 0 0 ## 253 0 0 0 0 1 0 0 1 0 ## 254 0 0 0 0 1 0 0 0 1 ## 255 0 0 0 0 1 0 0 0 0 ## 256 0 0 0 1 0 0 0 0 0 ## 257 0 0 0 0 0 1 0 0 0 ## 258 0 0 0 0 1 0 0 0 0 ## 259 0 0 0 0 0 1 0 0 0 ## 260 0 0 0 0 0 0 1 1 0 ## 261 0 0 0 0 0 0 1 1 0 ## 262 0 0 0 0 0 1 0 0 0 ## 263 0 0 0 0 0 0 0 0 0 ## 264 0 0 0 0 0 1 0 0 0 ## 265 0 0 0 0 0 0 0 0 0 ## 266 0 0 1 0 0 0 0 0 0 ## 267 0 0 0 0 0 0 1 1 0 ## 268 0 0 0 0 1 0 0 1 0 ## 269 0 0 0 0 1 0 0 0 1 ## 270 0 0 0 0 1 0 0 1 0 ## 271 0 0 0 0 0 0 0 0 0 ## 272 0 0 0 0 0 0 1 0 1 ## 273 0 0 0 1 0 0 0 0 0 ## 274 0 0 1 0 0 0 0 0 0 ## 275 0 1 0 0 0 0 0 0 0 ## 276 0 0 0 0 0 0 1 0 1 ## 277 0 0 0 0 1 0 0 1 0 ## 278 0 0 0 0 0 0 1 1 0 ## 279 0 0 0 0 1 0 0 1 0 ## 280 0 0 0 0 0 1 0 0 0 ## 281 0 0 0 0 0 0 1 1 0 ## 282 0 0 0 0 0 0 1 1 0 ## 283 0 0 1 0 0 0 0 1 0 ## 284 0 0 0 0 0 0 0 0 0 ## 285 0 0 0 0 0 0 1 0 1 ## 286 0 0 0 0 1 0 0 1 0 ## 287 0 0 1 0 0 0 0 0 0 ## 288 0 0 0 0 0 0 0 0 0 ## 289 0 0 0 0 1 0 0 0 0 ## 290 0 0 0 0 1 0 0 1 0 ## 291 0 0 1 0 0 0 0 1 0 ## 292 0 0 0 0 0 0 1 0 0 ## 293 0 0 0 0 0 0 1 1 0 ## 294 0 0 0 0 0 0 1 0 0 ## 295 0 0 0 0 1 0 0 0 0 ## 296 0 0 1 0 0 0 0 0 0 ## 297 0 0 1 0 0 0 0 0 0 ## 298 0 0 0 0 0 0 0 0 0 ## 299 0 1 0 0 0 0 0 0 0 ## 300 0 0 0 0 0 1 0 0 0 ## 301 0 0 0 0 0 1 0 1 0 ## 302 0 0 1 0 0 0 0 0 0 ## 303 0 0 1 0 0 0 0 0 0 ## 304 0 0 0 0 0 0 1 0 1 ## 305 0 0 0 0 0 0 0 0 0 ## 306 0 0 1 0 0 0 0 0 0 ## 307 0 0 0 0 1 0 0 0 0 ## 308 0 0 1 0 0 0 0 0 0 ## 309 0 0 0 0 1 0 0 0 0 ## 310 0 0 0 0 1 0 0 0 1 ## 311 0 0 0 0 0 0 1 1 0 ## 312 0 0 0 0 0 0 1 0 1 ## 313 0 0 1 0 0 0 0 1 0 ## 314 0 1 0 0 0 0 0 0 0 ## 315 0 0 0 0 1 0 0 1 0 ## 316 0 0 0 0 1 0 0 0 0 ## 317 0 0 0 0 0 0 1 0 0 ## 318 0 0 0 0 0 0 0 0 0 ## 319 0 0 0 0 0 0 0 0 0 ## 320 0 0 0 0 0 0 0 0 0 ## 321 0 0 0 0 1 0 0 1 0 ## 322 0 0 1 0 0 0 0 0 0 ## 323 0 0 0 0 0 0 1 0 0 ## 324 0 0 0 0 0 0 1 0 1 ## 325 0 0 1 0 0 0 0 1 0 ## 326 0 0 1 0 0 0 0 1 0 ## 327 0 0 0 0 0 1 0 1 0 ## 328 0 0 1 0 0 0 0 0 1 ## 329 0 0 0 0 0 0 1 1 0 ## 330 0 0 0 0 0 0 0 0 0 ## 331 0 0 0 1 0 0 0 0 0 ## 332 0 0 1 0 0 0 0 1 0 ## 333 0 0 0 0 1 0 0 1 0 ## 334 0 0 0 1 0 0 0 1 0 ## 335 0 0 0 0 0 0 1 0 1 ## 336 0 0 0 0 1 0 0 0 0 ## 337 0 0 0 0 0 0 1 1 0 ## 338 0 0 0 0 0 0 1 0 1 ## 339 0 0 0 0 0 0 1 1 0 ## 340 0 0 0 0 1 0 0 0 0 ## 341 0 0 1 0 0 0 0 0 0 ## 342 0 0 0 0 1 0 0 0 0 ## 343 0 0 1 0 0 0 0 0 0 ## 344 0 1 0 0 0 0 0 0 0 ## 345 0 0 1 0 0 0 0 0 0 ## 346 0 0 1 0 0 0 0 0 0 ## 347 0 0 0 0 1 0 0 1 0 ## 348 0 0 0 0 0 0 1 0 1 ## 349 0 0 0 0 0 1 0 0 0 ## 350 0 0 0 0 1 0 0 1 0 ## 351 0 0 0 0 0 0 1 1 0 ## 352 0 0 0 0 0 1 0 0 1 ## 353 0 0 0 0 1 0 0 0 1 ## 354 0 0 0 0 0 0 1 0 1 ## 355 0 0 0 0 1 0 0 1 0 ## 356 0 0 0 0 1 0 0 0 0 ## 357 0 0 0 0 1 0 0 0 0 ## 358 0 0 0 0 0 0 1 1 0 ## 359 0 0 0 0 0 1 0 1 0 ## 360 0 0 0 0 0 0 1 0 1 ## 361 0 0 0 0 1 0 0 0 1 ## 362 0 0 0 0 1 0 0 0 1 ## 363 0 1 0 0 0 0 0 1 0 ## 364 0 0 0 0 1 0 0 1 0 ## 365 0 0 0 0 1 0 0 0 0 ## 366 0 0 0 0 0 0 0 0 0 ## 367 0 0 1 0 0 0 0 0 0 ## 368 0 0 0 0 1 0 0 0 1 ## 369 0 1 0 0 0 0 0 0 1 ## 370 0 0 0 0 1 0 0 0 1 ## 371 0 0 0 0 0 1 0 1 0 ## 372 0 0 0 0 1 0 0 0 0 ## 373 0 0 0 0 1 0 0 0 1 ## 374 0 0 0 0 0 0 1 0 1 ## 375 0 0 0 0 1 0 0 1 0 ## 376 0 0 1 0 0 0 0 1 0 ## 377 0 0 1 0 0 0 0 0 0 ## 378 0 0 0 0 0 0 0 0 0 ## 379 0 0 0 0 0 1 0 0 0 ## 380 0 0 0 1 0 0 0 1 0 ## 381 0 0 0 0 0 0 1 0 1 ## 382 0 0 0 0 1 0 0 1 0 ## 383 0 0 1 0 0 0 0 1 0 ## 384 0 0 1 0 0 0 0 0 0 ## 385 0 0 0 0 0 0 1 0 0 ## 386 0 0 0 0 0 0 1 0 1 ## 387 0 0 0 0 1 0 0 0 1 ## 388 0 0 0 0 1 0 0 1 0 ## 389 0 0 0 0 1 0 0 0 1 ## 390 0 0 0 0 1 0 0 1 0 ## 391 0 0 0 0 0 1 0 0 0 ## 392 0 0 0 0 0 0 1 0 1 ## 393 0 0 0 0 0 0 1 0 0 ## 394 0 0 1 0 0 0 0 0 0 ## 395 0 0 0 0 0 0 1 1 0 ## 396 0 0 0 0 0 0 0 0 0 ## 397 0 0 0 0 1 0 0 0 1 ## 398 0 0 1 0 0 0 0 0 1 ## 399 0 0 0 0 0 0 1 1 0 ## 400 0 0 0 0 0 0 1 1 0 ## 401 0 0 0 0 0 0 0 0 0 ## 402 0 0 0 0 0 0 0 0 0 ## 403 0 0 0 0 1 0 0 0 0 ## 404 0 0 0 0 1 0 0 0 1 ## 405 0 0 0 0 0 0 0 0 0 ## 406 0 0 1 0 0 0 0 0 0 ## 407 0 0 0 0 0 1 0 0 0 ## 408 1 0 0 0 1 0 0 1 0 ## 409 1 0 0 0 0 0 1 1 0 ## 410 1 1 0 0 0 0 0 0 0 ## 411 1 0 0 0 0 1 0 0 0 ## 412 0 0 0 0 1 0 0 1 0 ## 413 0 0 0 0 0 1 0 1 0 ## 414 0 0 0 0 1 0 0 1 0 ## 415 0 0 0 0 0 0 1 0 0 ## 416 0 0 0 0 1 0 0 1 0 ## 417 0 0 0 0 1 0 0 0 0 ## 418 0 0 0 0 0 0 1 0 1 ## 419 0 0 0 1 0 0 0 0 1 ## 420 0 0 0 0 1 0 0 1 0 ## 421 0 0 0 0 0 0 0 0 0 ## 422 0 0 0 0 1 0 0 0 0 ## 423 0 0 1 0 0 0 0 1 0 ## 424 0 0 0 0 1 0 0 0 0 ## 425 0 0 0 0 0 0 0 0 0 ## 426 0 0 0 0 0 0 1 0 0 ## 427 0 0 0 0 0 0 0 1 0 ## 428 0 0 0 0 1 0 0 0 1 ## 429 0 0 0 0 1 0 0 0 0 ## 430 0 0 0 0 0 0 0 0 0 ## 431 0 0 0 0 0 0 1 1 0 ## 432 0 0 0 0 0 0 1 0 0 ## 433 0 0 0 1 0 0 0 1 0 ## 434 0 0 0 0 0 1 0 0 0 ## 435 0 0 0 0 0 0 1 1 0 ## 436 0 0 0 0 0 1 0 0 0 ## 437 0 0 0 0 0 0 0 1 0 ## 438 0 0 0 0 0 0 0 0 0 ## 439 0 0 0 0 1 0 0 1 0 ## 440 0 0 0 0 1 0 0 1 0 ## 441 0 0 1 0 0 0 0 0 0 ## 442 0 1 0 0 0 0 0 0 0 ## 443 0 0 0 0 0 0 1 0 0 ## 444 0 0 1 0 0 0 0 1 0 ## 445 0 0 1 0 0 0 0 1 0 ## 446 0 0 1 0 0 0 0 0 0 ## 447 0 0 0 0 0 1 0 0 0 ## 448 0 0 0 0 1 0 0 1 0 ## 449 0 0 0 0 1 0 0 0 1 ## 450 0 0 0 0 1 0 0 0 0 ## 451 0 0 0 0 1 0 0 1 0 ## 452 0 0 0 0 1 0 0 0 0 ## 453 0 0 0 0 0 0 1 0 0 ## 454 0 0 0 0 0 0 0 0 0 ## 455 0 0 0 0 1 0 0 1 0 ## 456 0 0 0 0 1 0 0 1 0 ## 457 0 0 0 0 1 0 0 1 0 ## 458 0 0 0 0 1 0 0 1 0 ## 459 0 1 0 0 0 0 0 0 1 ## 460 0 0 0 0 1 0 0 0 0 ## 461 0 0 0 0 0 0 1 0 1 ## 462 0 0 0 0 0 1 0 1 0 ## 463 0 0 1 0 0 0 0 0 0 ## 464 0 0 1 0 0 0 0 0 0 ## 465 0 0 0 0 0 1 0 0 0 ## 466 0 1 0 0 0 0 0 0 0 ## 467 0 0 0 0 0 1 0 0 1 ## 468 0 1 0 0 0 0 0 0 0 ## 469 0 0 0 0 1 0 0 1 0 ## 470 0 1 0 0 0 0 0 0 0 ## 471 0 0 0 0 0 0 0 0 0 ## 472 0 0 0 0 1 0 0 0 0 ## 473 1 0 0 0 0 1 0 0 1 ## 474 1 0 0 0 1 0 0 0 0 ## 475 1 0 0 0 0 0 1 0 1 ## 476 1 0 0 0 1 0 0 1 0 ## 477 1 0 0 0 1 0 0 1 0 ## 478 1 0 0 1 0 0 0 1 0 ## 479 1 0 0 1 0 0 0 1 0 ## 480 1 0 0 0 1 0 0 1 0 ## 481 1 0 0 0 0 0 0 0 0 ## 482 1 0 0 0 0 0 0 0 0 ## 483 0 0 1 0 0 0 0 0 0 ## 484 0 0 0 0 1 0 0 0 0 ## 485 0 0 0 0 0 0 1 0 0 ## 486 0 0 0 0 0 0 0 0 1 ## 487 0 0 0 0 0 0 1 1 0 ## 488 1 0 0 0 1 0 0 0 0 ## 489 1 0 0 1 0 0 0 0 1 ## 490 0 0 0 0 1 0 0 0 1 ## 491 0 0 0 0 1 0 0 1 0 ## 492 0 0 0 0 0 0 1 1 0 ## 493 0 0 0 0 0 0 1 0 0 ## 494 0 0 0 0 1 0 0 0 0 ## 495 0 0 0 0 0 0 1 1 0 ## 496 0 0 0 1 0 0 0 0 1 ## 497 0 0 0 0 0 0 0 1 0 ## 498 1 0 0 0 0 0 1 0 1 ## 499 1 0 0 0 1 0 0 0 0 ## 500 1 0 0 1 0 0 0 0 0 ## 501 1 0 0 0 0 0 1 0 1 ## 502 1 0 0 0 1 0 0 0 0 ## 503 1 0 0 0 0 1 0 0 0 ## 504 1 0 1 0 0 0 0 0 0 ## 505 1 1 0 0 0 0 0 1 0 ## 506 1 0 0 1 0 0 0 0 0 ## 507 1 0 0 0 1 0 0 0 0 ## 508 1 0 0 0 0 0 1 1 0 ## 509 1 0 0 0 0 1 0 0 0 ## 510 1 0 0 0 1 0 0 0 0 ## 511 1 0 0 0 0 1 0 0 0 ## 512 1 0 1 0 0 0 0 0 0 ## 513 0 0 0 0 0 1 0 1 0 ## 514 0 1 0 0 0 0 0 0 0 ## 515 0 0 0 0 0 0 1 1 0 ## 516 0 0 0 0 1 0 0 0 0 ## 517 0 0 0 0 0 0 1 1 0 ## 518 0 0 0 0 0 0 1 1 0 ## 519 1 0 0 0 1 0 0 0 0 ## 520 1 0 0 0 1 0 0 0 0 ## 521 1 1 0 0 0 0 0 0 0 ## 522 1 0 0 0 0 0 1 1 0 ## 523 1 0 0 0 1 0 0 1 0 ## 524 1 1 0 0 0 0 0 1 0 ## 525 1 0 1 0 0 0 0 0 0 ## 526 1 0 0 0 0 0 1 0 1 ## servocc lwage expersq tenursq ## 2 1 1.1755733 484 4 ## 3 0 1.0986123 4 0 ## 4 0 1.7917595 1936 784 ## 5 0 1.6677068 49 4 ## 6 0 2.1690538 81 64 ## 7 0 2.4203682 225 49 ## 8 0 1.6094379 25 9 ## 9 0 1.2809339 676 16 ## 10 0 2.9003222 484 441 ## 11 0 1.8325815 64 4 ## 12 0 2.0955610 9 0 ## 13 0 2.1713369 225 0 ## 14 0 1.7047480 324 9 ## 15 0 3.1000924 961 225 ## 16 0 2.8524392 196 0 ## 17 0 2.0149031 100 0 ## 18 0 2.3636801 256 100 ## 19 0 1.2809339 169 0 ## 20 1 1.5040774 1296 36 ## 21 0 1.9286187 121 16 ## 22 0 2.1377103 841 169 ## 23 0 1.8453002 81 81 ## 24 1 -0.6348783 9 1 ## 25 0 1.7917595 1369 64 ## 26 0 2.2575877 9 9 ## 27 0 2.0515563 121 100 ## 28 0 2.5257287 961 0 ## 29 0 2.5257287 900 0 ## 30 0 1.1786550 81 1 ## 31 0 2.5649493 529 25 ## 32 0 1.5040774 4 25 ## 33 0 2.2700620 256 256 ## 34 0 1.6094379 49 9 ## 35 0 1.5432981 9 0 ## 36 0 1.4516138 484 16 ## 37 1 1.8164521 225 36 ## 38 0 1.2556161 1521 225 ## 39 0 1.0986123 9 9 ## 40 0 1.8325815 121 0 ## 41 1 2.0554049 9 0 ## 42 0 2.3025851 400 25 ## 43 0 1.5040774 256 0 ## 44 1 1.3862944 2025 144 ## 45 0 1.8531681 121 16 ## 46 0 2.6173959 400 169 ## 47 0 0.5128236 1 0 ## 48 0 1.0750024 1296 4 ## 49 0 1.2947272 81 4 ## 50 1 1.0647107 225 1 ## 51 1 0.4885800 324 0 ## 52 0 2.1517622 9 4 ## 53 0 1.6094379 225 25 ## 54 0 1.7917595 49 49 ## 55 0 0.9162908 4 0 ## 56 1 1.1786550 9 0 ## 57 1 1.2237755 1 1 ## 58 0 2.3025851 169 0 ## 59 0 3.0740812 64 64 ## 60 1 1.4770488 49 0 ## 61 0 2.4604433 1600 400 ## 62 0 2.5168898 1764 25 ## 63 1 1.8325815 1296 64 ## 64 0 1.3110319 169 0 ## 65 0 2.0515563 81 9 ## 66 0 2.9947317 676 529 ## 67 0 1.8325815 49 16 ## 68 0 2.3025851 625 9 ## 69 0 1.7422190 100 25 ## 70 0 0.6931472 9 4 ## 71 0 1.7422190 9 0 ## 72 0 2.5710843 289 4 ## 73 0 1.5912739 289 64 ## 74 0 1.0681531 400 1156 ## 75 0 1.3217559 49 0 ## 76 0 2.4765384 576 361 ## 77 0 1.3862944 784 0 ## 78 0 1.1314021 4 1 ## 79 0 2.1341665 361 169 ## 80 0 1.9657128 169 0 ## 81 0 1.5040774 484 25 ## 82 0 1.5368673 9 1 ## 83 1 1.0647107 16 0 ## 84 0 1.8976198 49 25 ## 85 1 1.2527629 36 4 ## 86 0 1.1817272 169 9 ## 87 0 1.1786550 196 0 ## 88 0 2.0794415 196 16 ## 89 0 2.2874715 1600 576 ## 90 0 2.0149031 121 49 ## 91 0 1.7766458 196 36 ## 92 0 2.4647040 1600 1521 ## 93 0 1.0986123 1 0 ## 94 1 1.5706971 4 0 ## 95 0 1.8718022 16 1 ## 96 1 1.3862944 361 1 ## 97 0 1.2527629 1 0 ## 98 0 2.5771818 1156 484 ## 99 0 1.4469190 25 4 ## 100 1 1.2527629 9 0 ## 101 0 1.6351057 36 36 ## 102 1 1.3217559 196 0 ## 103 0 1.5040774 1225 144 ## 104 0 2.0320878 64 16 ## 105 0 2.7080503 49 49 ## 106 0 1.9242487 121 9 ## 107 0 2.5900171 196 121 ## 108 0 1.8976198 1225 100 ## 109 0 0.9282193 2116 0 ## 110 0 2.2823825 49 0 ## 111 0 1.2149127 2025 144 ## 112 0 3.2180755 841 625 ## 113 0 1.6863990 36 9 ## 114 0 1.8099267 225 0 ## 115 0 1.4350845 1089 256 ## 116 0 1.3217559 225 0 ## 117 0 1.2527629 25 0 ## 118 1 1.2919837 49 4 ## 119 0 1.3350011 36 1 ## 120 0 1.0986123 1089 144 ## 121 0 1.6094379 4 1 ## 122 1 1.5325569 16 0 ## 123 0 1.0986123 1 0 ## 124 0 1.1631508 841 0 ## 125 0 1.3635374 289 9 ## 126 0 1.8609746 289 9 ## 127 0 1.7011051 1296 9 ## 128 0 0.4054651 961 900 ## 129 1 1.0647107 529 4 ## 130 0 1.6094379 169 1 ## 131 0 2.1882958 9 9 ## 132 0 1.6094379 225 0 ## 133 0 1.2584610 2304 1 ## 134 0 1.0647107 36 0 ## 135 0 1.5040774 144 0 ## 136 1 0.8109302 25 0 ## 137 0 1.6094379 361 25 ## 138 0 2.3025851 81 9 ## 139 1 1.3217559 1521 169 ## 140 0 2.3025851 784 121 ## 141 0 2.3933394 529 400 ## 142 0 2.0668628 4 0 ## 143 0 1.5518087 225 1 ## 144 0 1.7647308 25 0 ## 145 0 1.3428648 324 4 ## 146 0 1.1631508 4 4 ## 147 1 0.6931472 9 0 ## 148 0 1.5040774 961 16 ## 149 0 2.4466856 400 25 ## 150 0 0.7608059 1156 225 ## 151 1 0.8671005 25 0 ## 152 0 1.3217559 121 0 ## 153 0 1.7083778 961 9 ## 154 0 1.8718022 64 25 ## 155 0 1.1314021 4 4 ## 156 0 2.3025851 324 25 ## 157 0 1.8916048 9 0 ## 158 0 2.3025851 9 4 ## 159 1 0.8372475 16 1 ## 160 0 1.9286187 16 16 ## 161 0 1.0402766 1 0 ## 162 0 1.1410331 1 0 ## 163 0 2.0794415 784 25 ## 164 0 1.5040774 2209 16 ## 165 0 2.1575594 169 1 ## 166 1 0.6931472 4 36 ## 167 1 1.5581446 2304 4 ## 168 0 1.8325815 36 25 ## 169 1 1.7917595 64 0 ## 170 0 2.7330680 625 441 ## 171 0 2.6796508 169 49 ## 172 0 2.5257287 64 1 ## 173 0 1.6582280 361 100 ## 174 1 0.7747272 1 16 ## 175 0 1.9657128 1849 25 ## 176 0 1.8277699 361 81 ## 177 0 2.1972246 121 25 ## 178 0 2.3025851 1849 16 ## 179 0 1.7526721 1936 9 ## 180 0 1.3862944 484 121 ## 181 0 2.1690538 9 4 ## 182 0 1.8764070 9 4 ## 183 0 2.0281482 1681 121 ## 184 0 1.6094379 25 0 ## 185 1 1.6094379 196 121 ## 186 0 3.0846586 576 256 ## 187 0 2.1564026 784 64 ## 188 0 1.1939224 625 64 ## 189 0 1.4906543 9 0 ## 190 0 1.5151273 121 0 ## 191 0 1.2527629 49 36 ## 192 0 1.8325815 81 4 ## 193 0 1.3480731 25 0 ## 194 0 1.8213183 81 9 ## 195 0 1.0681531 1 0 ## 196 0 1.8325815 4 1 ## 197 0 1.8325815 169 0 ## 198 0 2.2027647 100 4 ## 199 0 2.3025851 25 9 ## 200 0 2.4078455 900 64 ## 201 1 1.9286187 961 361 ## 202 0 2.1690538 1 4 ## 203 0 2.3025851 81 0 ## 204 0 1.1151416 100 0 ## 205 0 1.0986123 1444 0 ## 206 0 1.7578579 361 36 ## 207 0 1.4109869 25 0 ## 208 0 2.0794415 676 4 ## 209 0 1.8164521 1225 144 ## 210 0 0.9932518 4 0 ## 211 1 1.0116010 1 4 ## 212 0 1.0986123 361 100 ## 213 1 1.0986123 9 4 ## 214 0 1.9960599 1296 576 ## 215 0 2.0149031 841 576 ## 216 0 1.2527629 1 4 ## 217 0 2.0918641 1444 9 ## 218 0 1.3217559 1 4 ## 219 0 1.1786550 841 0 ## 220 0 1.7630169 1296 225 ## 221 1 1.2527629 16 0 ## 222 0 1.2029723 2025 16 ## 223 0 1.3862944 484 9 ## 224 0 1.2527629 400 16 ## 225 0 1.8325815 25 0 ## 226 0 1.0818052 225 4 ## 227 0 1.7422190 100 4 ## 228 0 1.0986123 9 0 ## 229 0 3.1293886 256 49 ## 230 0 2.1972246 1444 1 ## 231 0 2.1198635 1089 676 ## 232 0 1.0986123 4 0 ## 233 0 1.7491999 36 25 ## 234 0 1.9110229 361 9 ## 235 0 2.3025851 841 0 ## 236 0 1.0986123 4 0 ## 237 0 1.2527629 9 1 ## 238 0 1.1786550 16 0 ## 239 0 1.3862944 100 1 ## 240 0 1.0715836 16 0 ## 241 0 1.1184149 196 100 ## 242 0 1.1631508 225 25 ## 243 0 1.5581446 361 0 ## 244 0 1.0986123 289 0 ## 245 0 2.8992214 841 49 ## 246 0 1.2527629 4 0 ## 247 0 1.4134231 25 0 ## 248 0 0.6729445 1444 9 ## 249 1 1.4562867 9 0 ## 250 1 1.0986123 2209 0 ## 251 0 1.8640801 49 36 ## 252 0 1.6486586 2209 169 ## 253 0 1.5040774 529 4 ## 254 0 1.3558352 144 9 ## 255 1 1.2383742 121 0 ## 256 0 2.3896797 625 529 ## 257 1 1.4109869 36 0 ## 258 0 1.0986123 9 1 ## 259 0 1.7749524 196 49 ## 260 0 2.8903718 169 0 ## 261 0 1.3862944 81 0 ## 262 0 1.0986123 1 0 ## 263 0 1.2669476 36 0 ## 264 1 1.0986123 121 1 ## 265 0 2.1690538 2209 1936 ## 266 0 1.0647107 2401 36 ## 267 0 1.8341802 1369 289 ## 268 0 1.2527629 4 0 ## 269 0 1.5260563 49 0 ## 270 0 1.7917595 484 64 ## 271 0 1.0612565 64 0 ## 272 0 1.7191888 1 1 ## 273 0 1.3862944 1849 36 ## 274 0 1.7917595 4 4 ## 275 0 1.5040774 4 1 ## 276 0 1.0715836 1 9 ## 277 0 1.4655675 1 0 ## 278 0 2.9386327 676 400 ## 279 0 1.4539530 1 1 ## 280 1 1.5195132 1369 49 ## 281 0 1.8325815 144 16 ## 282 0 1.0818052 1681 529 ## 283 0 2.1690538 576 1 ## 284 0 2.1400661 1444 676 ## 285 0 1.3217559 324 0 ## 286 0 1.1474025 676 1 ## 287 0 1.6094379 2025 4 ## 288 0 1.8656293 729 0 ## 289 1 0.6931472 4 0 ## 290 0 1.5665303 1681 64 ## 291 0 1.7544037 121 16 ## 292 1 1.1568812 25 0 ## 293 0 1.5432981 9 1 ## 294 1 1.4109869 9 4 ## 295 0 1.0681531 16 0 ## 296 0 1.7917595 441 169 ## 297 0 1.2809339 1156 676 ## 298 0 1.3737156 2401 36 ## 299 0 1.9459101 36 25 ## 300 1 1.0986123 676 81 ## 301 0 1.8050047 81 0 ## 302 0 2.1552446 529 81 ## 303 0 1.0986123 1089 4 ## 304 0 1.3217559 25 4 ## 305 1 1.0647107 2401 49 ## 306 0 1.0986123 2304 0 ## 307 0 1.8325815 1225 961 ## 308 0 1.2527629 529 4 ## 309 1 1.0986123 676 1 ## 310 0 1.1755733 256 0 ## 311 0 2.0819385 529 9 ## 312 0 1.2029723 1296 64 ## 313 0 1.6582280 16 0 ## 314 0 1.8325815 100 0 ## 315 0 1.2527629 324 4 ## 316 0 1.0818052 9 1 ## 317 1 1.0986123 49 0 ## 318 0 1.5454326 49 49 ## 319 0 1.3164083 1089 4 ## 320 0 1.3862944 1156 144 ## 321 0 1.3862944 64 0 ## 322 0 1.0647107 289 1 ## 323 1 1.1151416 4 0 ## 324 0 1.6193882 25 0 ## 325 0 2.6354795 1681 256 ## 326 0 2.8992214 1225 784 ## 327 0 1.8325815 121 16 ## 328 0 1.6582280 16 0 ## 329 0 1.5665303 144 9 ## 330 0 1.2089603 1225 0 ## 331 0 1.0986123 1089 0 ## 332 0 2.1317968 64 36 ## 333 0 1.7404661 4 0 ## 334 0 2.4832385 64 100 ## 335 0 1.2527629 841 1 ## 336 0 1.4445633 196 25 ## 337 0 1.9459101 676 9 ## 338 0 1.7917595 121 9 ## 339 0 2.5030739 100 4 ## 340 0 1.5040774 169 0 ## 341 0 1.0986123 529 400 ## 342 1 1.0647107 1 4 ## 343 0 2.7080503 1225 961 ## 344 0 1.3862944 25 4 ## 345 0 1.6582280 169 121 ## 346 0 1.3862944 484 9 ## 347 0 1.1939224 441 81 ## 348 0 1.6193882 361 0 ## 349 0 1.2753627 169 0 ## 350 0 1.6094379 225 25 ## 351 0 1.5195132 9 0 ## 352 0 2.5257287 36 4 ## 353 0 1.2383742 36 25 ## 354 0 1.5325569 256 1 ## 355 0 2.3025851 961 4 ## 356 1 1.0715836 1 0 ## 357 0 1.5062972 25 4 ## 358 0 1.8718022 9 0 ## 359 0 2.0149031 121 0 ## 360 0 1.2641267 36 49 ## 361 0 1.4350845 121 9 ## 362 0 1.2556161 49 4 ## 363 0 1.5040774 25 0 ## 364 0 1.2089603 25 16 ## 365 1 1.0681531 4 4 ## 366 0 1.6582280 1936 49 ## 367 1 1.3987169 1936 625 ## 368 0 1.3217559 169 0 ## 369 0 1.2237755 676 225 ## 370 0 1.0986123 4 1 ## 371 0 1.8389610 100 9 ## 372 1 0.9321641 4 0 ## 373 0 1.5040774 1225 0 ## 374 0 1.1410331 36 25 ## 375 0 1.8500284 64 1 ## 376 0 1.5432981 1 0 ## 377 0 1.9169227 196 100 ## 378 0 2.1435893 196 36 ## 379 0 1.4279160 484 100 ## 380 0 1.3217559 64 16 ## 381 0 2.4069452 1 16 ## 382 0 1.1817272 225 25 ## 383 0 2.2115657 196 144 ## 384 0 1.5040774 1369 100 ## 385 1 1.0986123 1 1 ## 386 0 2.1690538 16 16 ## 387 0 1.4206958 841 0 ## 388 0 1.0543120 2025 64 ## 389 0 1.2089603 484 0 ## 390 0 1.8050047 1764 100 ## 391 0 1.0986123 81 0 ## 392 0 1.4350845 64 0 ## 393 1 1.7227666 961 225 ## 394 0 2.3025851 576 576 ## 395 0 2.5257287 256 25 ## 396 0 1.3244189 36 0 ## 397 0 1.1314021 196 0 ## 398 0 1.4562867 2209 625 ## 399 0 2.3905959 1156 25 ## 400 0 2.0149031 36 4 ## 401 0 1.3987169 49 16 ## 402 1 1.5368673 729 4 ## 403 0 1.6094379 576 25 ## 404 0 1.0647107 324 0 ## 405 0 2.0794415 144 9 ## 406 0 2.1317968 729 9 ## 407 0 1.0715836 2401 0 ## 408 0 1.8325815 16 0 ## 409 0 1.8325815 576 4 ## 410 0 1.6311995 9 0 ## 411 0 1.3862944 4 0 ## 412 0 1.4906543 841 121 ## 413 0 1.9286187 1156 441 ## 414 0 1.6919391 100 9 ## 415 1 1.0986123 25 0 ## 416 0 1.0647107 4 0 ## 417 0 1.8325815 1521 441 ## 418 0 1.4678744 25 4 ## 419 0 1.1786550 196 4 ## 420 0 1.9823799 64 4 ## 421 0 1.8484548 100 1 ## 422 1 1.7281095 4 4 ## 423 0 2.1690538 81 9 ## 424 1 1.1631508 1 0 ## 425 0 1.0986123 2025 1 ## 426 1 1.0986123 1089 9 ## 427 0 2.5257287 441 324 ## 428 0 1.0577903 4 0 ## 429 0 1.2089603 81 1 ## 430 0 1.8718022 1089 4 ## 431 0 2.3398809 256 4 ## 432 0 1.5040774 100 0 ## 433 0 2.3025851 81 64 ## 434 1 1.3376292 64 1 ## 435 0 2.1747518 81 1 ## 436 0 2.2428350 529 0 ## 437 0 1.8453002 529 64 ## 438 0 1.3862944 484 324 ## 439 0 1.0647107 1369 0 ## 440 0 2.9957323 484 16 ## 441 0 2.4203682 784 625 ## 442 0 1.2527629 196 0 ## 443 1 1.7917595 361 16 ## 444 0 2.6658382 100 81 ## 445 0 1.8500284 625 0 ## 446 0 1.2669476 441 0 ## 447 1 1.0986123 1024 0 ## 448 0 1.5040774 441 100 ## 449 0 1.8916048 1296 0 ## 450 0 2.2300143 4 4 ## 451 0 1.0986123 121 0 ## 452 1 1.1786550 1600 4 ## 453 1 0.4054651 121 1 ## 454 0 1.7749524 81 49 ## 455 0 2.0794415 529 16 ## 456 0 1.0647107 1 0 ## 457 0 1.1908876 900 169 ## 458 0 1.8718022 1681 1089 ## 459 0 1.3862944 36 0 ## 460 0 1.7917595 121 0 ## 461 0 1.4060969 1849 289 ## 462 0 1.3217559 1521 4 ## 463 0 1.1151416 2500 576 ## 464 0 1.2527629 676 400 ## 465 1 1.0715836 2601 900 ## 466 0 1.5040774 9 81 ## 467 0 1.2089603 9 1 ## 468 0 1.7833912 225 81 ## 469 0 2.0794415 289 36 ## 470 0 1.0986123 1296 0 ## 471 0 1.6094379 961 81 ## 472 0 1.7047480 81 16 ## 473 0 0.9745597 1764 100 ## 474 0 1.0986123 9 0 ## 475 0 1.5040774 1369 196 ## 476 0 2.8622010 529 484 ## 477 0 2.1016922 441 25 ## 478 0 2.2071750 121 144 ## 479 0 2.4697931 1225 169 ## 480 0 1.1786550 1764 0 ## 481 0 1.5040774 9 0 ## 482 0 1.5040774 169 0 ## 483 0 1.3110319 196 49 ## 484 0 1.8718022 196 121 ## 485 1 1.0647107 1521 1 ## 486 0 1.7227666 121 64 ## 487 0 0.8020016 784 9 ## 488 1 1.6094379 324 0 ## 489 0 2.1198635 36 4 ## 490 0 1.0647107 676 1 ## 491 0 1.8325815 441 36 ## 492 0 1.5151273 1156 4 ## 493 1 1.1878434 289 4 ## 494 1 0.8329091 4 0 ## 495 0 1.1939224 25 0 ## 496 0 1.1474025 1 0 ## 497 0 2.5257287 1600 900 ## 498 0 1.6389967 1521 441 ## 499 1 1.1410331 1 1 ## 500 0 1.9810015 196 25 ## 501 0 1.0647107 4 4 ## 502 1 0.5596158 4 1 ## 503 0 1.0612565 1764 0 ## 504 0 1.0647107 1156 0 ## 505 0 2.8741293 100 9 ## 506 1 1.8325815 16 9 ## 507 0 0.9555114 16 0 ## 508 0 1.8916048 441 9 ## 509 0 1.2527629 961 9 ## 510 0 1.8718022 400 196 ## 511 0 1.0986123 1296 1 ## 512 0 1.4770488 49 0 ## 513 0 2.3025851 225 0 ## 514 0 1.5993875 625 289 ## 515 0 2.1972246 49 0 ## 516 0 0.3576744 289 0 ## 517 0 1.1249295 9 1 ## 518 0 2.2332351 144 121 ## 519 0 2.0149031 324 25 ## 520 0 1.5581446 2209 1 ## 521 0 1.7316556 4 0 ## 522 0 2.7080503 196 4 ## 523 0 0.8197798 4 0 ## 524 0 1.5411590 169 324 ## 525 0 2.4475510 25 1 ## 526 0 1.2527629 25 16 Drop the last column in the dataset ## wage educ exper tenure nonwhite female married numdep smsa northcen south ## 1 3.10 11 2 0 0 1 0 2 1 0 0 ## 2 3.24 12 22 2 0 1 1 3 1 0 0 ## 3 3.00 11 2 0 0 0 0 2 0 0 0 ## 4 6.00 8 44 28 0 0 1 0 1 0 0 ## 5 5.30 NA 7 2 0 0 1 1 0 0 0 ## 6 8.75 16 9 8 0 0 1 0 1 0 0 ## 7 11.25 18 15 7 0 0 0 0 1 0 0 ## 8 5.00 12 5 3 0 1 0 0 1 0 0 ## 9 3.60 12 26 4 0 1 0 2 1 0 0 ## 10 18.18 17 22 21 0 0 1 0 1 0 0 ## 11 6.25 16 8 2 0 1 0 0 1 0 0 ## 12 8.13 13 3 0 0 1 0 0 1 0 0 ## 13 8.77 12 15 0 0 0 1 2 1 0 0 ## 14 5.50 12 18 3 0 0 0 0 1 0 0 ## 15 22.20 12 31 15 0 0 1 1 1 0 0 ## 16 17.33 16 14 0 0 0 1 1 1 0 0 ## 17 7.50 12 10 0 0 1 1 0 1 0 0 ## 18 10.63 13 16 10 0 1 0 0 1 0 0 ## 19 3.60 12 13 0 0 1 1 3 1 0 0 ## 20 4.50 12 36 6 0 1 1 0 1 0 0 ## 21 6.88 12 11 4 0 1 0 0 1 0 0 ## 22 8.48 12 29 13 0 0 1 3 1 0 0 ## 23 6.33 16 9 9 0 1 0 0 1 0 0 ## 24 0.53 12 3 1 0 1 0 0 1 0 0 ## 25 6.00 11 37 8 1 1 0 0 1 0 0 ## 26 9.56 16 3 3 1 0 1 1 1 0 0 ## 27 7.78 16 11 10 0 0 1 1 1 0 0 ## 28 12.50 16 31 0 0 0 1 0 1 0 0 ## 29 12.50 15 30 0 0 0 1 2 1 0 0 ## 30 3.25 8 9 1 0 1 1 2 1 0 0 ## 31 13.00 14 23 5 0 0 1 2 1 0 0 ## 32 4.50 14 2 5 0 1 0 2 1 0 0 ## 33 9.68 13 16 16 0 1 0 0 1 0 0 ## 34 5.00 12 7 3 0 1 0 0 1 0 0 ## 35 4.68 12 3 0 0 1 0 0 1 0 0 ## 36 4.27 16 22 4 1 1 1 1 1 0 0 ## 37 6.15 12 15 6 1 1 1 1 1 0 0 ## 38 3.51 4 39 15 0 0 1 5 1 0 0 ## 39 3.00 14 3 3 0 1 1 0 1 0 0 ## 40 6.25 12 11 0 0 1 0 0 1 0 0 ## 41 7.81 12 3 0 0 1 1 0 1 0 0 ## 42 10.00 12 20 5 1 1 1 4 1 0 0 ## 43 4.50 14 16 0 0 1 1 1 1 0 0 ## 44 4.00 11 45 12 0 1 1 0 1 0 0 ## 45 6.38 13 11 4 0 1 0 0 1 0 0 ## 46 13.70 15 20 13 0 0 1 2 1 0 0 ## 47 1.67 10 1 0 0 0 0 1 1 0 0 ## 48 2.93 12 36 2 0 1 1 1 1 0 0 ## 49 3.65 14 9 2 0 0 0 0 1 0 0 ## 50 2.90 12 15 1 0 1 1 2 1 0 0 ## 51 1.63 12 18 0 0 1 0 2 0 0 0 ## 52 8.60 16 3 2 0 1 0 0 1 0 0 ## 53 5.00 12 15 5 0 0 1 1 1 0 0 ## 54 6.00 12 7 7 0 0 1 0 1 0 0 ## 55 2.50 12 2 0 0 0 0 2 0 0 0 ## 56 3.25 15 3 0 0 0 0 1 0 0 0 ## 57 3.40 16 1 1 0 1 0 1 0 0 0 ## 58 10.00 8 13 0 1 0 0 0 1 0 0 ## 59 21.63 18 8 8 0 1 0 0 1 0 0 ## 60 4.38 16 7 0 0 0 0 0 1 0 0 ## 61 11.71 13 40 20 0 1 0 0 1 0 0 ## 62 12.39 14 42 5 0 0 0 0 1 0 0 ## 63 6.25 10 36 8 0 0 0 0 1 0 0 ## 64 3.71 10 13 0 1 1 0 4 1 0 0 ## 65 7.78 14 9 3 0 0 0 0 1 0 0 ## 66 19.98 14 26 23 0 0 1 2 1 0 0 ## 67 6.25 16 7 4 1 1 1 3 1 0 0 ## 68 10.00 12 25 3 0 0 1 3 1 0 0 ## 69 5.71 16 10 5 0 0 1 1 1 0 0 ## 70 2.00 12 3 2 0 1 0 4 1 0 0 ## 71 5.71 16 3 0 0 1 0 0 1 0 0 ## 72 13.08 17 17 2 1 0 1 3 1 0 0 ## 73 4.91 12 17 8 0 0 1 2 1 0 0 ## 74 2.91 12 20 34 0 1 1 2 1 0 0 ## 75 3.75 12 7 0 0 1 1 0 1 0 0 ## 76 11.90 13 24 19 0 0 1 2 1 0 0 ## 77 4.00 12 28 0 0 1 1 1 1 0 0 ## 78 3.10 12 2 1 0 1 0 1 1 0 0 ## 79 8.45 12 19 13 0 0 1 4 1 0 0 ## 80 7.14 18 13 0 0 0 1 2 1 0 0 ## 81 4.50 9 22 5 0 0 1 4 1 0 0 ## 82 4.65 16 3 1 0 0 0 0 1 0 0 ## 83 2.90 10 4 0 0 1 0 1 1 0 0 ## 84 6.67 12 7 5 0 0 0 0 1 0 0 ## 85 3.50 12 6 2 1 1 0 1 1 0 0 ## 86 3.26 12 13 3 0 1 1 1 1 0 0 ## 87 3.25 12 14 0 0 1 1 1 1 0 0 ## 88 8.00 12 14 4 0 1 0 1 1 0 0 ## 89 9.85 8 40 24 1 0 1 2 1 0 0 ## 90 7.50 12 11 7 0 0 1 1 1 0 0 ## 91 5.91 12 14 6 0 0 1 2 0 0 0 ## 92 11.76 14 40 39 0 0 1 0 1 0 0 ## 93 3.00 12 1 0 0 1 0 2 1 0 0 ## 94 4.81 12 2 0 0 1 0 0 1 0 0 ## 95 6.50 12 4 1 0 1 0 0 1 0 0 ## 96 4.00 9 19 1 0 1 1 1 1 0 0 ## 97 3.50 13 1 0 0 0 0 1 1 0 0 ## 98 13.16 12 34 22 0 0 1 0 1 0 0 ## 99 4.25 14 5 2 0 0 1 2 1 0 0 ## 100 3.50 12 3 0 0 0 0 0 1 0 0 ## 101 5.13 15 6 6 0 1 0 0 1 0 0 ## 102 3.75 12 14 0 0 1 1 3 1 0 0 ## 103 4.50 12 35 12 0 1 1 0 1 0 0 ## 104 7.63 12 8 4 0 1 0 0 1 0 0 ## 105 15.00 14 7 7 1 0 1 1 1 0 0 ## 106 6.85 15 11 3 0 1 1 2 1 0 0 ## 107 13.33 12 14 11 0 0 1 2 1 0 0 ## 108 6.67 12 35 10 0 0 0 0 1 0 0 ## 109 2.53 12 46 0 0 1 0 0 1 0 0 ## 110 9.80 17 7 0 0 0 1 0 1 0 0 ## 111 3.37 11 45 12 0 0 1 0 1 0 0 ## 112 24.98 18 29 25 0 0 1 0 1 0 0 ## 113 5.40 12 6 3 0 0 1 0 1 0 0 ## 114 6.11 14 15 0 0 0 1 2 1 0 0 ## 115 4.20 14 33 16 0 1 1 0 1 0 0 ## 116 3.75 10 15 0 0 0 0 0 1 0 0 ## 117 3.50 14 5 0 0 1 1 0 0 0 1 ## 118 3.64 12 7 2 0 0 0 0 1 0 1 ## 119 3.80 15 6 1 0 0 1 1 1 0 1 ## 120 3.00 8 33 12 0 1 1 3 0 0 1 ## 121 5.00 16 2 1 0 1 1 0 0 0 1 ## 122 4.63 14 4 0 0 1 1 2 1 0 1 ## 123 3.00 15 1 0 0 0 0 0 1 0 1 ## 124 3.20 12 29 0 0 1 0 1 1 0 1 ## 125 3.91 18 17 3 0 0 1 2 0 0 1 ## 126 6.43 16 17 3 0 1 1 2 0 0 1 ## 127 5.48 10 36 3 0 1 1 0 1 0 1 ## 128 1.50 8 31 30 0 0 0 0 0 0 1 ## 129 2.90 10 23 2 0 1 0 2 0 0 1 ## 130 5.00 11 13 1 0 0 1 0 0 0 1 ## 131 8.92 18 3 3 0 0 1 0 1 0 1 ## 132 5.00 15 15 0 0 0 0 0 1 0 1 ## 133 3.52 12 48 1 0 0 1 0 0 0 1 ## 134 2.90 11 6 0 0 1 0 1 0 0 1 ## 135 4.50 12 12 0 0 0 1 3 1 0 1 ## 136 2.25 12 5 0 0 1 0 2 1 0 1 ## 137 5.00 14 19 5 0 0 1 4 1 0 1 ## 138 10.00 16 9 3 1 0 1 2 1 0 1 ## 139 3.75 2 39 13 0 0 1 0 1 0 1 ## 140 10.00 14 28 11 0 1 1 0 1 0 1 ## 141 10.95 16 23 20 0 0 1 4 1 0 1 ## 142 7.90 12 2 0 0 0 0 0 1 0 1 ## 143 4.72 12 15 1 0 1 0 0 1 0 1 ## 144 5.84 13 5 0 0 1 1 1 1 0 1 ## 145 3.83 12 18 2 0 1 0 3 1 0 1 ## 146 3.20 15 2 2 0 1 0 2 1 0 1 ## 147 2.00 10 3 0 1 1 0 5 1 0 1 ## 148 4.50 12 31 4 0 1 1 3 1 0 1 ## 149 11.55 16 20 5 0 1 1 3 1 0 1 ## 150 2.14 13 34 15 1 1 1 0 1 0 1 ## 151 2.38 9 5 0 1 0 0 5 1 0 1 ## 152 3.75 12 11 0 0 0 0 1 1 0 1 ## 153 5.52 13 31 3 0 0 0 0 1 0 1 ## 154 6.50 12 8 5 0 1 1 0 1 0 1 ## 155 3.10 12 2 2 0 1 0 1 1 0 1 ## 156 10.00 14 18 5 0 0 1 2 1 0 1 ## 157 6.63 16 3 0 0 0 1 0 1 0 1 ## 158 10.00 16 3 2 0 0 1 0 1 0 1 ## 159 2.31 9 4 1 0 1 0 5 0 1 0 ## 160 6.88 18 4 4 0 0 0 0 1 1 0 ## 161 2.83 10 1 0 0 0 0 4 1 1 0 ## 162 3.13 10 1 0 0 1 0 1 1 1 0 ## 163 8.00 13 28 5 0 0 1 1 1 1 0 ## 164 4.50 12 47 4 0 1 1 0 1 1 0 ## 165 8.65 18 13 1 0 1 0 0 1 1 0 ## 166 2.00 13 2 6 0 1 0 0 1 1 0 ## 167 4.75 12 48 2 0 1 1 0 1 1 0 ## 168 6.25 13 6 5 0 1 1 1 1 1 0 ## 169 6.00 13 8 0 0 0 1 2 1 1 0 ## 170 15.38 13 25 21 0 0 1 2 1 1 0 ## 171 14.58 18 13 7 0 1 0 0 1 1 0 ## 172 12.50 12 8 1 0 0 0 0 1 1 0 ## 173 5.25 12 19 10 0 1 1 2 1 1 0 ## 174 2.17 13 1 4 0 1 0 1 1 1 0 ## 175 7.14 12 43 5 0 1 0 0 1 1 0 ## 176 6.22 12 19 9 0 1 1 1 1 1 0 ## 177 9.00 12 11 5 0 1 1 0 1 1 0 ## 178 10.00 14 43 4 0 0 1 0 1 1 0 ## 179 5.77 10 44 3 0 0 1 0 1 1 0 ## 180 4.00 12 22 11 0 1 1 2 1 1 0 ## 181 8.75 16 3 2 0 0 1 1 1 1 0 ## 182 6.53 16 3 2 0 1 0 0 1 1 0 ## 183 7.60 12 41 11 0 1 0 0 1 1 0 ## 184 5.00 14 5 0 0 0 0 0 1 1 0 ## 185 5.00 12 14 11 0 1 0 0 1 1 0 ## 186 21.86 12 24 16 0 0 1 3 1 1 0 ## 187 8.64 12 28 8 0 0 1 0 1 1 0 ## 188 3.30 12 25 8 0 0 1 1 1 1 0 ## 189 4.44 12 3 0 0 0 0 0 1 1 0 ## 190 4.55 12 11 0 0 0 1 0 0 1 0 ## 191 3.50 12 7 6 1 1 1 0 0 1 0 ## 192 6.25 16 9 2 0 0 1 1 0 1 0 ## 193 3.85 16 5 0 0 0 1 0 1 1 0 ## 194 6.18 14 9 3 0 1 1 0 1 1 0 ## 195 2.91 11 1 0 0 1 0 3 1 1 0 ## 196 6.25 16 2 1 1 1 0 0 1 1 0 ## 197 6.25 12 13 0 0 1 1 0 1 1 0 ## 198 9.05 12 10 2 0 0 1 3 1 1 0 ## 199 10.00 17 5 3 0 0 1 0 1 1 0 ## 200 11.11 12 30 8 0 0 1 0 1 1 0 ## 201 6.88 12 31 19 0 0 1 3 1 1 0 ## 202 8.75 16 1 2 1 0 0 0 1 1 0 ## 203 10.00 8 9 0 0 0 1 0 1 1 0 ## 204 3.05 12 10 0 0 1 1 2 1 1 0 ## 205 3.00 12 38 0 0 1 1 0 0 1 0 ## 206 5.80 12 19 6 0 1 1 2 0 1 0 ## 207 4.10 16 5 0 0 1 1 0 1 1 0 ## 208 8.00 12 26 2 0 0 1 1 1 1 0 ## 209 6.15 12 35 12 0 1 0 0 1 1 0 ## 210 2.70 9 2 0 0 1 0 1 1 1 0 ## 211 2.75 13 1 2 0 1 0 1 1 1 0 ## 212 3.00 16 19 10 0 1 1 0 1 1 0 ## 213 3.00 14 3 2 0 1 0 0 1 1 0 ## 214 7.36 8 36 24 0 0 1 3 1 1 0 ## 215 7.50 14 29 24 0 0 1 1 1 1 0 ## 216 3.50 13 1 2 0 0 0 3 1 1 0 ## 217 8.10 12 38 3 0 1 1 1 1 1 0 ## 218 3.75 18 1 2 0 0 0 1 1 1 0 ## 219 3.25 9 29 0 1 0 1 1 1 1 0 ## 220 5.83 8 36 15 0 1 1 0 0 0 0 ## 221 3.50 8 4 0 0 0 0 3 1 0 0 ## 222 3.33 12 45 4 0 1 1 0 0 0 0 ## 223 4.00 14 22 3 0 1 0 0 1 0 0 ## 224 3.50 12 20 4 0 1 1 2 0 0 0 ## 225 6.25 16 5 0 0 0 1 0 1 0 0 ## 226 2.95 8 15 2 1 1 1 1 1 0 0 ## 227 5.71 13 10 2 0 1 1 0 1 0 0 ## 228 3.00 9 3 0 0 1 0 0 1 0 0 ## 229 22.86 16 16 7 0 0 1 2 1 0 0 ## 230 9.00 12 38 1 0 0 1 0 1 0 0 ## 231 8.33 15 33 26 0 0 1 1 1 0 0 ## 232 3.00 11 2 0 0 0 0 0 1 0 0 ## 233 5.75 14 6 5 0 0 1 0 1 0 0 ## 234 6.76 12 19 3 1 0 1 2 1 0 0 ## 235 10.00 12 29 0 0 0 1 2 1 0 0 ## 236 3.00 12 2 0 0 0 0 0 1 0 0 ## 237 3.50 18 3 1 0 1 0 0 1 0 0 ## 238 3.25 12 4 0 0 0 0 0 1 0 0 ## 239 4.00 12 10 1 1 1 0 0 1 0 0 ## 240 2.92 12 4 0 0 1 0 1 0 0 1 ## 241 3.06 12 14 10 1 1 0 3 0 0 1 ## 242 3.20 12 15 5 0 1 1 1 0 0 1 ## 243 4.75 12 19 0 0 0 1 3 0 0 1 ## 244 3.00 14 17 0 0 1 1 4 1 0 1 ## 245 18.16 16 29 7 0 0 1 1 1 0 1 ## 246 3.50 12 2 0 0 1 0 2 1 0 1 ## 247 4.11 14 5 0 0 0 0 0 0 0 1 ## 248 1.96 11 38 3 1 1 0 0 0 0 1 ## 249 4.29 12 3 0 0 1 0 0 0 0 1 ## 250 3.00 10 47 0 0 0 0 0 0 0 1 ## 251 6.45 12 7 6 0 0 1 0 0 0 1 ## 252 5.20 6 47 13 1 0 1 0 1 0 1 ## 253 4.50 13 23 2 1 1 0 0 1 0 1 ## 254 3.88 12 12 3 0 0 1 3 1 0 1 ## 255 3.45 10 11 0 0 1 0 2 1 0 1 ## 256 10.91 12 25 23 0 0 0 0 1 0 1 ## 257 4.10 14 6 0 0 1 1 0 1 0 1 ## 258 3.00 13 3 1 0 0 0 0 1 0 1 ## 259 5.90 12 14 7 1 0 1 2 1 0 1 ## 260 18.00 18 13 0 0 1 0 0 1 0 1 ## 261 4.00 12 9 0 0 0 0 0 1 0 1 ## 262 3.00 12 1 0 1 0 0 0 1 0 1 ## 263 3.55 12 6 0 0 1 1 1 0 0 0 ## 264 3.00 12 11 1 0 1 1 2 0 0 0 ## 265 8.75 12 47 44 0 0 1 0 0 0 0 ## 266 2.90 8 49 6 0 1 0 1 0 0 0 ## 267 6.26 13 37 17 0 1 1 0 1 0 0 ## 268 3.50 13 2 0 0 1 0 0 1 0 0 ## 269 4.60 14 7 0 0 1 1 0 1 0 0 ## 270 6.00 12 22 8 0 0 0 2 1 0 0 ## 271 2.89 10 8 0 0 1 0 1 1 0 0 ## 272 5.58 16 1 1 0 0 1 0 1 0 0 ## 273 4.00 12 43 6 0 1 1 0 0 0 0 ## 274 6.00 16 2 2 0 0 0 0 1 0 0 ## 275 4.50 12 2 1 0 1 0 2 1 0 0 ## 276 2.92 14 1 3 0 0 0 0 1 0 0 ## 277 4.33 18 1 0 0 0 0 0 1 0 0 ## 278 18.89 17 26 20 0 0 1 2 1 0 0 ## 279 4.28 13 1 1 0 1 0 2 1 0 0 ## 280 4.57 14 37 7 0 1 1 0 1 0 0 ## 281 6.25 15 12 4 0 1 1 2 1 0 0 ## 282 2.95 14 41 23 1 0 1 0 0 1 0 ## 283 8.75 12 24 1 0 0 0 0 1 1 0 ## 284 8.50 8 38 26 0 0 0 0 1 1 0 ## 285 3.75 12 18 0 0 1 1 4 1 1 0 ## 286 3.15 12 26 1 0 0 1 0 1 1 0 ## 287 5.00 8 45 2 0 0 0 0 0 1 0 ## 288 6.46 12 27 0 0 0 1 3 0 1 0 ## 289 2.00 9 2 0 0 0 0 3 0 1 0 ## 290 4.79 12 41 8 0 0 1 0 1 1 0 ## 291 5.78 16 11 4 0 0 1 2 1 1 0 ## 292 3.18 12 5 0 0 1 1 0 1 1 0 ## 293 4.68 16 3 1 0 1 0 0 1 1 0 ## 294 4.10 12 3 2 0 1 0 0 1 1 0 ## 295 2.91 12 4 0 0 0 1 0 1 0 1 ## 296 6.00 13 21 13 0 0 1 4 0 0 1 ## 297 3.60 10 34 26 0 1 1 0 0 0 1 ## 298 3.95 6 49 6 0 0 1 6 0 0 1 ## 299 7.00 12 6 5 1 0 1 1 0 0 1 ## 300 3.00 12 26 9 0 1 1 0 0 0 1 ## 301 6.08 16 9 0 0 0 0 0 0 0 1 ## 302 8.63 12 23 9 0 0 1 1 0 0 1 ## 303 3.00 8 33 2 0 0 1 0 0 0 1 ## 304 3.75 12 5 2 0 1 1 1 0 0 1 ## 305 2.90 6 49 7 0 0 1 0 0 0 1 ## 306 3.00 4 48 0 1 0 1 0 0 0 1 ## 307 6.25 11 35 31 0 0 1 0 1 0 1 ## 308 3.50 11 23 2 1 1 0 2 0 0 1 ## 309 3.00 7 26 1 0 1 0 3 0 0 1 ## 310 3.24 12 16 0 0 1 1 2 1 0 1 ## 311 8.02 18 23 3 0 1 1 1 1 0 1 ## 312 3.33 12 36 8 0 1 1 1 1 0 1 ## 313 5.25 16 4 0 0 0 0 0 1 0 1 ## 314 6.25 12 10 0 0 0 1 3 1 0 1 ## 315 3.50 14 18 2 0 0 1 1 0 1 0 ## 316 2.95 12 3 1 0 0 0 1 0 1 0 ## 317 3.00 10 7 0 0 1 1 2 0 1 0 ## 318 4.69 10 7 7 0 0 0 0 0 1 0 ## 319 3.73 9 33 2 0 1 0 1 0 1 0 ## 320 4.00 10 34 12 0 0 1 0 0 1 0 ## 321 4.00 12 8 0 0 1 1 2 0 1 0 ## 322 2.90 12 17 1 0 1 1 2 0 1 0 ## 323 3.05 12 2 0 0 1 0 1 0 1 0 ## 324 5.05 10 5 0 0 1 0 0 1 1 0 ## 325 13.95 16 41 16 0 0 1 0 1 1 0 ## 326 18.16 16 35 28 0 0 1 1 1 1 0 ## 327 6.25 16 11 4 0 0 0 0 1 1 0 ## 328 5.25 12 4 0 1 0 0 0 1 1 0 ## 329 4.79 12 12 3 0 1 1 3 1 1 0 ## 330 3.35 7 35 0 0 1 0 1 1 0 1 ## 331 3.00 8 33 0 0 0 1 1 1 0 1 ## 332 8.43 16 8 6 0 0 1 0 1 0 1 ## 333 5.70 16 2 0 0 0 0 0 1 0 1 ## 334 11.98 18 8 10 0 0 1 2 1 0 1 ## 335 3.50 13 29 1 0 1 1 0 0 0 1 ## 336 4.24 10 14 5 1 0 1 1 0 0 1 ## 337 7.00 16 26 3 0 1 1 1 1 0 1 ## 338 6.00 14 11 3 0 1 1 2 1 0 1 ## 339 12.22 16 10 2 0 0 0 0 1 0 1 ## 340 4.50 12 13 0 0 0 1 0 1 0 1 ## 341 3.00 9 23 20 0 1 1 2 0 0 1 ## 342 2.90 11 1 2 1 0 0 2 0 0 1 ## 343 15.00 11 35 31 0 0 1 0 0 0 1 ## 344 4.00 12 5 2 0 0 1 0 0 0 1 ## 345 5.25 11 13 11 0 0 1 2 0 0 1 ## 346 4.00 12 22 3 0 0 1 3 0 0 1 ## 347 3.30 12 21 9 0 1 1 2 0 0 1 ## 348 5.05 12 19 0 0 1 1 3 0 0 1 ## 349 3.58 12 13 0 0 0 0 0 1 0 1 ## 350 5.00 14 15 5 0 0 1 2 0 0 1 ## 351 4.57 14 3 0 0 0 1 0 0 0 1 ## 352 12.50 18 6 2 0 0 0 0 1 0 1 ## 353 3.45 12 6 5 0 1 1 0 1 0 1 ## 354 4.63 12 16 1 0 1 0 0 1 0 1 ## 355 10.00 12 31 2 0 0 1 0 1 0 1 ## 356 2.92 11 1 0 0 1 0 1 0 1 0 ## 357 4.51 12 5 2 0 0 0 0 1 1 0 ## 358 6.50 17 3 0 0 0 1 0 1 1 0 ## 359 7.50 16 11 0 0 0 1 0 1 1 0 ## 360 3.54 13 6 7 0 1 1 0 0 1 0 ## 361 4.20 13 11 3 0 1 1 2 1 1 0 ## 362 3.51 12 7 2 0 1 1 0 1 1 0 ## 363 4.50 14 5 0 0 0 1 0 1 1 0 ## 364 3.35 14 5 4 0 1 1 0 1 1 0 ## 365 2.91 11 2 2 0 0 0 2 1 1 0 ## 366 5.25 10 44 7 0 0 1 1 1 1 0 ## 367 4.05 8 44 25 0 0 1 0 1 1 0 ## 368 3.75 14 13 0 0 1 1 3 1 1 0 ## 369 3.40 12 26 15 0 1 1 1 1 1 0 ## 370 3.00 10 2 1 0 1 0 1 1 0 1 ## 371 6.29 17 10 3 0 0 1 1 1 0 1 ## 372 2.54 9 2 0 0 1 0 1 1 0 1 ## 373 4.50 12 35 0 0 1 1 1 1 0 1 ## 374 3.13 12 6 5 0 1 1 1 1 0 1 ## 375 6.36 14 8 1 0 0 1 0 1 0 1 ## 376 4.68 16 1 0 0 0 0 0 1 0 1 ## 377 6.80 12 14 10 0 0 1 2 0 0 1 ## 378 8.53 10 14 6 0 0 1 0 0 0 1 ## 379 4.17 0 22 10 0 1 0 0 0 0 1 ## 380 3.75 14 8 4 0 1 1 0 0 0 1 ## 381 11.10 15 1 4 0 1 0 2 0 0 1 ## 382 3.26 16 15 5 1 1 1 2 1 0 1 ## 383 9.13 12 14 12 0 0 1 3 1 0 1 ## 384 4.50 11 37 10 0 0 1 2 1 0 1 ## 385 3.00 11 1 1 0 1 0 1 1 0 1 ## 386 8.75 12 4 4 0 0 1 0 1 0 1 ## 387 4.14 13 29 0 0 1 1 0 1 0 1 ## 388 2.87 12 45 8 0 1 1 0 0 1 0 ## 389 3.35 13 22 0 0 1 0 2 0 1 0 ## 390 6.08 16 42 10 0 0 0 0 0 1 0 ## 391 3.00 15 9 0 1 0 0 0 0 1 0 ## 392 4.20 16 8 0 0 1 1 1 1 1 0 ## 393 5.60 15 31 15 0 1 0 2 1 1 0 ## 394 10.00 12 24 24 0 0 1 0 1 1 0 ## 395 12.50 18 16 5 0 0 1 1 1 1 0 ## 396 3.76 6 6 0 0 0 0 4 0 0 1 ## 397 3.10 6 14 0 1 1 0 5 1 0 1 ## 398 4.29 12 47 25 0 1 1 2 1 0 1 ## 399 10.92 12 34 5 0 0 1 0 0 0 1 ## 400 7.50 16 6 2 0 1 1 1 1 0 1 ## 401 4.05 9 7 4 0 1 1 1 1 0 1 ## 402 4.65 12 27 2 0 0 1 3 1 0 1 ## 403 5.00 11 24 5 0 0 1 0 1 0 1 ## 404 2.90 10 18 0 0 1 1 2 0 0 1 ## 405 8.00 12 12 3 1 1 0 1 0 0 1 ## 406 8.43 8 27 3 0 0 1 3 1 0 1 ## 407 2.92 9 49 0 0 1 0 0 1 0 1 ## 408 6.25 17 4 0 0 1 0 0 1 0 0 ## 409 6.25 16 24 2 0 1 1 2 1 0 0 ## 410 5.11 11 3 0 0 0 0 0 1 0 0 ## 411 4.00 10 2 0 0 1 0 1 1 0 0 ## 412 4.44 8 29 11 0 0 1 3 0 0 1 ## 413 6.88 13 34 21 0 0 1 0 0 0 1 ## 414 5.43 14 10 3 1 0 1 1 1 0 1 ## 415 3.00 13 5 0 0 1 0 1 1 0 1 ## 416 2.90 11 2 0 0 1 0 2 1 0 1 ## 417 6.25 7 39 21 1 0 1 0 1 0 1 ## 418 4.34 16 5 2 1 1 0 0 1 0 1 ## 419 3.25 12 14 2 0 1 1 2 0 0 1 ## 420 7.26 13 8 2 0 0 0 0 1 0 1 ## 421 6.35 14 10 1 0 1 1 2 1 0 1 ## 422 5.63 16 2 2 0 0 1 0 1 0 0 ## 423 8.75 14 9 3 0 0 1 1 1 0 0 ## 424 3.20 11 1 0 0 0 0 0 0 0 0 ## 425 3.00 8 45 1 0 1 1 0 0 0 0 ## 426 3.00 14 33 3 0 1 1 0 1 0 0 ## 427 12.50 17 21 18 0 0 1 3 1 0 0 ## 428 2.88 10 2 0 0 1 0 3 1 0 0 ## 429 3.35 12 9 1 0 0 1 0 1 1 0 ## 430 6.50 12 33 2 0 0 1 0 0 1 0 ## 431 10.38 18 16 2 1 0 1 1 1 1 0 ## 432 4.50 14 10 0 0 0 1 0 1 1 0 ## 433 10.00 18 9 8 0 0 0 0 0 1 0 ## 434 3.81 12 8 1 0 1 1 2 0 1 0 ## 435 8.80 16 9 1 0 0 1 0 0 0 1 ## 436 9.42 14 23 0 0 1 1 2 0 0 1 ## 437 6.33 12 23 8 0 0 1 2 0 0 1 ## 438 4.00 9 22 18 1 0 0 0 0 0 1 ## 439 2.90 12 37 0 0 1 1 0 0 0 1 ## 440 20.00 12 22 4 0 0 1 1 0 0 1 ## 441 11.25 17 28 25 0 0 1 1 1 0 1 ## 442 3.50 12 14 0 1 1 0 2 1 0 1 ## 443 6.00 15 19 4 0 1 1 2 1 0 1 ## 444 14.38 17 10 9 1 0 1 1 0 0 1 ## 445 6.36 16 25 0 0 0 1 1 1 0 1 ## 446 3.55 12 21 0 0 1 1 1 0 0 1 ## 447 3.00 15 32 0 0 0 1 0 1 0 1 ## 448 4.50 16 21 10 0 0 1 1 1 0 1 ## 449 6.63 12 36 0 0 1 0 0 1 0 1 ## 450 9.30 15 2 2 0 0 1 0 1 0 1 ## 451 3.00 12 11 0 0 1 1 2 1 0 1 ## 452 3.25 12 40 2 0 1 0 0 0 1 0 ## 453 1.50 12 11 1 0 1 1 2 0 1 0 ## 454 5.90 12 9 7 0 1 1 1 1 1 0 ## 455 8.00 16 23 4 0 0 0 0 1 1 0 ## 456 2.90 11 1 0 0 1 0 1 1 1 0 ## 457 3.29 14 30 13 0 0 0 0 1 1 0 ## 458 6.50 14 41 33 0 0 1 0 1 1 0 ## 459 4.00 13 6 0 0 1 0 1 1 1 0 ## 460 6.00 14 11 0 0 0 1 0 0 1 0 ## 461 4.08 12 43 17 0 1 1 0 1 1 0 ## 462 3.75 12 39 2 0 1 1 0 1 1 0 ## 463 3.05 8 50 24 0 1 0 0 0 0 1 ## 464 3.50 12 26 20 0 1 1 0 0 0 1 ## 465 2.92 3 51 30 1 0 0 0 1 0 1 ## 466 4.50 11 3 9 0 0 0 0 1 0 1 ## 467 3.35 15 3 1 0 1 1 2 1 0 1 ## 468 5.95 11 15 9 1 0 1 1 0 0 1 ## 469 8.00 12 17 6 0 0 1 2 0 0 1 ## 470 3.00 4 36 0 0 0 1 1 0 0 1 ## 471 5.00 9 31 9 1 0 1 6 0 0 1 ## 472 5.50 12 9 4 0 0 1 1 0 0 1 ## 473 2.65 12 42 10 0 1 1 0 1 0 0 ## 474 3.00 11 3 0 0 1 0 2 1 0 0 ## 475 4.50 12 37 14 0 1 1 0 0 0 0 ## 476 17.50 16 23 22 0 0 1 3 1 0 0 ## 477 8.18 13 21 5 0 0 0 0 1 0 0 ## 478 9.09 15 11 12 0 0 1 1 1 0 0 ## 479 11.82 16 35 13 0 0 1 0 1 0 0 ## 480 3.25 12 42 0 0 1 1 0 1 0 0 ## 481 4.50 12 3 0 0 0 0 0 1 0 0 ## 482 4.50 12 13 0 0 0 1 0 1 0 0 ## 483 3.71 9 14 7 0 0 1 2 0 0 1 ## 484 6.50 10 14 11 0 0 1 3 0 0 1 ## 485 2.90 12 39 1 0 1 1 0 0 0 1 ## 486 5.60 11 11 8 0 0 1 2 0 0 1 ## 487 2.23 8 28 3 0 0 1 4 0 0 1 ## 488 5.00 6 18 0 0 1 1 3 1 0 0 ## 489 8.33 16 6 2 0 0 1 0 1 0 0 ## 490 2.90 12 26 1 0 1 0 4 1 0 1 ## 491 6.25 12 21 6 0 0 1 3 1 0 1 ## 492 4.55 16 34 2 0 0 1 0 0 0 1 ## 493 3.28 12 17 2 0 1 0 0 0 0 1 ## 494 2.30 10 2 0 0 1 0 4 1 1 0 ## 495 3.30 13 5 0 0 1 0 0 1 1 0 ## 496 3.15 13 1 0 0 1 0 4 1 1 0 ## 497 12.50 14 40 30 0 0 1 0 1 1 0 ## 498 5.15 16 39 21 0 1 0 0 1 0 0 ## 499 3.13 10 1 1 0 0 0 2 1 0 0 ## 500 7.25 12 14 5 0 0 1 3 1 0 0 ## 501 2.90 12 2 2 0 1 0 0 1 0 0 ## 502 1.75 11 2 1 0 0 0 2 1 0 0 ## 503 2.89 0 42 0 0 1 1 2 0 0 0 ## 504 2.90 5 34 0 0 1 1 5 0 0 0 ## 505 17.71 16 10 3 0 0 1 1 1 0 0 ## 506 6.25 16 4 3 0 0 1 0 0 0 0 ## 507 2.60 9 4 0 0 0 0 1 0 0 0 ## 508 6.63 15 21 3 0 1 1 2 0 0 0 ## 509 3.50 12 31 3 1 1 0 1 1 0 0 ## 510 6.50 12 20 14 0 0 1 3 1 0 0 ## 511 3.00 12 36 1 1 1 0 0 1 0 0 ## 512 4.38 13 7 0 1 0 1 1 1 0 0 ## 513 10.00 12 15 0 0 0 0 1 0 0 0 ## 514 4.95 7 25 17 0 0 1 5 0 0 0 ## 515 9.00 17 7 0 0 1 1 2 0 0 0 ## 516 1.43 12 17 0 0 1 1 2 0 0 0 ## 517 3.08 12 3 1 0 0 0 0 0 0 0 ## 518 9.33 14 12 11 0 0 1 3 0 0 0 ## 519 7.50 12 18 5 0 0 1 2 0 0 0 ## 520 4.75 13 47 1 0 0 1 0 0 0 0 ## 521 5.65 12 2 0 0 0 0 0 0 0 0 ## 522 15.00 16 14 2 0 1 1 2 0 0 0 ## 523 2.27 10 2 0 0 1 0 3 0 0 0 ## 524 4.67 15 13 18 0 0 1 3 0 0 0 ## 525 11.56 16 5 1 0 0 1 0 0 0 0 ## 526 3.50 14 5 4 1 1 0 2 0 0 0 ## west construc ndurman trcommpu trade services profserv profocc clerocc ## 1 1 0 0 0 0 0 0 0 0 ## 2 1 0 0 0 0 1 0 0 0 ## 3 1 0 0 0 1 0 0 0 0 ## 4 1 0 0 0 0 0 0 0 1 ## 5 1 0 0 0 0 0 0 0 0 ## 6 1 0 0 0 0 0 1 1 0 ## 7 1 0 0 0 1 0 0 1 0 ## 8 1 0 0 0 0 0 0 1 0 ## 9 1 0 0 0 1 0 0 1 0 ## 10 1 0 0 0 0 0 0 1 0 ## 11 1 0 0 0 1 0 0 1 0 ## 12 1 0 0 0 0 0 1 0 1 ## 13 1 0 0 0 0 0 0 0 0 ## 14 1 0 0 0 0 0 0 0 0 ## 15 1 0 0 0 0 0 0 1 0 ## 16 1 0 0 0 0 0 1 1 0 ## 17 1 0 0 0 0 0 0 0 1 ## 18 1 0 0 0 0 0 1 1 0 ## 19 1 0 0 0 0 0 0 0 0 ## 20 1 0 0 0 0 0 1 0 0 ## 21 1 0 0 0 0 0 1 1 0 ## 22 1 0 0 0 0 0 0 0 0 ## 23 1 0 0 0 0 0 0 1 0 ## 24 1 0 0 0 0 1 0 0 0 ## 25 1 0 0 0 0 1 0 0 0 ## 26 1 0 1 0 0 0 0 1 0 ## 27 1 0 0 1 0 0 0 0 0 ## 28 1 0 0 0 0 0 0 1 0 ## 29 1 0 0 0 0 0 0 1 0 ## 30 1 0 0 0 0 0 0 0 0 ## 31 1 0 0 0 0 0 1 1 0 ## 32 1 0 0 0 1 0 0 0 0 ## 33 1 0 0 0 0 0 0 1 0 ## 34 1 0 0 0 0 0 0 0 1 ## 35 1 0 0 0 0 1 0 0 1 ## 36 1 0 0 0 1 0 0 1 0 ## 37 1 0 0 0 0 1 0 0 0 ## 38 1 0 0 0 1 0 0 0 0 ## 39 1 0 0 0 0 0 1 0 1 ## 40 1 0 0 0 0 0 1 0 1 ## 41 1 0 0 0 0 1 0 0 0 ## 42 1 0 0 0 0 0 1 0 1 ## 43 1 0 0 0 0 0 1 0 1 ## 44 1 0 0 0 0 0 1 0 0 ## 45 1 0 0 0 0 0 0 0 0 ## 46 1 0 0 0 0 0 1 1 0 ## 47 1 0 0 0 0 1 0 0 0 ## 48 1 0 0 0 1 0 0 1 0 ## 49 1 0 0 0 1 0 0 1 0 ## 50 1 0 0 0 0 1 0 0 0 ## 51 0 0 0 0 0 1 0 0 0 ## 52 0 0 0 0 0 0 1 1 0 ## 53 0 0 0 0 1 0 0 0 1 ## 54 0 0 0 0 0 0 0 0 0 ## 55 0 0 0 0 0 0 0 0 0 ## 56 0 0 0 0 1 0 0 0 0 ## 57 0 0 0 0 1 0 0 0 0 ## 58 0 0 0 0 1 0 0 0 0 ## 59 0 0 0 0 0 0 1 1 0 ## 60 0 0 0 0 1 0 0 0 0 ## 61 0 0 1 0 0 0 0 1 0 ## 62 0 0 0 0 0 0 1 1 0 ## 63 0 0 0 0 1 0 0 0 0 ## 64 0 0 0 0 0 0 1 0 1 ## 65 0 0 0 0 0 0 1 1 0 ## 66 0 0 0 0 0 0 0 1 0 ## 67 0 0 0 0 0 0 1 1 0 ## 68 0 0 0 0 0 0 1 1 0 ## 69 0 0 0 0 0 0 1 1 0 ## 70 0 0 0 0 0 1 0 0 0 ## 71 0 0 0 0 1 0 0 0 1 ## 72 0 0 0 0 0 0 1 1 0 ## 73 0 0 0 0 0 0 0 0 0 ## 74 0 0 0 0 0 1 0 0 1 ## 75 0 1 0 0 0 0 0 0 1 ## 76 0 0 0 0 0 0 1 1 0 ## 77 0 0 0 0 0 0 1 0 1 ## 78 0 0 0 0 1 0 0 0 1 ## 79 0 0 0 0 0 0 0 0 0 ## 80 0 0 0 0 0 0 1 1 0 ## 81 0 0 0 0 0 0 0 0 0 ## 82 0 0 1 0 0 0 0 0 0 ## 83 0 0 0 0 0 0 1 0 0 ## 84 0 0 0 0 0 0 0 1 0 ## 85 0 0 0 0 0 0 1 0 0 ## 86 0 0 0 0 1 0 0 1 0 ## 87 0 0 0 0 0 0 1 0 1 ## 88 0 0 0 0 0 0 1 1 0 ## 89 0 0 0 0 0 0 0 0 0 ## 90 0 0 0 0 0 0 0 0 0 ## 91 0 0 0 1 0 0 0 0 0 ## 92 0 0 0 1 0 0 0 1 0 ## 93 0 0 0 0 1 0 0 1 0 ## 94 0 0 0 0 0 0 1 0 0 ## 95 0 0 0 0 0 0 1 1 0 ## 96 0 0 0 0 0 1 0 0 0 ## 97 0 0 1 0 0 0 0 1 0 ## 98 0 0 0 0 1 0 0 1 0 ## 99 0 0 0 0 1 0 0 0 0 ## 100 0 0 0 0 1 0 0 0 0 ## 101 0 0 1 0 0 0 0 1 0 ## 102 0 0 0 0 0 1 0 0 0 ## 103 0 0 0 0 0 0 1 0 1 ## 104 0 0 0 1 0 0 0 0 1 ## 105 0 0 0 0 0 0 0 0 0 ## 106 0 0 0 0 0 0 1 1 0 ## 107 0 0 0 0 1 0 0 1 0 ## 108 0 0 0 0 0 0 0 0 0 ## 109 0 0 0 0 1 0 0 1 0 ## 110 0 0 0 0 0 1 0 1 0 ## 111 0 0 0 0 1 0 0 1 0 ## 112 0 0 0 0 0 0 0 1 0 ## 113 0 0 0 0 0 1 0 0 0 ## 114 0 0 0 0 1 0 0 1 0 ## 115 0 0 0 0 0 0 1 1 0 ## 116 0 0 0 0 1 0 0 0 0 ## 117 0 0 0 0 0 0 1 0 1 ## 118 0 0 0 0 0 0 1 0 0 ## 119 0 0 0 0 1 0 0 0 0 ## 120 0 0 1 0 0 0 0 0 0 ## 121 0 0 0 0 1 0 0 1 0 ## 122 0 0 0 0 0 0 1 0 0 ## 123 0 0 0 0 1 0 0 1 0 ## 124 0 0 0 0 0 0 0 0 1 ## 125 0 0 0 0 0 0 1 1 0 ## 126 0 0 0 0 0 0 1 1 0 ## 127 0 0 0 0 0 0 0 0 0 ## 128 0 0 0 0 1 0 0 0 0 ## 129 0 0 0 0 0 0 1 0 0 ## 130 0 0 0 0 1 0 0 0 0 ## 131 0 0 0 0 1 0 0 1 0 ## 132 0 0 0 0 1 0 0 1 0 ## 133 0 0 0 0 0 0 1 0 0 ## 134 0 0 1 0 0 0 0 0 0 ## 135 0 0 0 0 1 0 0 0 0 ## 136 0 0 0 0 1 0 0 0 0 ## 137 0 0 0 0 1 0 0 0 0 ## 138 0 0 0 0 0 0 0 1 0 ## 139 0 0 0 0 0 0 1 0 0 ## 140 0 0 1 0 0 0 0 1 0 ## 141 0 0 1 0 0 0 0 1 0 ## 142 0 1 0 0 0 0 0 0 0 ## 143 0 0 0 0 0 0 0 0 0 ## 144 0 0 0 0 0 0 1 0 1 ## 145 0 0 0 0 0 0 0 0 1 ## 146 0 0 0 1 0 0 0 0 0 ## 147 0 0 0 0 1 0 0 0 0 ## 148 0 0 0 0 0 0 1 0 1 ## 149 0 1 0 0 0 0 0 0 1 ## 150 0 0 0 0 0 1 0 0 0 ## 151 0 0 0 0 0 0 0 0 0 ## 152 0 0 0 0 0 0 0 0 0 ## 153 0 0 0 0 1 0 0 0 0 ## 154 0 0 0 1 0 0 0 0 1 ## 155 0 0 1 0 0 0 0 0 0 ## 156 0 0 0 0 0 0 0 1 0 ## 157 0 0 0 0 0 0 1 1 0 ## 158 0 0 0 1 0 0 0 1 0 ## 159 0 0 0 0 1 0 0 0 0 ## 160 0 0 0 0 0 0 1 1 0 ## 161 0 0 0 0 1 0 0 0 0 ## 162 0 0 0 0 1 0 0 1 0 ## 163 0 0 0 0 0 1 0 1 0 ## 164 0 0 0 0 1 0 0 0 1 ## 165 0 0 1 0 0 0 0 1 0 ## 166 0 0 0 0 0 1 0 0 0 ## 167 0 0 0 0 0 0 1 0 0 ## 168 0 0 0 0 0 0 1 1 0 ## 169 0 0 0 0 0 0 1 0 0 ## 170 0 0 0 0 0 0 1 1 0 ## 171 0 0 0 0 0 0 1 1 0 ## 172 0 0 0 0 0 0 0 0 0 ## 173 0 0 0 1 0 0 0 0 1 ## 174 0 0 1 0 0 0 0 0 0 ## 175 0 0 0 0 0 0 0 0 0 ## 176 0 0 0 0 0 0 1 0 1 ## 177 0 0 0 0 0 0 1 1 0 ## 178 0 0 0 0 0 0 1 1 0 ## 179 0 0 0 0 0 0 1 1 0 ## 180 0 0 1 0 0 0 0 0 0 ## 181 0 0 1 0 0 0 0 1 0 ## 182 0 0 0 0 0 0 1 1 0 ## 183 0 0 0 0 0 0 1 1 0 ## 184 0 0 0 0 1 0 0 0 0 ## 185 0 0 0 0 0 0 1 0 0 ## 186 0 0 0 0 1 0 0 1 0 ## 187 0 0 1 0 0 0 0 1 0 ## 188 0 0 0 0 1 0 0 0 0 ## 189 0 0 0 0 1 0 0 1 0 ## 190 0 0 0 0 0 0 0 0 0 ## 191 0 0 0 0 0 0 0 0 0 ## 192 0 0 0 0 0 0 1 1 0 ## 193 0 0 0 0 1 0 0 0 0 ## 194 0 0 0 0 0 0 1 1 0 ## 195 0 0 0 0 0 1 0 1 0 ## 196 0 0 0 0 1 0 0 1 0 ## 197 0 0 0 0 0 0 1 0 1 ## 198 0 1 0 0 0 0 0 1 0 ## 199 0 0 0 0 0 0 0 1 0 ## 200 0 0 0 0 1 0 0 1 0 ## 201 0 0 0 0 0 0 1 0 0 ## 202 0 0 0 0 0 0 0 1 0 ## 203 0 1 0 0 0 0 0 0 0 ## 204 0 0 0 0 1 0 0 0 0 ## 205 0 0 0 0 0 1 0 0 1 ## 206 0 0 1 0 0 0 0 0 0 ## 207 0 0 0 0 0 0 1 1 0 ## 208 0 0 0 0 0 0 0 0 0 ## 209 0 0 0 0 1 0 0 0 0 ## 210 0 0 0 0 1 0 0 0 1 ## 211 0 0 0 0 0 0 1 0 0 ## 212 0 0 0 0 1 0 0 0 1 ## 213 0 0 0 0 0 0 1 0 0 ## 214 0 0 0 0 0 0 0 0 0 ## 215 0 0 0 0 0 0 0 1 0 ## 216 0 0 0 0 1 0 0 0 0 ## 217 0 0 0 0 0 0 1 1 0 ## 218 0 0 0 0 0 0 1 1 0 ## 219 0 0 0 0 0 0 0 0 0 ## 220 0 0 0 0 0 0 1 0 1 ## 221 0 0 0 0 1 0 0 0 0 ## 222 0 0 0 0 1 0 0 0 0 ## 223 0 1 0 0 0 0 0 0 1 ## 224 0 0 0 0 0 0 1 0 1 ## 225 0 0 1 0 0 0 0 1 0 ## 226 0 0 1 0 0 0 0 0 0 ## 227 0 0 0 0 1 0 0 1 0 ## 228 0 0 0 0 0 1 0 0 0 ## 229 0 0 0 0 0 0 0 1 0 ## 230 0 0 0 0 0 0 1 1 0 ## 231 0 0 0 0 0 0 1 1 0 ## 232 0 0 0 0 1 0 0 0 0 ## 233 0 0 0 0 1 0 0 1 0 ## 234 0 0 1 0 0 0 0 0 0 ## 235 0 0 0 0 0 0 1 0 1 ## 236 0 1 0 0 0 0 0 0 0 ## 237 0 0 0 1 0 0 0 1 0 ## 238 0 0 0 0 0 1 0 0 0 ## 239 0 0 0 0 0 0 1 0 1 ## 240 0 0 0 0 1 0 0 1 0 ## 241 0 0 0 0 1 0 0 0 0 ## 242 0 0 0 0 0 0 1 1 0 ## 243 0 0 0 0 0 0 0 0 0 ## 244 0 0 0 0 0 1 0 0 1 ## 245 0 0 1 0 0 0 0 1 0 ## 246 0 0 0 1 0 0 0 0 1 ## 247 0 0 0 0 0 0 1 1 0 ## 248 0 0 0 0 0 1 0 0 0 ## 249 0 0 0 0 1 0 0 0 0 ## 250 0 0 0 0 0 1 0 0 0 ## 251 0 0 0 0 1 0 0 0 0 ## 252 0 1 0 0 0 0 0 0 0 ## 253 0 0 0 0 1 0 0 1 0 ## 254 0 0 0 0 1 0 0 0 1 ## 255 0 0 0 0 1 0 0 0 0 ## 256 0 0 0 1 0 0 0 0 0 ## 257 0 0 0 0 0 1 0 0 0 ## 258 0 0 0 0 1 0 0 0 0 ## 259 0 0 0 0 0 1 0 0 0 ## 260 0 0 0 0 0 0 1 1 0 ## 261 0 0 0 0 0 0 1 1 0 ## 262 0 0 0 0 0 1 0 0 0 ## 263 0 0 0 0 0 0 0 0 0 ## 264 0 0 0 0 0 1 0 0 0 ## 265 0 0 0 0 0 0 0 0 0 ## 266 0 0 1 0 0 0 0 0 0 ## 267 0 0 0 0 0 0 1 1 0 ## 268 0 0 0 0 1 0 0 1 0 ## 269 0 0 0 0 1 0 0 0 1 ## 270 0 0 0 0 1 0 0 1 0 ## 271 0 0 0 0 0 0 0 0 0 ## 272 0 0 0 0 0 0 1 0 1 ## 273 0 0 0 1 0 0 0 0 0 ## 274 0 0 1 0 0 0 0 0 0 ## 275 0 1 0 0 0 0 0 0 0 ## 276 0 0 0 0 0 0 1 0 1 ## 277 0 0 0 0 1 0 0 1 0 ## 278 0 0 0 0 0 0 1 1 0 ## 279 0 0 0 0 1 0 0 1 0 ## 280 0 0 0 0 0 1 0 0 0 ## 281 0 0 0 0 0 0 1 1 0 ## 282 0 0 0 0 0 0 1 1 0 ## 283 0 0 1 0 0 0 0 1 0 ## 284 0 0 0 0 0 0 0 0 0 ## 285 0 0 0 0 0 0 1 0 1 ## 286 0 0 0 0 1 0 0 1 0 ## 287 0 0 1 0 0 0 0 0 0 ## 288 0 0 0 0 0 0 0 0 0 ## 289 0 0 0 0 1 0 0 0 0 ## 290 0 0 0 0 1 0 0 1 0 ## 291 0 0 1 0 0 0 0 1 0 ## 292 0 0 0 0 0 0 1 0 0 ## 293 0 0 0 0 0 0 1 1 0 ## 294 0 0 0 0 0 0 1 0 0 ## 295 0 0 0 0 1 0 0 0 0 ## 296 0 0 1 0 0 0 0 0 0 ## 297 0 0 1 0 0 0 0 0 0 ## 298 0 0 0 0 0 0 0 0 0 ## 299 0 1 0 0 0 0 0 0 0 ## 300 0 0 0 0 0 1 0 0 0 ## 301 0 0 0 0 0 1 0 1 0 ## 302 0 0 1 0 0 0 0 0 0 ## 303 0 0 1 0 0 0 0 0 0 ## 304 0 0 0 0 0 0 1 0 1 ## 305 0 0 0 0 0 0 0 0 0 ## 306 0 0 1 0 0 0 0 0 0 ## 307 0 0 0 0 1 0 0 0 0 ## 308 0 0 1 0 0 0 0 0 0 ## 309 0 0 0 0 1 0 0 0 0 ## 310 0 0 0 0 1 0 0 0 1 ## 311 0 0 0 0 0 0 1 1 0 ## 312 0 0 0 0 0 0 1 0 1 ## 313 0 0 1 0 0 0 0 1 0 ## 314 0 1 0 0 0 0 0 0 0 ## 315 0 0 0 0 1 0 0 1 0 ## 316 0 0 0 0 1 0 0 0 0 ## 317 0 0 0 0 0 0 1 0 0 ## 318 0 0 0 0 0 0 0 0 0 ## 319 0 0 0 0 0 0 0 0 0 ## 320 0 0 0 0 0 0 0 0 0 ## 321 0 0 0 0 1 0 0 1 0 ## 322 0 0 1 0 0 0 0 0 0 ## 323 0 0 0 0 0 0 1 0 0 ## 324 0 0 0 0 0 0 1 0 1 ## 325 0 0 1 0 0 0 0 1 0 ## 326 0 0 1 0 0 0 0 1 0 ## 327 0 0 0 0 0 1 0 1 0 ## 328 0 0 1 0 0 0 0 0 1 ## 329 0 0 0 0 0 0 1 1 0 ## 330 0 0 0 0 0 0 0 0 0 ## 331 0 0 0 1 0 0 0 0 0 ## 332 0 0 1 0 0 0 0 1 0 ## 333 0 0 0 0 1 0 0 1 0 ## 334 0 0 0 1 0 0 0 1 0 ## 335 0 0 0 0 0 0 1 0 1 ## 336 0 0 0 0 1 0 0 0 0 ## 337 0 0 0 0 0 0 1 1 0 ## 338 0 0 0 0 0 0 1 0 1 ## 339 0 0 0 0 0 0 1 1 0 ## 340 0 0 0 0 1 0 0 0 0 ## 341 0 0 1 0 0 0 0 0 0 ## 342 0 0 0 0 1 0 0 0 0 ## 343 0 0 1 0 0 0 0 0 0 ## 344 0 1 0 0 0 0 0 0 0 ## 345 0 0 1 0 0 0 0 0 0 ## 346 0 0 1 0 0 0 0 0 0 ## 347 0 0 0 0 1 0 0 1 0 ## 348 0 0 0 0 0 0 1 0 1 ## 349 0 0 0 0 0 1 0 0 0 ## 350 0 0 0 0 1 0 0 1 0 ## 351 0 0 0 0 0 0 1 1 0 ## 352 0 0 0 0 0 1 0 0 1 ## 353 0 0 0 0 1 0 0 0 1 ## 354 0 0 0 0 0 0 1 0 1 ## 355 0 0 0 0 1 0 0 1 0 ## 356 0 0 0 0 1 0 0 0 0 ## 357 0 0 0 0 1 0 0 0 0 ## 358 0 0 0 0 0 0 1 1 0 ## 359 0 0 0 0 0 1 0 1 0 ## 360 0 0 0 0 0 0 1 0 1 ## 361 0 0 0 0 1 0 0 0 1 ## 362 0 0 0 0 1 0 0 0 1 ## 363 0 1 0 0 0 0 0 1 0 ## 364 0 0 0 0 1 0 0 1 0 ## 365 0 0 0 0 1 0 0 0 0 ## 366 0 0 0 0 0 0 0 0 0 ## 367 0 0 1 0 0 0 0 0 0 ## 368 0 0 0 0 1 0 0 0 1 ## 369 0 1 0 0 0 0 0 0 1 ## 370 0 0 0 0 1 0 0 0 1 ## 371 0 0 0 0 0 1 0 1 0 ## 372 0 0 0 0 1 0 0 0 0 ## 373 0 0 0 0 1 0 0 0 1 ## 374 0 0 0 0 0 0 1 0 1 ## 375 0 0 0 0 1 0 0 1 0 ## 376 0 0 1 0 0 0 0 1 0 ## 377 0 0 1 0 0 0 0 0 0 ## 378 0 0 0 0 0 0 0 0 0 ## 379 0 0 0 0 0 1 0 0 0 ## 380 0 0 0 1 0 0 0 1 0 ## 381 0 0 0 0 0 0 1 0 1 ## 382 0 0 0 0 1 0 0 1 0 ## 383 0 0 1 0 0 0 0 1 0 ## 384 0 0 1 0 0 0 0 0 0 ## 385 0 0 0 0 0 0 1 0 0 ## 386 0 0 0 0 0 0 1 0 1 ## 387 0 0 0 0 1 0 0 0 1 ## 388 0 0 0 0 1 0 0 1 0 ## 389 0 0 0 0 1 0 0 0 1 ## 390 0 0 0 0 1 0 0 1 0 ## 391 0 0 0 0 0 1 0 0 0 ## 392 0 0 0 0 0 0 1 0 1 ## 393 0 0 0 0 0 0 1 0 0 ## 394 0 0 1 0 0 0 0 0 0 ## 395 0 0 0 0 0 0 1 1 0 ## 396 0 0 0 0 0 0 0 0 0 ## 397 0 0 0 0 1 0 0 0 1 ## 398 0 0 1 0 0 0 0 0 1 ## 399 0 0 0 0 0 0 1 1 0 ## 400 0 0 0 0 0 0 1 1 0 ## 401 0 0 0 0 0 0 0 0 0 ## 402 0 0 0 0 0 0 0 0 0 ## 403 0 0 0 0 1 0 0 0 0 ## 404 0 0 0 0 1 0 0 0 1 ## 405 0 0 0 0 0 0 0 0 0 ## 406 0 0 1 0 0 0 0 0 0 ## 407 0 0 0 0 0 1 0 0 0 ## 408 1 0 0 0 1 0 0 1 0 ## 409 1 0 0 0 0 0 1 1 0 ## 410 1 1 0 0 0 0 0 0 0 ## 411 1 0 0 0 0 1 0 0 0 ## 412 0 0 0 0 1 0 0 1 0 ## 413 0 0 0 0 0 1 0 1 0 ## 414 0 0 0 0 1 0 0 1 0 ## 415 0 0 0 0 0 0 1 0 0 ## 416 0 0 0 0 1 0 0 1 0 ## 417 0 0 0 0 1 0 0 0 0 ## 418 0 0 0 0 0 0 1 0 1 ## 419 0 0 0 1 0 0 0 0 1 ## 420 0 0 0 0 1 0 0 1 0 ## 421 0 0 0 0 0 0 0 0 0 ## 422 0 0 0 0 1 0 0 0 0 ## 423 0 0 1 0 0 0 0 1 0 ## 424 0 0 0 0 1 0 0 0 0 ## 425 0 0 0 0 0 0 0 0 0 ## 426 0 0 0 0 0 0 1 0 0 ## 427 0 0 0 0 0 0 0 1 0 ## 428 0 0 0 0 1 0 0 0 1 ## 429 0 0 0 0 1 0 0 0 0 ## 430 0 0 0 0 0 0 0 0 0 ## 431 0 0 0 0 0 0 1 1 0 ## 432 0 0 0 0 0 0 1 0 0 ## 433 0 0 0 1 0 0 0 1 0 ## 434 0 0 0 0 0 1 0 0 0 ## 435 0 0 0 0 0 0 1 1 0 ## 436 0 0 0 0 0 1 0 0 0 ## 437 0 0 0 0 0 0 0 1 0 ## 438 0 0 0 0 0 0 0 0 0 ## 439 0 0 0 0 1 0 0 1 0 ## 440 0 0 0 0 1 0 0 1 0 ## 441 0 0 1 0 0 0 0 0 0 ## 442 0 1 0 0 0 0 0 0 0 ## 443 0 0 0 0 0 0 1 0 0 ## 444 0 0 1 0 0 0 0 1 0 ## 445 0 0 1 0 0 0 0 1 0 ## 446 0 0 1 0 0 0 0 0 0 ## 447 0 0 0 0 0 1 0 0 0 ## 448 0 0 0 0 1 0 0 1 0 ## 449 0 0 0 0 1 0 0 0 1 ## 450 0 0 0 0 1 0 0 0 0 ## 451 0 0 0 0 1 0 0 1 0 ## 452 0 0 0 0 1 0 0 0 0 ## 453 0 0 0 0 0 0 1 0 0 ## 454 0 0 0 0 0 0 0 0 0 ## 455 0 0 0 0 1 0 0 1 0 ## 456 0 0 0 0 1 0 0 1 0 ## 457 0 0 0 0 1 0 0 1 0 ## 458 0 0 0 0 1 0 0 1 0 ## 459 0 1 0 0 0 0 0 0 1 ## 460 0 0 0 0 1 0 0 0 0 ## 461 0 0 0 0 0 0 1 0 1 ## 462 0 0 0 0 0 1 0 1 0 ## 463 0 0 1 0 0 0 0 0 0 ## 464 0 0 1 0 0 0 0 0 0 ## 465 0 0 0 0 0 1 0 0 0 ## 466 0 1 0 0 0 0 0 0 0 ## 467 0 0 0 0 0 1 0 0 1 ## 468 0 1 0 0 0 0 0 0 0 ## 469 0 0 0 0 1 0 0 1 0 ## 470 0 1 0 0 0 0 0 0 0 ## 471 0 0 0 0 0 0 0 0 0 ## 472 0 0 0 0 1 0 0 0 0 ## 473 1 0 0 0 0 1 0 0 1 ## 474 1 0 0 0 1 0 0 0 0 ## 475 1 0 0 0 0 0 1 0 1 ## 476 1 0 0 0 1 0 0 1 0 ## 477 1 0 0 0 1 0 0 1 0 ## 478 1 0 0 1 0 0 0 1 0 ## 479 1 0 0 1 0 0 0 1 0 ## 480 1 0 0 0 1 0 0 1 0 ## 481 1 0 0 0 0 0 0 0 0 ## 482 1 0 0 0 0 0 0 0 0 ## 483 0 0 1 0 0 0 0 0 0 ## 484 0 0 0 0 1 0 0 0 0 ## 485 0 0 0 0 0 0 1 0 0 ## 486 0 0 0 0 0 0 0 0 1 ## 487 0 0 0 0 0 0 1 1 0 ## 488 1 0 0 0 1 0 0 0 0 ## 489 1 0 0 1 0 0 0 0 1 ## 490 0 0 0 0 1 0 0 0 1 ## 491 0 0 0 0 1 0 0 1 0 ## 492 0 0 0 0 0 0 1 1 0 ## 493 0 0 0 0 0 0 1 0 0 ## 494 0 0 0 0 1 0 0 0 0 ## 495 0 0 0 0 0 0 1 1 0 ## 496 0 0 0 1 0 0 0 0 1 ## 497 0 0 0 0 0 0 0 1 0 ## 498 1 0 0 0 0 0 1 0 1 ## 499 1 0 0 0 1 0 0 0 0 ## 500 1 0 0 1 0 0 0 0 0 ## 501 1 0 0 0 0 0 1 0 1 ## 502 1 0 0 0 1 0 0 0 0 ## 503 1 0 0 0 0 1 0 0 0 ## 504 1 0 1 0 0 0 0 0 0 ## 505 1 1 0 0 0 0 0 1 0 ## 506 1 0 0 1 0 0 0 0 0 ## 507 1 0 0 0 1 0 0 0 0 ## 508 1 0 0 0 0 0 1 1 0 ## 509 1 0 0 0 0 1 0 0 0 ## 510 1 0 0 0 1 0 0 0 0 ## 511 1 0 0 0 0 1 0 0 0 ## 512 1 0 1 0 0 0 0 0 0 ## 513 0 0 0 0 0 1 0 1 0 ## 514 0 1 0 0 0 0 0 0 0 ## 515 0 0 0 0 0 0 1 1 0 ## 516 0 0 0 0 1 0 0 0 0 ## 517 0 0 0 0 0 0 1 1 0 ## 518 0 0 0 0 0 0 1 1 0 ## 519 1 0 0 0 1 0 0 0 0 ## 520 1 0 0 0 1 0 0 0 0 ## 521 1 1 0 0 0 0 0 0 0 ## 522 1 0 0 0 0 0 1 1 0 ## 523 1 0 0 0 1 0 0 1 0 ## 524 1 1 0 0 0 0 0 1 0 ## 525 1 0 1 0 0 0 0 0 0 ## 526 1 0 0 0 0 0 1 0 1 ## servocc lwage expersq tenursq ## 1 0 1.1314021 4 0 ## 2 1 1.1755733 484 4 ## 3 0 1.0986123 4 0 ## 4 0 1.7917595 1936 784 ## 5 0 1.6677068 49 4 ## 6 0 2.1690538 81 64 ## 7 0 2.4203682 225 49 ## 8 0 1.6094379 25 9 ## 9 0 1.2809339 676 16 ## 10 0 2.9003222 484 441 ## 11 0 1.8325815 64 4 ## 12 0 2.0955610 9 0 ## 13 0 2.1713369 225 0 ## 14 0 1.7047480 324 9 ## 15 0 3.1000924 961 225 ## 16 0 2.8524392 196 0 ## 17 0 2.0149031 100 0 ## 18 0 2.3636801 256 100 ## 19 0 1.2809339 169 0 ## 20 1 1.5040774 1296 36 ## 21 0 1.9286187 121 16 ## 22 0 2.1377103 841 169 ## 23 0 1.8453002 81 81 ## 24 1 -0.6348783 9 1 ## 25 0 1.7917595 1369 64 ## 26 0 2.2575877 9 9 ## 27 0 2.0515563 121 100 ## 28 0 2.5257287 961 0 ## 29 0 2.5257287 900 0 ## 30 0 1.1786550 81 1 ## 31 0 2.5649493 529 25 ## 32 0 1.5040774 4 25 ## 33 0 2.2700620 256 256 ## 34 0 1.6094379 49 9 ## 35 0 1.5432981 9 0 ## 36 0 1.4516138 484 16 ## 37 1 1.8164521 225 36 ## 38 0 1.2556161 1521 225 ## 39 0 1.0986123 9 9 ## 40 0 1.8325815 121 0 ## 41 1 2.0554049 9 0 ## 42 0 2.3025851 400 25 ## 43 0 1.5040774 256 0 ## 44 1 1.3862944 2025 144 ## 45 0 1.8531681 121 16 ## 46 0 2.6173959 400 169 ## 47 0 0.5128236 1 0 ## 48 0 1.0750024 1296 4 ## 49 0 1.2947272 81 4 ## 50 1 1.0647107 225 1 ## 51 1 0.4885800 324 0 ## 52 0 2.1517622 9 4 ## 53 0 1.6094379 225 25 ## 54 0 1.7917595 49 49 ## 55 0 0.9162908 4 0 ## 56 1 1.1786550 9 0 ## 57 1 1.2237755 1 1 ## 58 0 2.3025851 169 0 ## 59 0 3.0740812 64 64 ## 60 1 1.4770488 49 0 ## 61 0 2.4604433 1600 400 ## 62 0 2.5168898 1764 25 ## 63 1 1.8325815 1296 64 ## 64 0 1.3110319 169 0 ## 65 0 2.0515563 81 9 ## 66 0 2.9947317 676 529 ## 67 0 1.8325815 49 16 ## 68 0 2.3025851 625 9 ## 69 0 1.7422190 100 25 ## 70 0 0.6931472 9 4 ## 71 0 1.7422190 9 0 ## 72 0 2.5710843 289 4 ## 73 0 1.5912739 289 64 ## 74 0 1.0681531 400 1156 ## 75 0 1.3217559 49 0 ## 76 0 2.4765384 576 361 ## 77 0 1.3862944 784 0 ## 78 0 1.1314021 4 1 ## 79 0 2.1341665 361 169 ## 80 0 1.9657128 169 0 ## 81 0 1.5040774 484 25 ## 82 0 1.5368673 9 1 ## 83 1 1.0647107 16 0 ## 84 0 1.8976198 49 25 ## 85 1 1.2527629 36 4 ## 86 0 1.1817272 169 9 ## 87 0 1.1786550 196 0 ## 88 0 2.0794415 196 16 ## 89 0 2.2874715 1600 576 ## 90 0 2.0149031 121 49 ## 91 0 1.7766458 196 36 ## 92 0 2.4647040 1600 1521 ## 93 0 1.0986123 1 0 ## 94 1 1.5706971 4 0 ## 95 0 1.8718022 16 1 ## 96 1 1.3862944 361 1 ## 97 0 1.2527629 1 0 ## 98 0 2.5771818 1156 484 ## 99 0 1.4469190 25 4 ## 100 1 1.2527629 9 0 ## 101 0 1.6351057 36 36 ## 102 1 1.3217559 196 0 ## 103 0 1.5040774 1225 144 ## 104 0 2.0320878 64 16 ## 105 0 2.7080503 49 49 ## 106 0 1.9242487 121 9 ## 107 0 2.5900171 196 121 ## 108 0 1.8976198 1225 100 ## 109 0 0.9282193 2116 0 ## 110 0 2.2823825 49 0 ## 111 0 1.2149127 2025 144 ## 112 0 3.2180755 841 625 ## 113 0 1.6863990 36 9 ## 114 0 1.8099267 225 0 ## 115 0 1.4350845 1089 256 ## 116 0 1.3217559 225 0 ## 117 0 1.2527629 25 0 ## 118 1 1.2919837 49 4 ## 119 0 1.3350011 36 1 ## 120 0 1.0986123 1089 144 ## 121 0 1.6094379 4 1 ## 122 1 1.5325569 16 0 ## 123 0 1.0986123 1 0 ## 124 0 1.1631508 841 0 ## 125 0 1.3635374 289 9 ## 126 0 1.8609746 289 9 ## 127 0 1.7011051 1296 9 ## 128 0 0.4054651 961 900 ## 129 1 1.0647107 529 4 ## 130 0 1.6094379 169 1 ## 131 0 2.1882958 9 9 ## 132 0 1.6094379 225 0 ## 133 0 1.2584610 2304 1 ## 134 0 1.0647107 36 0 ## 135 0 1.5040774 144 0 ## 136 1 0.8109302 25 0 ## 137 0 1.6094379 361 25 ## 138 0 2.3025851 81 9 ## 139 1 1.3217559 1521 169 ## 140 0 2.3025851 784 121 ## 141 0 2.3933394 529 400 ## 142 0 2.0668628 4 0 ## 143 0 1.5518087 225 1 ## 144 0 1.7647308 25 0 ## 145 0 1.3428648 324 4 ## 146 0 1.1631508 4 4 ## 147 1 0.6931472 9 0 ## 148 0 1.5040774 961 16 ## 149 0 2.4466856 400 25 ## 150 0 0.7608059 1156 225 ## 151 1 0.8671005 25 0 ## 152 0 1.3217559 121 0 ## 153 0 1.7083778 961 9 ## 154 0 1.8718022 64 25 ## 155 0 1.1314021 4 4 ## 156 0 2.3025851 324 25 ## 157 0 1.8916048 9 0 ## 158 0 2.3025851 9 4 ## 159 1 0.8372475 16 1 ## 160 0 1.9286187 16 16 ## 161 0 1.0402766 1 0 ## 162 0 1.1410331 1 0 ## 163 0 2.0794415 784 25 ## 164 0 1.5040774 2209 16 ## 165 0 2.1575594 169 1 ## 166 1 0.6931472 4 36 ## 167 1 1.5581446 2304 4 ## 168 0 1.8325815 36 25 ## 169 1 1.7917595 64 0 ## 170 0 2.7330680 625 441 ## 171 0 2.6796508 169 49 ## 172 0 2.5257287 64 1 ## 173 0 1.6582280 361 100 ## 174 1 0.7747272 1 16 ## 175 0 1.9657128 1849 25 ## 176 0 1.8277699 361 81 ## 177 0 2.1972246 121 25 ## 178 0 2.3025851 1849 16 ## 179 0 1.7526721 1936 9 ## 180 0 1.3862944 484 121 ## 181 0 2.1690538 9 4 ## 182 0 1.8764070 9 4 ## 183 0 2.0281482 1681 121 ## 184 0 1.6094379 25 0 ## 185 1 1.6094379 196 121 ## 186 0 3.0846586 576 256 ## 187 0 2.1564026 784 64 ## 188 0 1.1939224 625 64 ## 189 0 1.4906543 9 0 ## 190 0 1.5151273 121 0 ## 191 0 1.2527629 49 36 ## 192 0 1.8325815 81 4 ## 193 0 1.3480731 25 0 ## 194 0 1.8213183 81 9 ## 195 0 1.0681531 1 0 ## 196 0 1.8325815 4 1 ## 197 0 1.8325815 169 0 ## 198 0 2.2027647 100 4 ## 199 0 2.3025851 25 9 ## 200 0 2.4078455 900 64 ## 201 1 1.9286187 961 361 ## 202 0 2.1690538 1 4 ## 203 0 2.3025851 81 0 ## 204 0 1.1151416 100 0 ## 205 0 1.0986123 1444 0 ## 206 0 1.7578579 361 36 ## 207 0 1.4109869 25 0 ## 208 0 2.0794415 676 4 ## 209 0 1.8164521 1225 144 ## 210 0 0.9932518 4 0 ## 211 1 1.0116010 1 4 ## 212 0 1.0986123 361 100 ## 213 1 1.0986123 9 4 ## 214 0 1.9960599 1296 576 ## 215 0 2.0149031 841 576 ## 216 0 1.2527629 1 4 ## 217 0 2.0918641 1444 9 ## 218 0 1.3217559 1 4 ## 219 0 1.1786550 841 0 ## 220 0 1.7630169 1296 225 ## 221 1 1.2527629 16 0 ## 222 0 1.2029723 2025 16 ## 223 0 1.3862944 484 9 ## 224 0 1.2527629 400 16 ## 225 0 1.8325815 25 0 ## 226 0 1.0818052 225 4 ## 227 0 1.7422190 100 4 ## 228 0 1.0986123 9 0 ## 229 0 3.1293886 256 49 ## 230 0 2.1972246 1444 1 ## 231 0 2.1198635 1089 676 ## 232 0 1.0986123 4 0 ## 233 0 1.7491999 36 25 ## 234 0 1.9110229 361 9 ## 235 0 2.3025851 841 0 ## 236 0 1.0986123 4 0 ## 237 0 1.2527629 9 1 ## 238 0 1.1786550 16 0 ## 239 0 1.3862944 100 1 ## 240 0 1.0715836 16 0 ## 241 0 1.1184149 196 100 ## 242 0 1.1631508 225 25 ## 243 0 1.5581446 361 0 ## 244 0 1.0986123 289 0 ## 245 0 2.8992214 841 49 ## 246 0 1.2527629 4 0 ## 247 0 1.4134231 25 0 ## 248 0 0.6729445 1444 9 ## 249 1 1.4562867 9 0 ## 250 1 1.0986123 2209 0 ## 251 0 1.8640801 49 36 ## 252 0 1.6486586 2209 169 ## 253 0 1.5040774 529 4 ## 254 0 1.3558352 144 9 ## 255 1 1.2383742 121 0 ## 256 0 2.3896797 625 529 ## 257 1 1.4109869 36 0 ## 258 0 1.0986123 9 1 ## 259 0 1.7749524 196 49 ## 260 0 2.8903718 169 0 ## 261 0 1.3862944 81 0 ## 262 0 1.0986123 1 0 ## 263 0 1.2669476 36 0 ## 264 1 1.0986123 121 1 ## 265 0 2.1690538 2209 1936 ## 266 0 1.0647107 2401 36 ## 267 0 1.8341802 1369 289 ## 268 0 1.2527629 4 0 ## 269 0 1.5260563 49 0 ## 270 0 1.7917595 484 64 ## 271 0 1.0612565 64 0 ## 272 0 1.7191888 1 1 ## 273 0 1.3862944 1849 36 ## 274 0 1.7917595 4 4 ## 275 0 1.5040774 4 1 ## 276 0 1.0715836 1 9 ## 277 0 1.4655675 1 0 ## 278 0 2.9386327 676 400 ## 279 0 1.4539530 1 1 ## 280 1 1.5195132 1369 49 ## 281 0 1.8325815 144 16 ## 282 0 1.0818052 1681 529 ## 283 0 2.1690538 576 1 ## 284 0 2.1400661 1444 676 ## 285 0 1.3217559 324 0 ## 286 0 1.1474025 676 1 ## 287 0 1.6094379 2025 4 ## 288 0 1.8656293 729 0 ## 289 1 0.6931472 4 0 ## 290 0 1.5665303 1681 64 ## 291 0 1.7544037 121 16 ## 292 1 1.1568812 25 0 ## 293 0 1.5432981 9 1 ## 294 1 1.4109869 9 4 ## 295 0 1.0681531 16 0 ## 296 0 1.7917595 441 169 ## 297 0 1.2809339 1156 676 ## 298 0 1.3737156 2401 36 ## 299 0 1.9459101 36 25 ## 300 1 1.0986123 676 81 ## 301 0 1.8050047 81 0 ## 302 0 2.1552446 529 81 ## 303 0 1.0986123 1089 4 ## 304 0 1.3217559 25 4 ## 305 1 1.0647107 2401 49 ## 306 0 1.0986123 2304 0 ## 307 0 1.8325815 1225 961 ## 308 0 1.2527629 529 4 ## 309 1 1.0986123 676 1 ## 310 0 1.1755733 256 0 ## 311 0 2.0819385 529 9 ## 312 0 1.2029723 1296 64 ## 313 0 1.6582280 16 0 ## 314 0 1.8325815 100 0 ## 315 0 1.2527629 324 4 ## 316 0 1.0818052 9 1 ## 317 1 1.0986123 49 0 ## 318 0 1.5454326 49 49 ## 319 0 1.3164083 1089 4 ## 320 0 1.3862944 1156 144 ## 321 0 1.3862944 64 0 ## 322 0 1.0647107 289 1 ## 323 1 1.1151416 4 0 ## 324 0 1.6193882 25 0 ## 325 0 2.6354795 1681 256 ## 326 0 2.8992214 1225 784 ## 327 0 1.8325815 121 16 ## 328 0 1.6582280 16 0 ## 329 0 1.5665303 144 9 ## 330 0 1.2089603 1225 0 ## 331 0 1.0986123 1089 0 ## 332 0 2.1317968 64 36 ## 333 0 1.7404661 4 0 ## 334 0 2.4832385 64 100 ## 335 0 1.2527629 841 1 ## 336 0 1.4445633 196 25 ## 337 0 1.9459101 676 9 ## 338 0 1.7917595 121 9 ## 339 0 2.5030739 100 4 ## 340 0 1.5040774 169 0 ## 341 0 1.0986123 529 400 ## 342 1 1.0647107 1 4 ## 343 0 2.7080503 1225 961 ## 344 0 1.3862944 25 4 ## 345 0 1.6582280 169 121 ## 346 0 1.3862944 484 9 ## 347 0 1.1939224 441 81 ## 348 0 1.6193882 361 0 ## 349 0 1.2753627 169 0 ## 350 0 1.6094379 225 25 ## 351 0 1.5195132 9 0 ## 352 0 2.5257287 36 4 ## 353 0 1.2383742 36 25 ## 354 0 1.5325569 256 1 ## 355 0 2.3025851 961 4 ## 356 1 1.0715836 1 0 ## 357 0 1.5062972 25 4 ## 358 0 1.8718022 9 0 ## 359 0 2.0149031 121 0 ## 360 0 1.2641267 36 49 ## 361 0 1.4350845 121 9 ## 362 0 1.2556161 49 4 ## 363 0 1.5040774 25 0 ## 364 0 1.2089603 25 16 ## 365 1 1.0681531 4 4 ## 366 0 1.6582280 1936 49 ## 367 1 1.3987169 1936 625 ## 368 0 1.3217559 169 0 ## 369 0 1.2237755 676 225 ## 370 0 1.0986123 4 1 ## 371 0 1.8389610 100 9 ## 372 1 0.9321641 4 0 ## 373 0 1.5040774 1225 0 ## 374 0 1.1410331 36 25 ## 375 0 1.8500284 64 1 ## 376 0 1.5432981 1 0 ## 377 0 1.9169227 196 100 ## 378 0 2.1435893 196 36 ## 379 0 1.4279160 484 100 ## 380 0 1.3217559 64 16 ## 381 0 2.4069452 1 16 ## 382 0 1.1817272 225 25 ## 383 0 2.2115657 196 144 ## 384 0 1.5040774 1369 100 ## 385 1 1.0986123 1 1 ## 386 0 2.1690538 16 16 ## 387 0 1.4206958 841 0 ## 388 0 1.0543120 2025 64 ## 389 0 1.2089603 484 0 ## 390 0 1.8050047 1764 100 ## 391 0 1.0986123 81 0 ## 392 0 1.4350845 64 0 ## 393 1 1.7227666 961 225 ## 394 0 2.3025851 576 576 ## 395 0 2.5257287 256 25 ## 396 0 1.3244189 36 0 ## 397 0 1.1314021 196 0 ## 398 0 1.4562867 2209 625 ## 399 0 2.3905959 1156 25 ## 400 0 2.0149031 36 4 ## 401 0 1.3987169 49 16 ## 402 1 1.5368673 729 4 ## 403 0 1.6094379 576 25 ## 404 0 1.0647107 324 0 ## 405 0 2.0794415 144 9 ## 406 0 2.1317968 729 9 ## 407 0 1.0715836 2401 0 ## 408 0 1.8325815 16 0 ## 409 0 1.8325815 576 4 ## 410 0 1.6311995 9 0 ## 411 0 1.3862944 4 0 ## 412 0 1.4906543 841 121 ## 413 0 1.9286187 1156 441 ## 414 0 1.6919391 100 9 ## 415 1 1.0986123 25 0 ## 416 0 1.0647107 4 0 ## 417 0 1.8325815 1521 441 ## 418 0 1.4678744 25 4 ## 419 0 1.1786550 196 4 ## 420 0 1.9823799 64 4 ## 421 0 1.8484548 100 1 ## 422 1 1.7281095 4 4 ## 423 0 2.1690538 81 9 ## 424 1 1.1631508 1 0 ## 425 0 1.0986123 2025 1 ## 426 1 1.0986123 1089 9 ## 427 0 2.5257287 441 324 ## 428 0 1.0577903 4 0 ## 429 0 1.2089603 81 1 ## 430 0 1.8718022 1089 4 ## 431 0 2.3398809 256 4 ## 432 0 1.5040774 100 0 ## 433 0 2.3025851 81 64 ## 434 1 1.3376292 64 1 ## 435 0 2.1747518 81 1 ## 436 0 2.2428350 529 0 ## 437 0 1.8453002 529 64 ## 438 0 1.3862944 484 324 ## 439 0 1.0647107 1369 0 ## 440 0 2.9957323 484 16 ## 441 0 2.4203682 784 625 ## 442 0 1.2527629 196 0 ## 443 1 1.7917595 361 16 ## 444 0 2.6658382 100 81 ## 445 0 1.8500284 625 0 ## 446 0 1.2669476 441 0 ## 447 1 1.0986123 1024 0 ## 448 0 1.5040774 441 100 ## 449 0 1.8916048 1296 0 ## 450 0 2.2300143 4 4 ## 451 0 1.0986123 121 0 ## 452 1 1.1786550 1600 4 ## 453 1 0.4054651 121 1 ## 454 0 1.7749524 81 49 ## 455 0 2.0794415 529 16 ## 456 0 1.0647107 1 0 ## 457 0 1.1908876 900 169 ## 458 0 1.8718022 1681 1089 ## 459 0 1.3862944 36 0 ## 460 0 1.7917595 121 0 ## 461 0 1.4060969 1849 289 ## 462 0 1.3217559 1521 4 ## 463 0 1.1151416 2500 576 ## 464 0 1.2527629 676 400 ## 465 1 1.0715836 2601 900 ## 466 0 1.5040774 9 81 ## 467 0 1.2089603 9 1 ## 468 0 1.7833912 225 81 ## 469 0 2.0794415 289 36 ## 470 0 1.0986123 1296 0 ## 471 0 1.6094379 961 81 ## 472 0 1.7047480 81 16 ## 473 0 0.9745597 1764 100 ## 474 0 1.0986123 9 0 ## 475 0 1.5040774 1369 196 ## 476 0 2.8622010 529 484 ## 477 0 2.1016922 441 25 ## 478 0 2.2071750 121 144 ## 479 0 2.4697931 1225 169 ## 480 0 1.1786550 1764 0 ## 481 0 1.5040774 9 0 ## 482 0 1.5040774 169 0 ## 483 0 1.3110319 196 49 ## 484 0 1.8718022 196 121 ## 485 1 1.0647107 1521 1 ## 486 0 1.7227666 121 64 ## 487 0 0.8020016 784 9 ## 488 1 1.6094379 324 0 ## 489 0 2.1198635 36 4 ## 490 0 1.0647107 676 1 ## 491 0 1.8325815 441 36 ## 492 0 1.5151273 1156 4 ## 493 1 1.1878434 289 4 ## 494 1 0.8329091 4 0 ## 495 0 1.1939224 25 0 ## 496 0 1.1474025 1 0 ## 497 0 2.5257287 1600 900 ## 498 0 1.6389967 1521 441 ## 499 1 1.1410331 1 1 ## 500 0 1.9810015 196 25 ## 501 0 1.0647107 4 4 ## 502 1 0.5596158 4 1 ## 503 0 1.0612565 1764 0 ## 504 0 1.0647107 1156 0 ## 505 0 2.8741293 100 9 ## 506 1 1.8325815 16 9 ## 507 0 0.9555114 16 0 ## 508 0 1.8916048 441 9 ## 509 0 1.2527629 961 9 ## 510 0 1.8718022 400 196 ## 511 0 1.0986123 1296 1 ## 512 0 1.4770488 49 0 ## 513 0 2.3025851 225 0 ## 514 0 1.5993875 625 289 ## 515 0 2.1972246 49 0 ## 516 0 0.3576744 289 0 ## 517 0 1.1249295 9 1 ## 518 0 2.2332351 144 121 ## 519 0 2.0149031 324 25 ## 520 0 1.5581446 2209 1 ## 521 0 1.7316556 4 0 ## 522 0 2.7080503 196 4 ## 523 0 0.8197798 4 0 ## 524 0 1.5411590 169 324 ## 525 0 2.4475510 25 1 ## 526 0 1.2527629 25 16 Just check the number of row and col in wage1 with the dim() function ## [1] 526 24 Converting dollars to euros and add the new column variable to the dataset 2.1 Conditional selection "],["chapter3.html", "Chapter 3 Code 3", " Chapter 3 Code 3 "],["chapter4.html", "Chapter 4 Math 4.1 Derivative rules 4.2 Matrix operations", " Chapter 4 Math 4.1 Derivative rules The rules for sums and differences Given $ f(x) = g(x) h(x) $, where \\(g(x)\\) and \\(h(x)\\) are both differentiable functions, the derivative of a sum or difference of two functions is given by: \\[\\begin{equation} f&#39;(x) = g&#39;(x) \\pm h&#39;(x) \\end{equation}\\] The product rule Given \\(f(x) = g(x) \\cdot h(x)\\), where \\(g(x)\\) and \\(h(x)\\) are both differentiable functions, the derivative is given by: \\[\\begin{equation} f&#39;(x) = h(x) \\cdot g&#39;(x) + h&#39;(x) \\cdot g(x) \\end{equation}\\] The quotient rule Given \\(f(x) = \\frac{g(x)}{h(x)}\\), where \\(g(x)\\) and \\(h(x)\\) are both differentiable functions and \\(h(x)\\neq0\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = \\dfrac{h(x) \\cdot g&#39;(x) - h&#39;(x) \\cdot g(x)}{[h(x)]^2} \\end{equation}\\] The generalized power function rule Given \\(f(x) = [g(x)]^n\\), where \\(g(x)\\) is a differentiable functions and n is any real number, the derivative is given by: \\[\\begin{equation} f&#39;(x) = n[g(x)]^{n-1} \\cdot g&#39;(x) \\end{equation}\\] The chain rule Given \\(f(x) = h(g(x))\\), where \\(f\\) is a function of a function where \\(h\\) is in turn function of , the derivative is given by: \\[\\begin{equation} f&#39;(x) = h&#39;(g(x)) \\cdot g&#39;(x) \\end{equation}\\] Additional rules Given \\(f(x) = e^x\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = e^x \\end{equation}\\] Given \\(f(x) = ln(x)\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = \\frac{1}{x} \\end{equation}\\] Given \\(f(x) = sin(x)\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = cos(x) \\end{equation}\\] Given \\(f(x) = cos(x)\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = -sin(x) \\end{equation}\\] 4.2 Matrix operations R can perform standard matrix algebra operations. We can use matrix algebra functions in R to solve our problem from class. \\[ 7x + 5y - 3z = 16 \\\\ 3x -5y +2z = -8 \\\\ 5x +3y -7z = 0 \\] First, we rewrite the system using matrix and vector notation: \\[ \\mathbf{A} = \\left[\\begin{array} {rrr} 7 &amp; 5 &amp; -3 \\\\ 3 &amp; -5 &amp; 2 \\\\ 5 &amp; 3 &amp; -7 \\\\ \\end{array}\\right] \\mathbf{b} = \\left[\\begin{array} {rrr} x \\\\ y \\\\ z \\\\ \\end{array}\\right] \\mathbf{r} = \\left[\\begin{array} {rrr} 16 \\\\ -8 \\\\ 0 \\\\ \\end{array}\\right] \\] In order to obtain the result vector b, we have to rearrange the model performing some simple matrix algebra operations. \\[ \\mathbf{A}^{-1}\\mathbf{A}\\mathbf{b} = \\mathbf{A}^{-1}\\mathbf{r} \\\\ \\text{remember that} \\space \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I} \\\\ \\mathbf{b} = \\mathbf{A}^{-1}\\mathbf{r} \\] We are now ready to solve our system of equations using R: ## [,1] ## [1,] 1 ## [2,] 3 ## [3,] 2 "],["chapter5.html", "Chapter 5 Stats 5.1 Find mean and median 5.2 Boxplot 5.3 Expected value 5.4 Variance 5.5 Correlation 5.6 Covariance 5.7 Expected value, variance and covariance rules", " Chapter 5 Stats 5.1 Find mean and median ## [1] 5 ## [1] 5 ## [1] 83 A percentile is a point in a distribution at which or below which a given proportion of data is found. The k-th percentile divides the data in a way that k-percent of the data lie below the percentile and (100 - k)-percent lie above the percentile. It is also common to hear about quantiles. Quantiles is a more generic terms which indicates values partitioning data in equally spaced groups. Specific types of quantiles are percentiles (see above), deciles, quartiles, etc. ## 25% 50% 75% 100% ## 3.75 8.00 12.25 18.00 ## [1] 5 10 15 20 ## [1] 3 8 12 18 ## [1] 30 ## [1] 90 ## [1] 30 90 5.2 Boxplot ## 25% ## -9 ## 75% ## 25 5.3 Expected value ## [1] 4.333333 ## [1] 3.5 ## [1] 5 ## [1] 0.12 Simulation of the expected value of the fair die/dice. This exercise is taken from the book Hands-On Programming with R by Garrett Grolemund. 5.4 Variance Find variance and standard deviation ## [1] 3.466667 ## [1] 1.861899 ## [1] TRUE ## [1] 3.466667 ## [1] 1.861899 ## [1] 346.6667 ## [1] 18.61899 5.5 Correlation With this short exercise we want to see how the Pearson correlation coefficient is a measure of linear correlation between two variables. ## [1] -0.04431211 ## [1] TRUE ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = -0.4391, df = 98, p-value = 0.6616 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2386531 0.1534415 ## sample estimates: ## cor ## -0.04431211 ## [1] 0.9336273 ## ## Pearson&#39;s product-moment correlation ## ## data: a and b ## t = 25.799, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9027582 0.9549292 ## sample estimates: ## cor ## 0.9336273 Programming challenge. Program a function that takes two inputs, x and y, and returns the correlation coefficient between x and y. ## [1] 0.1042097 ## [1] 0.1042097 5.6 Covariance Find covariance and Pearsons correlation coefficient between the variables \\(X\\) and \\(Y\\). ## [1] 10 ## [1] 0.5 5.7 Expected value, variance and covariance rules Expected value The of a (discrete) random variable is the arithmetic mean of that variable where each value is weighted by its probability. It can also be thought as the long-run average for any random variable over an indefinite number of trials. For a discrete random variable X having the possible values \\(x_1, \\dots, x_N\\) the expectation of X is defined as: \\[ E(X) = x_1P(X=x_1) + \\dots + x_NP(X=x_N) = \\sum\\limits_{i=1}^{N} x_iP(X = x_i) = \\sum\\limits_{i=1}^{n} x_ip_{i}\\] The expected value of a random variable can be also understood as the population mean, \\(E(X) = \\mu_x\\) or simply \\(\\mu\\). With equals weights, the formula for the expected value will be equal to the formula for the arithmetic average. Similarly, the expected value of functions of discrete random variables is given by the following: \\[ E\\{g(X)\\} = g(x_1)p_1 + \\dots + g(x_N)p_N = \\sum\\limits_{i=1}^{N} g(x_i)p_i\\] Rule 1 The expected value of a constant, for example , is that constant \\[\\begin{align} E(b) = b \\end{align}\\] This rule can be easily understood following the rules of the summation operator. \\[\\begin{align} E(b) &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}b \\nonumber \\\\ &amp; = \\dfrac{1}{N}Nb \\nonumber \\\\ &amp; = b \\nonumber \\end{align}\\] Rule 2 If \\(X\\) is a random variable and \\(b\\) is a constant then, \\[\\begin{align} E(bX) = bE(X) \\end{align}\\] Again, using the rules of the summation operator and substituting back the population mean \\(\\mu_x\\) (remember that \\(E(X) = \\mu_x\\)): \\[\\begin{align} E(bX) &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}x_{i}b \\nonumber\\\\ &amp; = b\\dfrac{1}{N}\\sum_{i=1}^{N}x_{i} \\nonumber\\\\ &amp; = b\\mu_x \\nonumber \\\\ &amp; = bE(X) \\nonumber \\end{align}\\] Rule 3 The expected value of the sum of several variables is the sum of the expected values. If X, Y, and Z are three random variables, then, \\[\\begin{align} E(X + Y + Z) = E(X) + E(Y) + E(Z) \\end{align}\\] The rules of the sigma operator can be applied also to this case. \\[\\begin{align} E(X + Y + Z) &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}(x_{i} + y_{i} + z_{i}) \\nonumber\\\\ &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}x_{i} + \\dfrac{1}{N}\\sum_{i=1}^{N}y_{i} + \\dfrac{1}{N}\\sum_{i=1}^{N}z_{i} \\nonumber\\\\ &amp; = \\mu_x + \\mu_y + \\mu_z \\nonumber\\\\ &amp; = E(X) + E(Y) + E(Z) \\nonumber \\end{align}\\] Note also that \\(E(X^2) \\neq [E(X)]^2\\) and that for non-linear functions \\(E[g(X)] \\neq g[E(X)]\\). and can be seen with simple numerical examples. This is left as an exercise. Covariance The population between two random variables X and Y \\(cov(X,Y)\\) is defined as the expected value of the product of the deviation of the variables from their respective means. \\[\\begin{align} cov(X,Y) &amp; = E\\{(X-\\mu_x)(Y-\\mu_y)\\} \\\\ &amp; = E[XY - \\mu_xY - \\mu_yX + \\mu_y\\mu_x]\\nonumber\\\\ &amp; = E(XY) - \\mu_xE(Y) - \\mu_y(X) + E(\\mu_x\\mu_y)\\nonumber\\\\ &amp; = E(XY) - \\mu_x\\mu_y - \\mu_y\\mu_x + \\mu_x\\mu_y\\nonumber\\\\ &amp; = E(XY) - \\mu_x\\mu_y\\nonumber\\\\ &amp; = E(XY) - E(X)E(Y)\\nonumber \\end{align}\\] On the contrary, two random variables \\(X\\) and \\(Y\\) are said to be independent if \\(cov(X,Y) = 0\\). Only in this case, \\[\\begin{align} E(XY) = E(X)E(Y) \\end{align}\\] Rule 1 If \\(Y = V + W\\), then, \\[\\begin{align} cov(X, Y) = cov(X, V) + cov(X, W) \\end{align}\\] PROOF for Rule 1: \\[\\begin{align} cov(X,Y) &amp; = E\\{(X - \\mu_x)(Y - \\mu_y)\\} \\nonumber \\\\ &amp; = E\\{(X - \\mu_x)([V + W] - [\\mu_v + \\mu_w])\\} \\nonumber \\\\ &amp; = E\\{(X - \\mu_x)(V - \\mu_v) + (X - \\mu_x)(W - \\mu_w)\\}\\nonumber \\\\ &amp; = cov(X, V) + cov(X, W)\\nonumber \\end{align}\\] Rule 2 If \\(Y = bZ\\), where is a constant and a random variable, then, \\[\\begin{align} cov(X, Y) = b\\cdot cov(X, Z) \\end{align}\\] PROOF for Rule 2 \\[\\begin{align} cov(X,Y) &amp; = E\\{(X - \\mu_x)(Y - \\mu_y)\\}\\nonumber \\\\ &amp; = E\\{(X - \\mu_x) (bZ - b\\mu_z)\\}\\nonumber \\\\ &amp; = bE\\{(X - \\mu_x) (Z - \\mu_z)\\}\\nonumber \\\\ &amp; = bcov(X,Z)\\nonumber \\end{align}\\] Rule 3 If \\(Y = b\\), where is a constant, then, \\[\\begin{align} cov(X, Y) = 0 \\end{align}\\] PROOF for Rule 3: \\[\\begin{align} cov(X,Y) &amp; = E\\{(X - \\mu_x)(Y - \\mu_y)\\}\\nonumber \\\\ &amp; = E\\{(X - \\mu_x) (b - b)\\}\\nonumber \\\\ &amp; = E\\{0\\}\\nonumber \\end{align}\\] Variance The population of a random variable \\(X\\) can be understood as a measure of the dispersion of its probability distribution. It is defined as the expected (or average) squared deviation of its values from the mean. \\[\\begin{align} var(X) &amp; = E\\{(X - \\mu_x)^2\\}\\nonumber \\\\ &amp; = E(X^2 - 2\\mu_xX + \\mu_{X}^{2})\\nonumber \\\\ &amp; = E(X^2) + E(-2\\mu_xX) + E(\\mu_{X}^{2})\\nonumber \\\\ &amp; = E(X^2) - 2\\mu_xE(X) + \\mu_{X}^{2}\\nonumber \\\\ &amp; = E(X^2) - 2\\mu_x\\mu_x + \\mu_{X}^{2}\\nonumber \\\\ &amp; = E(X^2) - \\mu_x^{2}\\nonumber \\end{align}\\] We can also think of a random variable \\(X\\) composed of two entities, the population mean \\(\\mu_x\\) and a disturbance term or random component \\(u\\). \\[\\begin{align} X = \\mu_x + u \\hspace{1cm}\\text{from which it follows that}\\hspace{1cm} u = X - \\mu_x \\end{align}\\] We can show that the expected value of \\(u\\) is zero and that the variance of X is the same as the variance of \\(u\\). In other words, the variance of X solely depends on the variance of \\(u\\) and not of its mean (!). \\[\\begin{align} E(u) = E(X - \\mu_x) = E(X) + E(- \\mu_x) = \\mu_x - \\mu_x = 0 \\end{align}\\] We know that, \\[\\begin{align} var(X) = E\\{(X - \\mu_x)^2\\} = E(u^2) \\end{align}\\] and, \\[\\begin{align} var(u) = E\\{(u - \\text{mean of}\\hspace{0.1cm} u)^2 \\} = E\\{(u - 0)^2 \\} = E(u^2) \\end{align}\\] therefore, It is also useful and pretty straight forward to note that the variance of a random variable \\(X\\) can be thought of the covariance of \\(X\\) with itself: \\[\\begin{align} var(X) &amp; = E\\{(X - \\mu_x)^2\\}\\nonumber \\\\ &amp; = E\\{(X - \\mu_x)(X - \\mu_x)\\}\\nonumber \\\\ &amp; = cov(X, X)\\nonumber \\end{align}\\] Rule 1 If \\(Y = V + W\\), then, \\[\\begin{align} var(Y) = var(V) + var(W) + 2cov(V, W) \\end{align}\\] PROOF for Rule 1: \\[\\begin{align} var(Y) &amp; = cov(Y,Y) \\nonumber \\\\ &amp; = cov(Y,[V + W]) \\nonumber \\\\ &amp; = cov(Y,V) + cov(Y,W) \\nonumber \\\\ &amp; = cov(V,[V + W]) + cov([V + W],W) \\nonumber \\\\ &amp; = cov(V,V) + cov(W,V) + cov(V,W) + cov(W,W) \\nonumber \\\\ &amp; = var(V) + var(W) + 2cov(V, W) \\nonumber \\\\ \\end{align}\\] Rule 2 If \\(Y = bZ\\), where \\(b\\) is a constant, then, \\[\\begin{align} var(Y) = b^2var(Z) \\end{align}\\] PROOF for Rule 2: \\[\\begin{align} var(Y) &amp; = cov(Y,Y)\\nonumber \\\\ &amp; = cov(bZ,Y)\\nonumber \\\\ &amp; = bcov(Z,Y)\\nonumber \\\\ &amp; = bcov(Z,bZ)\\nonumber \\\\ &amp; = b^2cov(Z,Z)\\nonumber \\\\ &amp; = b^2var(Z)\\nonumber \\end{align}\\] Rule 3 If \\(Y = b\\), where \\(b\\) is a constant, then, \\[\\begin{align} var(Y) = 0 \\end{align}\\] PROOF for Rule 3: \\[\\begin{align} var(Y) &amp; = cov(b,b)\\nonumber \\\\ &amp; = 0 \\nonumber \\end{align}\\] Rule 4 If \\(Y = V + b\\), where \\(b\\) is a constant, then, \\[\\begin{align} var(Y) = var(V) \\end{align}\\] PROOF for Rule 4: \\[\\begin{align} var(Y) &amp; = var(V + b) \\nonumber \\\\ &amp; = var(V) + var(b) + 2cov(V,b)\\nonumber \\\\ &amp; = var(V) \\nonumber \\\\ \\end{align}\\] "],["chapter6.html", "Chapter 6 Inference", " Chapter 6 Inference The basics of hypothesis testing1 In descriptive statistics, we work with a sample of data obtained from a larger population and we are interested in understanding the characteristics of the sample. In inferential statistics, we use the sample to try to obtain conclusions (more precisely said, to make inference) regarding the characteristics of the population from which the sample is obtained. For example, having a sample available we might be interested in inferring whether the sample mean (sample statistics, \\(\\bar{x}\\)) is representative of the population mean (population parameter, \\(\\mu\\)). That is, we are interested in generalising the information obtained from the sample to the entire population. Of course it is easy to imagine that our conclusion will be subject to error as we only have one sample available. With only one sample available, it is very unlikely that the sample mean will be identical to the population mean. So in our inference process we have to take this sampling error into account. Hypothesis testing is a widely used statistical procedure for making inference from a sample to a population.2 The test starts with two statements. The null hypothesis, denoted \\(H_0\\). The null hypothesis describes the condition that is assumed to be true at the time. It is often compared to the situation in court where an accused person is assumed to be innocent until there is enough evidence to find him guilty. The second statement is the alternative hypothesis, denote \\(H_1\\). The null hypothesis and the alternative hypothesis are mutually exclusive. The alternative hypothesis is the one that is favoured if enough evidence is found. Many times we have a theory that suggests which values to specify for the null hypothesis and the alternative hypothesis. Hypothesis testing is a means of testing the statistical (not practical) validity of our theory. \\[H_0: \\mu = \\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\] How do we decide between the null hypothesis and the alternative hypothesis? We assume that we have a normally distributed random variable \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Let us further assume that the population mean \\(\\mu\\) (which we do not know) is a value equal to \\(\\mu_0\\) (maybe because theory or experience tells us so). In a first sample obtained from our random variable \\(X\\), we compute the mean (\\(\\bar{x}\\)) and obtain a value close to the hypothesized value \\(\\mu_0\\). Since we know that data are obtained from a normally distributed \\(X\\), we know that the probability of obtaining a value close to the mean is actually quite high. In a second sample, a much larger value of \\(\\bar{x}\\) is obtained. In this case, we know that obtaining values much larger than the mean is less probable (in other words, we are in the tales of the distribution). What should we conclude with such a high value now? Does the sample results confirm or contradict the null hypothesis, i.e. the hypothesized value for the mean of the population? We can continue to believe that the null hypothesis is correct and that we have just an unlucky sample from which we have obtained a very high mean but we are somehow convinced that the hypothesized value is still correct (\\(H_0: \\mu = \\mu_0\\)) and the sample was just a bit off. Or we can convince ourselves that the sample result contradicts the assumed value \\(\\mu_0\\) and that we were wrong with the null hypothesis (therefore, \\(H_1: \\mu \\neq \\mu_0\\)). In the rule, the null hypothesis is rejected if the probability of obtaining such an extreme value is smaller than an arbitrarily defined probability. There are two kinds of mistakes that can be made. A Type I error and a Type II error. When we reject a hypothesis that is actually true, we are committing a Type I error. When we are not rejecting (or accepting, but this is not the right terminology) a hypothesis that should be rejected, we are committing a Type II error. Since we are basing our decision on a sample, and there is always uncertainty in the sampling process, we are more or less sure that there is some uncertainty attached to our test conclusion. In hypothesis testing we decide in advance what kind of error we want to make. The procedure is to decide in advance to commit to a certain Type I error, e.g. we decide in advance to tolerate a Type I error in 5% of the cases. This value is called significance level and is traditionally indicated with the greek letter \\(\\alpha\\).3 The discussion presented here is based on Wooldridge, J. Introductory Econometrics: A Modern Approach (Chapter 4 and Appendix C) and Dougherty, C. Introduction to Econometrics (Review chapter). This procedure is part of the branch of statistics called parametric statistics. In fact, it is assumed that the population from which the sample is obtained follows some kind of distribution (\\(t\\) distribution, normal distribution, \\(F\\) distribution) to which reference will be made to compare the plausibility of the sample statistics. Another branch of statistics is called non-parametric and does not assume any kind of distribution. Again, \\(\\alpha\\) is the probability of rejecting a true null hypothesis. Traditionally in econometrics, we use three levels of significance 10%, 5% and 1%. "],["example.html", "Chapter 7 Example 7.1 The normal distribution6", " Chapter 7 Example The mean lifetime of a sample of 100 light bulbs is computed to be 1570 hours with a standard deviation of 120 hours. If \\(\\mu\\) is the mean lifetime of all the light bulbs produced, test the hypothesis that the population mean is \\(\\mu = 1600\\) hours against the alternative that \\(\\mu \\neq 1600\\) using a significance level of 5%. Find the p-value of the test and build a 95% confidence interval. \\[H_0: \\mu = 1600\\] \\[H_1: \\mu \\neq \\space 1600\\] We need to construct our test statistic to perform the test. In practice, wee need to transform the computed mean obtained from the sample of light bulbs into a statistic that follow a standard normal distribution with zero mean and unit variance. \\[z_{test} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{1570 - 1600}{12} = -2.5\\] ## [1] 2.5 Since in this exercise we are interested in performing a two-sided test (look a the formulation of the null and alternative hypothesis), we take the absolute value of our \\(z\\) test. We can reject \\(H_0\\) at the 5% significance level (\\(\\alpha\\) = 5%) if, \\[|z_{test}| &gt; z_{crit}\\] ## [1] TRUE We can reject \\(H_0\\) at the 5% significance level. In the following graph, in green we can see the rejection region (the two green shaded areas add up to 5%) while with the yellow line it is indicated the value of our \\(z\\) test in both tails of the distribution. As we said, since the value of our \\(z\\) test falls within the rejection region, we can reject \\(H_0\\) at the 5% significance level. What do we see highlighted in black in the graph above? The sum of the two black areas represent our p-value for the z score that we have just calculated. We can think of the p-value as the smallest significance level at which we still reject the null hypothesis (or the largest significance level at which the null hypothesis cannot be rejected). How large is the p-value for our z test? ## [1] 1.241933 How can we interpret the p-value? In our exercise, the p-value of 1.24% represents the probability that a mean lifetime of less than 1570 or more than 1630 hours would occur by chance if \\(H_0\\) were actually true. A rather small probability. How did we obtain the values 1570 and 1630? We have used (half) of the p-value to calculate the quantiles of a normal distribution with mean \\(mu = 1600\\) and standard deviation \\(120/\\sqrt{100}\\). ## [1] 1630 ## [1] 1570 ## [1] 1.241933 Since we see that the p-value is larger than 1%, we already know that we will fail to reject the null hypothesis at the 1% significance level (\\(\\alpha\\) = 1%) . Shouldnt we have used a t test rather than a \\(z\\) test as suggested by professional statisticians? Probably yes. Since we do not know the population standard deviation (the standard deviation of all light bulbs), a t test sounds more appropriate. However, since our sample is relatively large (\\(n = 100\\)), much larger than the commonly suggested rule of thumb (\\(n &gt; 30\\)), we will practically obtain (almost) the same result using the normal distribution and the t distribution.4 To conclude, we need to construct the 95% confidence interval (\\(100 - \\alpha\\), where \\(\\alpha\\) = 5%). \\[(\\bar{x} + z_{\\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] We must be careful with the signs in the formula above. Once we look up the critical value \\(z_{\\alpha / 2}\\) (or we compute it using R) we will see that the quantity is actually negative. If we include the negative sign in the formula for the confidence interval, it would not be wrong, but it can create some confusion. This is why it is better to use the formula below. In this case, we have to include the negative sign in the formula because \\(z_{1 - \\alpha / 2}\\) is going to be a positive number and this is the version that we are going to implement in R. \\[(\\bar{x} - z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] ## [1] 1546.48 1593.52 The hypothesized value for the mean lifetime of all the bulbs produced was 1600 working hours. Since this particular value falls outside the confidence interval constructed around the sample mean, we can reject \\(H_0\\). With the confidence interval test the null hypothesis is rejected if and only if the hypothesized value falls outside the confidence interval. The \\(z\\) test (or \\(t\\) test) and the confidence interval test are basically an elaboration of one another and provides always the same test decision.5 7.1 The normal distribution6 The normal distribution is a common probability distribution in statistics and econometrics (it is just one of many distributions). The normal distribution fits a number of natural and social phenomena. When a phenomenon (a random variable) has a normal distribution, its probability density function (for short, PDF) assumes the well-known bell-shaped curve. The normal distribution is sometimes called the Gaussian distribution or the Gauss curve in honor of the famous mathematician Carl-Friedrich Gauss.7 Shape and position of the normal distribution are entirely determined by mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) of the normally distributed random variable. This is written as: \\[X \\sim Normal(\\mu, \\sigma)\\] For example, we see that the two normal distributions shown in the following graph have the same mean but different standard deviations. The mean determines the location of the normal distribution in the horizontal axis. The majority of the body is located around the mean to which correspond the peak in the distribution. The standard deviation determines the shape of the curve. In practice, it determines how far the values of the variable are from the mean. This means that a higher mean shifts the curve to the right without changing its shape. Similarly, a higher standard deviation widens the body of the curve without shifting its position on the horizontal axis. The normal distribution has a number of interesting and useful properties. First of all, it is symmetrical with respect to the mean, from which it follows that half of the values are distributed half to the right and half to the left of the mean. Knowing the mean and standard deviation of a certain event or random variable, the normal distribution allows us to calculate the probability that the event will assume a certain value or range of values. Roughly speaking, this correspond to the area below the curve. In reality, this is done using the cumulative distribution function (CDF) which is nothing more than the integral of the PDF. The following figure shows the relationship between PDF (left) and CDF (right) of a normally distributed random variable with mean 0 and standard deviation 1. If we want to see why this is the case, take a look here. For common misunderstandings about the confidence interval, see here. The discussion presented here and in particular the proof of mean and standard deviation of the the standardized random variable rely on Wooldridge, J. Introductory Econometrics: A Modern Approach (Appendix C). At first glance, many phenomena do not appear to follow a normal distribution. However, after a logarithmic transformation they assume a (log)normal distribution. "],["the-standard-normal-distribution.html", "Chapter 8 The standard normal distribution", " Chapter 8 The standard normal distribution A special case of normal distribution is the standard normal distribution where the mean is equal to 0 and the standard deviation is equal to 1 (this is actually what we used in the previous exercise but we had not yet used this term). Let us now see how it is possible to standardise a variable. This is a very important procedure that we will see again later when we talk about hypothesis testing. In the following section we will instead use it to calculate the so-called \\(z\\) scores. \\[Z = \\frac{X - \\mu}{\\sigma}\\] Rewriting \\(Z\\) as \\(aX + b\\), where \\(a = (1/\\sigma)\\) and \\(b = -(\\mu/\\sigma)\\) and using the properties of expectation and variance we can see that: \\[E(Z) = aE(X) + b = (\\mu/\\sigma) - (\\mu/\\sigma) = 0\\] \\[Var(Z) = a^2Var(X) = (\\sigma^2/\\sigma^2) = 1\\] What does that mean? It means that if we subtract the mean from a variable (\\(X\\)) and divide it by the standard deviation we will have a standardised variable (\\(Z\\)) that has a mean of zero and standard deviation of 1. "],["exercises.html", "Chapter 9 Exercises", " Chapter 9 Exercises We are given the following set of numbers: \\(6, 2 , 8, 7, 5\\). Transform the set into standard scores and check that mean and standard deviation of the transformed set are respectively 0 and 1. ## [1] 1.387779e-16 ## [1] 1 Let us assume that the random variable \\(X\\) is a normally distributed random variable with mean (\\(\\mu\\)) equal to 5 and population standard deviation (\\(\\sigma\\)) equal to 4. In short, \\(Normal ~ (5,4)\\). Calculate the probabilities that our random variable \\(X\\) assume a value smaller than 6, \\(P(X \\leq 6)\\), using the table of the standard normal probabilities or R (much better!). If we did not have R available we would have to find the \\(z\\) score corresponding to the value of interest, 6 in this case, and look in the table of standard normal probabilities (the area below the curve) the probability that our random variable assumes a value smaller than that.8 ## [1] 0.25 \\[z = \\frac{6 - 5}{4} = 0.25\\] Our \\(z\\) value of interest is 0.25. The probability that the variable \\(X\\) takes on a value less than 6 is given by the area under the normal curve to the left of \\(z = 0.25\\). This value is equal to: ## [1] 59.87063 We can achieve the same result by using the lower.tail = FALSE option. In this case we get the white area in the graph below and will have to subtract this quantity from 1 or 100%, i.e. the whole area under the curve. ## [1] 59.87063 In the graph below, the area marked in green indicates the probability that the independent variable \\(X\\) takes on a value less than 6 given mean and population standard deviation of 5 and 4, respectively. If we have software at our disposal we do not have to use tables. In this case there is no need to calculate the \\(z\\) score. The result (and the graph) will be exactly the same with the important difference that now the values reported in the horizontal axis will be the values of \\(X\\) and not the standardized scores. ## [1] 59.87063 The test scores for a class of students (this is the population) are normally distributed with mean (\\(\\mu\\)) equal to 75 points and standard deviation (\\(\\sigma\\)) equal to 10 points. What is the probability that a students scores above 80 points? Calculate the following probabilities: Given \\(X \\sim Normal(3,4)\\), find \\(P(X \\leq 1)\\) Given \\(X \\sim Normal(4,0)\\), find \\(P(2 &lt; X \\leq 6)\\) Since the normal distribution is continuous, \\(P(Z &lt; z) = P(Z \\leq z)\\). "],["chapter7.html", "Chapter 10 Econometrics 1 10.1 The linear regression model", " Chapter 10 Econometrics 1 10.1 The linear regression model Part 19 We are given the following set of values: We want to find the line that minimizes the sum of squared residuals, i.e. the squared distance between the observed value and the line. In a simple bivariate case as this one, we have to find a slope (\\(\\hat{\\beta_1}\\)) and an intercept (\\(\\hat{\\beta_0}\\)) for the line so that the sum of squared residuals is as small as possible. \\[\\begin{align} y_i &amp; = \\hat{y_i} + \\hat{u_i} \\\\ \\hat{y_i} &amp; = \\hat{\\beta_0} + \\hat{\\beta_1}x_i \\\\ y_i &amp; = \\hat{\\beta_0} + \\hat{\\beta_1}x_i + \\hat{u_i} \\\\ \\hat{u_i} &amp; = y_i - \\hat{y_i} \\\\ \\hat{u_i} &amp; = y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}x_i) \\end{align}\\] We have now a function that we want to minimize with respect to \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). \\[\\begin{align} f(\\hat{\\beta_0}, \\hat{\\beta_1}) = \\sum_{n=1}^{3} \\hat{u_i}^2 = \\sum_{n=1}^{3} (y_i - \\hat{y_i})^2 = \\sum_{n=1}^{3} (y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}x_i))^2 = \\sum_{n=1}^{3} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)^2 \\end{align}\\] We now take the derivative of (6) with respect to \\(\\hat{\\beta_0}\\) and set it equal to 0. We then do the same thing for \\(\\hat{\\beta_1}\\). These are the so-called . \\[\\begin{align} \\dfrac{\\partial f(\\hat{\\beta_0}, \\hat{\\beta_1})}{\\partial \\hat{\\beta_0}} &amp; = -2\\sum_{n=1}^{3} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0 \\\\ \\dfrac{\\partial f(\\hat{\\beta_0}, \\hat{\\beta_1})}{\\partial \\hat{\\beta_1}} &amp; = -2\\sum_{n=1}^{3} x_i (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0 \\end{align}\\] ## [1] 5.5 4.0 3.5 ## [1] -0.5 2.0 -1.5 ## [1] 6.5 ## [1] 2.166667 ## [1] 8.666667 ## [1] 8.666667 ## [1] 0.25 ## [1] 0.25 ## [1] 2.54951 Dependent variable: y x -0.250 (0.433) Constant 4.750 (1.639) Observations 3 R2 0.250 Residual Std. Error 2.550 (df = 1) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Coefficients: ## (Intercept) x I(x^2) ## 7.9375 0.1667 -0.2708 Which model has the largest \\(R^2\\)? Dependent variable: y 1 2 x 0.162 0.162* (0.360) (0.080) x -0.284*** (0.015) Constant -2.140 8.281*** (2.182) (0.724) Observations 21 21 R2 0.010 0.954 Residual Std. Error 9.999 (df = 19) 2.208 (df = 18) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Text and the derivation above rely on the suggested literature: Wooldridge, J. Introductory Econometrics: A Modern Approach (Chapter 2) and Dougherty, C. Introduction to Econometrics (Chapter 1). "],["chapter8.html", "Chapter 11 Econometrics 2 11.1 The basics of hypothesis testing10", " Chapter 11 Econometrics 2 11.1 The basics of hypothesis testing10 In descriptive statistics, we work with a sample of data obtained from a larger population and we are interested in understanding the characteristics of the sample. In inferential statistics, we use the sample to try to obtain conclusions (more precisely said, to make inference) regarding the characteristics of the population from which the sample is obtained. For example, having a sample available we might be interested in inferring whether the sample mean (sample statistics, \\(\\bar{x}\\)) is representative of the population mean (population parameter, \\(\\mu\\)). That is, we are interested in generalising the information obtained from the sample to the entire population. Of course it is easy to imagine that our conclusion will be subject to error as we only have one sample available. With only one sample available, it is very unlikely that the sample mean will be identical to the population mean. So in our inference process we have to take this sampling error into account. Hypothesis testing is a widely used statistical procedure for making inference from a sample to a population.11 The test starts with two statements. The null hypothesis, denoted \\(H_0\\). The null hypothesis describes the condition that is assumed to be true at the time. It is often compared to the situation in court where an accused person is assumed to be innocent until there is enough evidence to find him guilty. The second statement is the alternative hypothesis, denote \\(H_1\\). The null hypothesis and the alternative hypothesis are mutually exclusive. The alternative hypothesis is the one that is favoured if enough evidence is found. Many times we have a theory that suggests which values to specify for the null hypothesis and the alternative hypothesis. Hypothesis testing is a means of testing the statistical (not practical) validity of our theory. \\[H_0: \\mu = \\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\] How do we decide between the null hypothesis and the alternative hypothesis? We assume that we have a normally distributed random variable \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Let us further assume that the population mean \\(\\mu\\) (which we do not know) is a value equal to \\(\\mu_0\\) (maybe because theory or experience tells us so). In a first sample obtained from our random variable \\(X\\), we compute the mean (\\(\\bar{x}\\)) and obtain a value close to the hypothesized value \\(\\mu_0\\). Since we know that data are obtained from a normally distributed \\(X\\), we know that the probability of obtaining a value close to the mean is actually quite high. In a second sample, a much larger value of \\(\\bar{x}\\) is obtained. In this case, we know that obtaining values much larger than the mean is less probable (in other words, we are in the tales of the distribution). What should we conclude with such a high value now? Does the sample results confirm or contradict the null hypothesis, i.e. the hypothesized value for the mean of the population? We can continue to believe that the null hypothesis is correct and that we have just an unlucky sample from which we have obtained a very high mean but we are somehow convinced that the hypothesized value is still correct (\\(H_0: \\mu = \\mu_0\\)) and the sample was just a bit off. Or we can convince ourselves that the sample result contradicts the assumed value \\(\\mu_0\\) and that we were wrong with the null hypothesis (therefore, \\(H_1: \\mu \\neq \\mu_0\\)). In the rule, the null hypothesis is rejected if the probability of obtaining such an extreme value is smaller than an arbitrarily defined probability. There are two kinds of mistakes that can be made. A Type I error and a Type II error. When we reject a hypothesis that is actually true, we are committing a Type I error. When we are not rejecting (or accepting, but this is not the right terminology) a hypothesis that should be rejected, we are committing a Type II error. Since we are basing our decision on a sample, and there is always uncertainty in the sampling process, we are more or less sure that there is some uncertainty attached to our test conclusion. In hypothesis testing we decide in advance what kind of error we want to make. The procedure is to decide in advance to commit to a certain Type I error, e.g. we decide in advance to tolerate a Type I error in 5% of the cases. This value is called significance level and is traditionally indicated with the greek letter \\(\\alpha\\).12 The discussion presented here is based on Wooldridge, J. Introductory Econometrics: A Modern Approach (Chapter 4 and Appendix C) and Dougherty, C. Introduction to Econometrics (Review chapter). This procedure is part of the branch of statistics called parametric statistics. In fact, it is assumed that the population from which the sample is obtained follows some kind of distribution (\\(t\\) distribution, normal distribution, \\(F\\) distribution) to which reference will be made to compare the plausibility of the sample statistics. Another branch of statistics is called non-parametric and does not assume any kind of distribution. Again, \\(\\alpha\\) is the probability of rejecting a true null hypothesis. Traditionally in econometrics, we use three levels of significance 10%, 5% and 1%. "],["example-1.html", "Chapter 12 Example", " Chapter 12 Example The mean lifetime of a sample of 100 light bulbs is computed to be 1570 hours with a standard deviation of 120 hours. If \\(\\mu\\) is the mean lifetime of all the light bulbs produced, test the hypothesis that the population mean is \\(\\mu = 1600\\) hours against the alternative that \\(\\mu \\neq 1600\\) using a significance level of 5%. Find the p-value of the test and build a 95% confidence interval. \\[H_0: \\mu = 1600\\] \\[H_1: \\mu \\neq \\space 1600\\] We need to construct our test statistic to perform the test. In practice, wee need to transform the computed mean obtained from the sample of light bulbs into a statistic that follow a standard normal distribution with zero mean and unit variance. \\[z_{test} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{1570 - 1600}{12} = -2.5\\] ## [1] 2.5 Since in this exercise we are interested in performing a two-sided test (look a the formulation of the null and alternative hypothesis), we take the absolute value of our \\(z\\) test. We can reject \\(H_0\\) at the 5% significance level (\\(\\alpha\\) = 5%) if, \\[|z_{test}| &gt; z_{crit}\\] ## [1] TRUE We can reject \\(H_0\\) at the 5% significance level. In the following graph, in green we can see the rejection region (the two green shaded areas add up to 5%) while with the yellow line it is indicated the value of our \\(z\\) test in both tails of the distribution. As we said, since the value of our \\(z\\) test falls within the rejection region, we can reject \\(H_0\\) at the 5% significance level. What do we see highlighted in black in the graph above? The sum of the two black areas represent our p-value for the z score that we have just calculated. We can think of the p-value as the smallest significance level at which we still reject the null hypothesis (or the largest significance level at which the null hypothesis cannot be rejected). How large is the p-value for our z test? ## [1] 1.241933 How can we interpret the p-value? In our exercise, the p-value of 1.24% represents the probability that a mean lifetime of less than 1570 or more than 1630 hours would occur by chance if \\(H_0\\) were actually true. A rather small probability. How did we obtain the values 1570 and 1630? We have used (half) of the p-value to calculate the quantiles of a normal distribution with mean \\(mu = 1600\\) and standard deviation \\(120/\\sqrt{100}\\). ## [1] 1630 ## [1] 1570 ## [1] 1.241933 Since we see that the p-value is larger than 1%, we already know that we will fail to reject the null hypothesis at the 1% significance level (\\(\\alpha\\) = 1%) . Shouldnt we have used a t test rather than a \\(z\\) test as suggested by professional statisticians? Probably yes. Since we do not know the population standard deviation (the standard deviation of all light bulbs), a t test sounds more appropriate. However, since our sample is relatively large (\\(n = 100\\)), much larger than the commonly suggested rule of thumb (\\(n &gt; 30\\)), we will practically obtain (almost) the same result using the normal distribution and the t distribution.13 To conclude, we need to construct the 95% confidence interval (\\(100 - \\alpha\\), where \\(\\alpha\\) = 5%). \\[(\\bar{x} + z_{\\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] We must be careful with the signs in the formula above. Once we look up the critical value \\(z_{\\alpha / 2}\\) (or we compute it using R) we will see that the quantity is actually negative. If we include the negative sign in the formula for the confidence interval, it would not be wrong, but it can create some confusion. This is why it is better to use the formula below. In this case, we have to include the negative sign in the formula because \\(z_{1 - \\alpha / 2}\\) is going to be a positive number and this is the version that we are going to implement in R. \\[(\\bar{x} - z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] ## [1] 1546.48 1593.52 The hypothesized value for the mean lifetime of all the bulbs produced was 1600 working hours. Since this particular value falls outside the confidence interval constructed around the sample mean, we can reject \\(H_0\\). With the confidence interval test the null hypothesis is rejected if and only if the hypothesized value falls outside the confidence interval. The \\(z\\) test (or \\(t\\) test) and the confidence interval test are basically an elaboration of one another and provides always the same test decision.14 If we want to see why this is the case, take a look here. For common misunderstandings about the confidence interval, see here. "],["the-t-test-in-the-regression.html", "Chapter 13 The t test in the regression 13.1 F statistic for overall significance of a regression 13.2 The F test", " Chapter 13 The t test in the regression We are now going to perform the test of the hypothesis on the population parameters of a simple bivariate linear regression model. The good news is that this procedure is done automatically by all statistical packages that run regressions. In our case, we are going to replicate the results provided with the output of the \\(lm\\) function in R. First, we perform a two-sided t test for the intercept parameter (\\(\\beta_0\\)). We want to test the null hypothesis (\\(H_0\\)) that \\(\\beta_0 = 0\\) against the alternative hypothesis (\\(H_1\\)) that \\(\\beta_0 \\neq 0\\). This is the kind of hypothesis test that is usually performed by linear regression function in software packages. Since we are interested in a two-sided test, we will take the absolute value of our \\(t\\) statistic. We want to perform the test at the 5% significance level. We need to find the critical value for a 5% significance level with 524 degrees of freedom. We reject \\(H_0\\) if \\(|t_{\\hat{\\beta}_0}| &gt; t_{crit}\\) otherwise we will fail to reject \\(H_0\\). ## [1] FALSE We fail to reject \\(H_0\\) at the 5% significance level. Consequently, we will fail to reject the \\(H_0\\) for every significance level smaller than 5%. Which one is the smallest significance level at which we still reject \\(H_0\\)? We need to calculate the (in)famous p-value. In this case, it is given by \\(P(|T| &gt; |t_{\\hat{\\beta}_0}|)\\).15 We can use the function pt to calculate the area below the t distribution at the right of our \\(t_{\\hat{\\beta}_0}\\). Dont forget to multiply this quantity by two as we need the p-value for a two-sided test. ## [1] 18.70735 We have a relatively large sample. We know that the t distribution converges to the standard normal distribution as the number of observation in the sample approaches infinity (meaning that the quantiles of the t distribution and the standard normal distribution are the same). Therefore, we can obtain (almost) the same result using the function pnorm. ## [1] 18.64969 We can check that we will actually reject \\(H_0\\) for a significance level larger than our p-value. Lets try to see whats happening when the significance level is 20%.16 As before, we reject \\(H_0\\) if \\(|t_{\\hat{\\beta}_0}| &gt; t_{crit}\\) otherwise we will fail to reject \\(H_0\\). ## [1] TRUE Given the result of the test, we reject \\(H_0\\) at the 20% significance level. We can also build a 95% confidence interval for the intercept:17 ## [1] -2.250472 ## [1] 0.4407687 Since the value of 0 is contained in the confidence interval, we fail to reject \\(H_0\\) at 5% significance level. The \\(H_0\\) is rejected if and only if the value 0 is not contained in the 95% confidence interval. There is no need to repeat all these steps above every time. The confint function allows us to quickly calculate the confidence interval. ## 2.5 % 97.5 % ## (Intercept) -2.250472 0.4407687 Now, we perform a two-sided t test for the slope parameter. We want to test the null hypothesis (\\(H_0\\)) that \\(\\beta_1 = 0\\) against the alternative hypothesis (\\(H_1\\)) \\(\\beta_1 \\neq 0\\). Since we are interested in a two-sided test, we will take the absolute value of our t statistic. As we know, we reject \\(H_0\\) if \\(|t_{\\hat{\\beta}_1}| &gt; t_{crit}\\), otherwise, we fail to reject \\(H_0\\). ## [1] TRUE What is the conclusion of the test? We can reject \\(H_0\\) that \\(\\beta_1 = 0\\) at the 5% significance level. Can we reject \\(H_0\\) also at the 1% significance level? We must first calculate the critical value for a 1% significance level with 524 degrees of freedom. ## [1] TRUE We can reject \\(H_0\\) that \\(\\beta_1 = 0\\) also at the 1% significance level. We can also build a 95% confidence interval for the slope parameter. ## [1] 0.4367534 ## [1] 0.6459651 Since the value of 0 is not contained in the confidence interval, we reject \\(H_0\\) at 5% significance level. Also here we can check our result using the confint function. ## 2.5 % 97.5 % ## educ 0.4367534 0.6459651 13.1 F statistic for overall significance of a regression 13.2 The F test The F test is used to test whether a group of variables has no effect on the dependent variable. In this sense, the test allows to test if the parameters of a set (or at the limit all) the independent variables are jointly significance. Obviously it is the theory or intuition that tells us to operate such a test on a given group of variables. It is often the case that the F test is performed on all independent variables in a model. It is then said that the test is for the overall significance of the regression. In this exercise, to understand how the F test works in practice, we will replicate the F test provided by the regression function in R (as by any other statistical software packages). This is precisely a test for overall joint significance of the regression. We estimate the following model: \\[wage = \\beta_0 + \\beta_1educ + \\beta_2exper + \\beta_3tenure + u\\] We formulate the following joint null hypothesis (\\(H_0\\)) stating that the regressors have jointly no effect on the dependent variable: \\[H_0 : \\beta_1 = \\beta_2 = \\beta_3 = 0\\] The alternative hypothesis (\\(H_1\\)) is: \\[H_1 : H_0 \\space \\text{is not true}\\] The formula for the F statistic (or F ratio), where \\(q\\) is the number of restrictions (in this example we are imposing three restrictions), and \\(n-k-1\\) is the number of degrees of freedom of the unrestricted model, is defined by:18 \\[F = \\frac{SSR_r - SSR_{ur}}{SSR_{ur}} * \\frac{(n-k-1)}{q}\\] First, we estimate the unrestricted model. With this term, we mean the entire or complete model: We can now calculate the sum of squared residual (SSR) of the unrestricted model: We then estimate the restricted model. The restricted model has clearly less parameters than the unrestricted model. Since we are performing an F test for the overall significance of the regression, we must regress the dependent variable wage on just an intercept. In R, this is done by including only a 1 after the tilde sign in the lm function. We can now calculate the SSR of the restricted model. We report the results in a single table created using the stargazer package. Finally, we can calculate the F statistic and its corresponding p-value. We compare the value of our F statistic (and its p-value) with the value provided by R (see the last row of the first column in the table above). ## [1] 76.87317 ## [1] 3.405862e-41 We choose a significance level (\\(\\alpha\\)) of 1% and calculate the corresponding critical value in the F distribution. ## [1] 3.819327 What is the conclusion of the test? We can observe that our F value is clearly larger the critical value for the chosen significance level of 1%. Our p-value is also very very small, certainly smaller than the significance level of 1%. We can therefore soundly reject the null hypothesis that the variables are not jointly significant. We can also create the graph of the F distribution. In green we mark the rejection region for the significance level that we have choosen. See Wooldridge p. 126. Such a large significance level is never used in practice! See Wooldridge p. 130. The number of degrees of freedom of the unrestricted model is given by \\(n-k-1\\) where \\(n\\) is the number of observations, \\(k\\) is the number of independent variables and \\(1\\) stands for the coefficient of the intercept. "],["chapter9.html", "Chapter 14 Econometrics 3 14.1 Linear growth model 14.2 The consumption function", " Chapter 14 Econometrics 3 14.1 Linear growth model Time Series: Real Gross Domestic Product of USA (seasonally not adjusted) Observations: from 1960 to 2007 Units: Billions of dollars Data: https://fred.stlouisfed.org/ Source: Gujarati, D. Econometrics by Example 14.1.1 Level-level model ## DATE TIME GDPCA ## 1 1960 1 3108.707 ## 2 1961 2 3188.123 ## 3 1962 3 3383.085 ## 4 1963 4 3530.412 ## 5 1964 5 3734.043 ## ## ======================================== ## Dependent variable: ## --------------------------- ## GDPCA ## ---------------------------------------- ## TIME 242.900*** ## (6.697) ## ## Constant 1,953.991*** ## (188.490) ## ## ---------------------------------------- ## Observations 48 ## R2 0.966 ## ======================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 The slope coefficient gives us absolute change in real GDP per unit of time (year). These results show that over the period 19602007, real GDP in the USA increased on average by about $243 billion per year. 14.1.2 Log-level model We can also estimate the rate of growth of real GDP using logs. We know that: \\[RGDP_t = RGDP_{1960}*(1 + r)^t\\] Using logs we can rewrite the function above as: \\[ln(RGDP_t) = ln(RGDP_{1960}) + ln(1 + r)*t\\] Now letting \\(\\beta_0 = ln(RGDP_{1960})\\) and \\(\\beta_1 = ln(1 + r)\\) we can rewrite the model as: \\[ln(RGDP_t) = \\beta_0 + \\beta_1*t + u_t\\] ## ## ======================================== ## Dependent variable: ## --------------------------- ## lnGDPCA ## ---------------------------------------- ## TIME 0.032*** ## (0.0003) ## ## Constant 8.088*** ## (0.009) ## ## ---------------------------------------- ## Observations 48 ## R2 0.995 ## ======================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Results show that real GDP in the USA has been increasing at a rate of 3.22% per year. If we exponentiate the slope coefficient (\\(e^{\\beta_0}\\)) we obtain approximately the value of real GDP at the starting period (1960). ## [1] 3255.583 ## DATE TIME GDPCA lnGDPCA ## 1 1960 1 3108.707 8.041962 The coefficient \\(\\beta_2\\) gives us the instantaneous rate of growth. We can also calculate the compound rate of growth \\(r\\). ## [1] 3.276751 14.1.3 Quadratic regression model (level-level) We can now consider the following polynomial (i.e. quadratic) model: \\[ RGDP_t = \\beta_0 + \\beta_1*t + \\beta_2*t^2 + u_t \\] The fact that \\(t\\) and \\(t^2\\) are correlated does not raise the problem of collinearity. One of the assumption of the multiple linear regression model is that there is no perfect linear relationship between the regressors. This is because the two variables are not a linear function of each other but rather a quadratic function. ## ## ======================================== ## Dependent variable: ## --------------------------- ## GDPCA ## ---------------------------------------- ## TIME 72.987*** ## (9.011) ## ## TIME2) 3.468*** ## (0.178) ## ## Constant 3,369.931*** ## (95.719) ## ## ---------------------------------------- ## Observations 48 ## R2 0.996 ## ======================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 How can we interpret these results? In the multiple linear regression context, we interpret the partial effect of one regressor by differentiating the function with respect to that variable. In a linear model in levels, what we usually obtain is the coefficient (one of the betas) that multiply that variable. By doing so we are treating the other variables as constants. In this example, we have to differentiate with respect to \\(t\\) and we cannot treat \\(t^2\\) as a constant. \\[ \\frac{dRGDP}{dt} = \\beta_1 + 2 * \\beta_2 * t \\] \\[ \\frac{dRGDP}{dt} = 72.9873 + 2 * 3.4676 * t \\] We have obtained the rate of change in real GDP with respect to time. The rate of change in real GDP with respect to time is a positive function of time. As time increases, real GDP is increasing at an increasing rate. We can also notice that the rate of change in real GDP depends on time at which the rate of change is measured as opposed to the linear level-level model from above. 14.1.4 Quadratic regression model (log-level) We can now consider the same model as before, but with the log of real GDP: \\[ ln(RGDP_t) = \\beta_0 + \\beta_1*t + \\beta_2*t^2 + u_t \\] ## ## ======================================== ## Dependent variable: ## --------------------------- ## lnGDPCA ## ---------------------------------------- ## TIME 0.036*** ## (0.001) ## ## TIME2) -0.0001*** ## (0.00002) ## ## Constant 8.057*** ## (0.013) ## ## ---------------------------------------- ## Observations 48 ## R2 0.996 ## ======================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 What is the interpretation of what we have just obtained? \\[ \\frac{d ln(RGDP)}{dt} = \\beta_1 + 2 * \\beta_2 * t\\] \\[ \\frac{d ln(RGDP)}{dt} = 0.036 - 2 * 0.00007736 * t\\] On the left hand side of the derivative, we have the rate of growth in real GDP. From the estimated coefficients we see that real GDP is increasing at a decreasing rate. 14.2 The consumption function In this brief exercise we are going to estimate a Keynesian aggregate consumption function for the United States from 1960 to 2019. We will regress private consumption (at constant prices) on real GDP. The slope of the regression will be what in the economic literature is called the marginal propensity to consume (MPC for short). What is MPC? MPC is that value that tells us how much consumption will increase as income increases. Theory tells us that the MPC is a value between 0 and 1. When income increases by 1$, consumption will increase by a value less than the initial increase of 1$ but greater than 0. From our Macroeconomics lessons we remember that the Keynesian consumption function is represented by the following equation, where \\(C\\) indicates the aggregate household consumption, \\(Y\\) the national income (both expressed in billions of constant dollars), \\(c_Y\\) is the MPC and \\(c_{aut}\\), autonomous consumption, i.e. the part of consumption that does not depend on income. \\[C = c_{aut} + c_Y Y\\] We first get the data using the rdbnomics package and then we will produce a graph of the time series from 1960 to 2019 with the ggplot2 package. Both time series are expressed in billions of 2015 dollars. We are now ready to perform the linear regression. We will also produce the scatter plot of the data to get the graphical intuition of the regression. From the linear regression, we obtained the MPC. If income increases by 1$, consumption will increase by approximately 70 cents on average. Once the MPC is obtained, we can calculate the value of the Keynesian multiplier (\\(m\\)) as learned in Macroeconomics classes. \\[m = \\frac{1}{1 - c_Y} = \\frac{1}{1 - 0.7} = 3.33\\] However, this regression has a number of problems from an econometric perspective.19 The two series have a tendency to grow over time and the variables might seem highly correlated to us because they both have the same tendency to increase with time. This could lead to the conclusion that there is a correlation when in fact there is not (not the case here). This problem is known as spurious regression. We need to see if the independent variable is correlated with the error term. This is certainly an undesirable behavior. The residuals must be completely random and contain no predictive power. We can observe a clear pattern between residuals and income. Our coefficients are biased. If we were in a cross-sectional context, we would have to figure out which variable is missing from the model and where possible include it. Or it could be that the model is misspecified. To get to the point, econometricians would say that our two series follow a unit root process. The series are highly persistent over time and contain a, in our case positive, time trend.20 Wanting to simplify a lot we can say that our time series are not stationary and must be transformed before being used in a regression. A quick fix that works is to use logarithmic differences.21 We can see that the estimate in the slope has changed slightly. This time it is somewhat higher at 0.77. However, the interpretation of the model has changed. This time we estimated an elasticity. How can we calculate the MPC having estimated the elasticity of consumption with respect to income? From courses in Microeconomics we remind that elasticity is nothing more than the ratio of two percentage rates and that on the demand curve elasticity varies depending on where you measure it.22 In our case, our elasticity parameter (just called \\(\\epsilon_Y\\) for simplicity) is given by: \\[ \\epsilon_Y = \\frac{\\frac{\\partial C}{C}}{\\frac{\\partial Y}{Y}} = \\frac{\\partial C}{\\partial Y} \\frac{Y}{C}\\] Rearranging the terms, we get: \\[\\frac{\\partial C}{\\partial Y} = \\epsilon_Y \\frac{C}{Y} \\] The elasticity parameter is equal to 0.77 while the C/Y term that we will calculate as the average over the entire period from 1960 to 2019 is equal to 0.64. The marginal effect, the MPC, calculated on the avare over the entire period is therefore 0.4928. How do the residuals perform this time? There are actually a whole host of econometric issues that we have left out but that need to be properly addressed when estimating a model. Is there serial correlation in the residuals and if so what is the consequence? Is there heteroschedasticity in the residuals and if so what problems could it cause? There are tests appropriately developed by statisticians to identify these problems and solutions to fix them. All of these things will be the subject of the course in the following semester. The value of \\(R^2\\) almost equal to one must immediately raise doubts. The two concepts, trending behaviour and persistent behaviour, should not be confused. Please refer to the Wooldridge textbook (Chapter 11). Differencing will removes the linear trend. For a review of the elasticity concept in economics, see here. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
