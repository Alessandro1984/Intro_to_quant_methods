[["index.html", "Introduction to quantitative methods for economists About this book Idea Thanks", " Introduction to quantitative methods for economists Alessandro Bramucci 2022-06-16 About this book This book brings together a collection of notes, exercises and practical applications that I have used over the years for my lectures in the course Quantitative Methods for Economistsat the at the Berlin School of Economics and Law (HWR Berlin) in Berlin, Germany. Idea Thanks This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["chapter1.html", "Chapter 1 Code 1 1.1 Loading and managing data 1.2 Data manipulation using base R 1.3 Conditional selection", " Chapter 1 Code 1 1.1 Loading and managing data To begin with, lets see how to install a new package in R. We are going to install the wooldridge package which contains all the datasets used in the book Introductory Econometrics by Jeffrey M. Wooldridge. Additional information regarding the datasets can be found in the following document. Now, let us see how to unload a package from the library. The quickest way is to untick the checkbox of the package you want to remove in the Packages window. The following code is automatically executed in the console. There is no need to copy it or run it again. Another, perhaps more radical, way to unload all packages is to restart the RStudio session. In this case, the keyboard shortcut CTRL + SHIFT + F10 can be used. After loading the dataset, in our case contained in the wooldridge package, we can take a look at the data. It is important to take a look at the data, for example after having modified existing variables or having created new ones. The dataset we use in this exercise is wage1. We can take a glance at the data using the View function. 1.2 Data manipulation using base R 1.3 Conditional selection "],["chapter2.html", "Chapter 2 Code 2", " Chapter 2 Code 2 "],["chapter3.html", "Chapter 3 Code 3", " Chapter 3 Code 3 "],["chapter4.html", "Chapter 4 Math 4.1 Derivative rules 4.2 Matrix operations", " Chapter 4 Math 4.1 Derivative rules The rules for sums and differences Given $ f(x) = g(x) h(x) $, where \\(g(x)\\) and \\(h(x)\\) are both differentiable functions, the derivative of a sum or difference of two functions is given by: \\[\\begin{equation} f&#39;(x) = g&#39;(x) \\pm h&#39;(x) \\end{equation}\\] The product rule Given \\(f(x) = g(x) \\cdot h(x)\\), where \\(g(x)\\) and \\(h(x)\\) are both differentiable functions, the derivative is given by: \\[\\begin{equation} f&#39;(x) = h(x) \\cdot g&#39;(x) + h&#39;(x) \\cdot g(x) \\end{equation}\\] The quotient rule Given \\(f(x) = \\frac{g(x)}{h(x)}\\), where \\(g(x)\\) and \\(h(x)\\) are both differentiable functions and \\(h(x)\\neq0\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = \\dfrac{h(x) \\cdot g&#39;(x) - h&#39;(x) \\cdot g(x)}{[h(x)]^2} \\end{equation}\\] The generalized power function rule Given \\(f(x) = [g(x)]^n\\), where \\(g(x)\\) is a differentiable functions and n is any real number, the derivative is given by: \\[\\begin{equation} f&#39;(x) = n[g(x)]^{n-1} \\cdot g&#39;(x) \\end{equation}\\] The chain rule Given \\(f(x) = h(g(x))\\), where \\(f\\) is a function of a function where \\(h\\) is in turn function of , the derivative is given by: \\[\\begin{equation} f&#39;(x) = h&#39;(g(x)) \\cdot g&#39;(x) \\end{equation}\\] Additional rules Given \\(f(x) = e^x\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = e^x \\end{equation}\\] Given \\(f(x) = ln(x)\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = \\frac{1}{x} \\end{equation}\\] Given \\(f(x) = sin(x)\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = cos(x) \\end{equation}\\] Given \\(f(x) = cos(x)\\), the derivative is given by: \\[\\begin{equation} f&#39;(x) = -sin(x) \\end{equation}\\] 4.2 Matrix operations R can perform standard matrix algebra operations. We can use matrix algebra functions in R to solve our problem from class. \\[ 7x + 5y - 3z = 16 \\\\ 3x -5y +2z = -8 \\\\ 5x +3y -7z = 0 \\] First, we rewrite the system using matrix and vector notation: \\[ \\mathbf{A} = \\left[\\begin{array} {rrr} 7 &amp; 5 &amp; -3 \\\\ 3 &amp; -5 &amp; 2 \\\\ 5 &amp; 3 &amp; -7 \\\\ \\end{array}\\right] \\mathbf{b} = \\left[\\begin{array} {rrr} x \\\\ y \\\\ z \\\\ \\end{array}\\right] \\mathbf{r} = \\left[\\begin{array} {rrr} 16 \\\\ -8 \\\\ 0 \\\\ \\end{array}\\right] \\] In order to obtain the result vector b, we have to rearrange the model performing some simple matrix algebra operations. \\[ \\mathbf{A}^{-1}\\mathbf{A}\\mathbf{b} = \\mathbf{A}^{-1}\\mathbf{r} \\\\ \\text{remember that} \\space \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I} \\\\ \\mathbf{b} = \\mathbf{A}^{-1}\\mathbf{r} \\] We are now ready to solve our system of equations using R: ## [,1] ## [1,] 1 ## [2,] 3 ## [3,] 2 "],["chapter5.html", "Chapter 5 Stats 5.1 Find mean and median 5.2 Boxplot 5.3 Expected value 5.4 Variance 5.5 Correlation 5.6 Covariance 5.7 Expected value, variance and covariance rules", " Chapter 5 Stats 5.1 Find mean and median ## [1] 5 ## [1] 5 ## [1] 83 A percentile is a point in a distribution at which or below which a given proportion of data is found. The k-th percentile divides the data in a way that k-percent of the data lie below the percentile and (100 - k)-percent lie above the percentile. It is also common to hear about quantiles. Quantiles is a more generic terms which indicates values partitioning data in equally spaced groups. Specific types of quantiles are percentiles (see above), deciles, quartiles, etc. ## 25% 50% 75% 100% ## 3.75 8.00 12.25 18.00 ## [1] 5 10 15 20 ## [1] 3 8 12 18 ## [1] 30 ## [1] 90 ## [1] 30 90 5.2 Boxplot ## 25% ## -9 ## 75% ## 25 5.3 Expected value ## [1] 4.333333 ## [1] 3.5 ## [1] 5 ## [1] 0.12 Simulation of the expected value of the fair die/dice. This exercise is taken from the book Hands-On Programming with R by Garrett Grolemund. 5.4 Variance Find variance and standard deviation ## [1] 3.466667 ## [1] 1.861899 ## [1] TRUE ## [1] 3.466667 ## [1] 1.861899 ## [1] 346.6667 ## [1] 18.61899 5.5 Correlation With this short exercise we want to see how the Pearson correlation coefficient is a measure of linear correlation between two variables. ## [1] -0.04431211 ## [1] TRUE ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = -0.4391, df = 98, p-value = 0.6616 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2386531 0.1534415 ## sample estimates: ## cor ## -0.04431211 ## [1] 0.9336273 ## ## Pearson&#39;s product-moment correlation ## ## data: a and b ## t = 25.799, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9027582 0.9549292 ## sample estimates: ## cor ## 0.9336273 Programming challenge. Program a function that takes two inputs, x and y, and returns the correlation coefficient between x and y. ## [1] 0.1042097 ## [1] 0.1042097 5.6 Covariance Find covariance and Pearsons correlation coefficient between the variables \\(X\\) and \\(Y\\). ## [1] 10 ## [1] 0.5 5.7 Expected value, variance and covariance rules Expected value The of a (discrete) random variable is the arithmetic mean of that variable where each value is weighted by its probability. It can also be thought as the long-run average for any random variable over an indefinite number of trials. For a discrete random variable X having the possible values \\(x_1, \\dots, x_N\\) the expectation of X is defined as: \\[ E(X) = x_1P(X=x_1) + \\dots + x_NP(X=x_N) = \\sum\\limits_{i=1}^{N} x_iP(X = x_i) = \\sum\\limits_{i=1}^{n} x_ip_{i}\\] The expected value of a random variable can be also understood as the population mean, \\(E(X) = \\mu_x\\) or simply \\(\\mu\\). With equals weights, the formula for the expected value will be equal to the formula for the arithmetic average. Similarly, the expected value of functions of discrete random variables is given by the following: \\[ E\\{g(X)\\} = g(x_1)p_1 + \\dots + g(x_N)p_N = \\sum\\limits_{i=1}^{N} g(x_i)p_i\\] Rule 1 The expected value of a constant, for example , is that constant \\[\\begin{align} E(b) = b \\end{align}\\] This rule can be easily understood following the rules of the summation operator. \\[\\begin{align} E(b) &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}b \\nonumber \\\\ &amp; = \\dfrac{1}{N}Nb \\nonumber \\\\ &amp; = b \\nonumber \\end{align}\\] Rule 2 If \\(X\\) is a random variable and \\(b\\) is a constant then, \\[\\begin{align} E(bX) = bE(X) \\end{align}\\] Again, using the rules of the summation operator and substituting back the population mean \\(\\mu_x\\) (remember that \\(E(X) = \\mu_x\\)): \\[\\begin{align} E(bX) &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}x_{i}b \\nonumber\\\\ &amp; = b\\dfrac{1}{N}\\sum_{i=1}^{N}x_{i} \\nonumber\\\\ &amp; = b\\mu_x \\nonumber \\\\ &amp; = bE(X) \\nonumber \\end{align}\\] Rule 3 The expected value of the sum of several variables is the sum of the expected values. If X, Y, and Z are three random variables, then, \\[\\begin{align} E(X + Y + Z) = E(X) + E(Y) + E(Z) \\end{align}\\] The rules of the sigma operator can be applied also to this case. \\[\\begin{align} E(X + Y + Z) &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}(x_{i} + y_{i} + z_{i}) \\nonumber\\\\ &amp; = \\dfrac{1}{N}\\sum_{i=1}^{N}x_{i} + \\dfrac{1}{N}\\sum_{i=1}^{N}y_{i} + \\dfrac{1}{N}\\sum_{i=1}^{N}z_{i} \\nonumber\\\\ &amp; = \\mu_x + \\mu_y + \\mu_z \\nonumber\\\\ &amp; = E(X) + E(Y) + E(Z) \\nonumber \\end{align}\\] Note also that \\(E(X^2) \\neq [E(X)]^2\\) and that for non-linear functions \\(E[g(X)] \\neq g[E(X)]\\). and can be seen with simple numerical examples. This is left as an exercise. Covariance The population between two random variables X and Y \\(cov(X,Y)\\) is defined as the expected value of the product of the deviation of the variables from their respective means. \\[\\begin{align} cov(X,Y) &amp; = E\\{(X-\\mu_x)(Y-\\mu_y)\\} \\\\ &amp; = E[XY - \\mu_xY - \\mu_yX + \\mu_y\\mu_x]\\nonumber\\\\ &amp; = E(XY) - \\mu_xE(Y) - \\mu_y(X) + E(\\mu_x\\mu_y)\\nonumber\\\\ &amp; = E(XY) - \\mu_x\\mu_y - \\mu_y\\mu_x + \\mu_x\\mu_y\\nonumber\\\\ &amp; = E(XY) - \\mu_x\\mu_y\\nonumber\\\\ &amp; = E(XY) - E(X)E(Y)\\nonumber \\end{align}\\] On the contrary, two random variables \\(X\\) and \\(Y\\) are said to be independent if \\(cov(X,Y) = 0\\). Only in this case, \\[\\begin{align} E(XY) = E(X)E(Y) \\end{align}\\] Rule 1 If \\(Y = V + W\\), then, \\[\\begin{align} cov(X, Y) = cov(X, V) + cov(X, W) \\end{align}\\] PROOF for Rule 1: \\[\\begin{align} cov(X,Y) &amp; = E\\{(X - \\mu_x)(Y - \\mu_y)\\} \\nonumber \\\\ &amp; = E\\{(X - \\mu_x)([V + W] - [\\mu_v + \\mu_w])\\} \\nonumber \\\\ &amp; = E\\{(X - \\mu_x)(V - \\mu_v) + (X - \\mu_x)(W - \\mu_w)\\}\\nonumber \\\\ &amp; = cov(X, V) + cov(X, W)\\nonumber \\end{align}\\] Rule 2 If \\(Y = bZ\\), where is a constant and a random variable, then, \\[\\begin{align} cov(X, Y) = b\\cdot cov(X, Z) \\end{align}\\] PROOF for Rule 2 \\[\\begin{align} cov(X,Y) &amp; = E\\{(X - \\mu_x)(Y - \\mu_y)\\}\\nonumber \\\\ &amp; = E\\{(X - \\mu_x) (bZ - b\\mu_z)\\}\\nonumber \\\\ &amp; = bE\\{(X - \\mu_x) (Z - \\mu_z)\\}\\nonumber \\\\ &amp; = bcov(X,Z)\\nonumber \\end{align}\\] Rule 3 If \\(Y = b\\), where is a constant, then, \\[\\begin{align} cov(X, Y) = 0 \\end{align}\\] PROOF for Rule 3: \\[\\begin{align} cov(X,Y) &amp; = E\\{(X - \\mu_x)(Y - \\mu_y)\\}\\nonumber \\\\ &amp; = E\\{(X - \\mu_x) (b - b)\\}\\nonumber \\\\ &amp; = E\\{0\\}\\nonumber \\end{align}\\] Variance The population of a random variable \\(X\\) can be understood as a measure of the dispersion of its probability distribution. It is defined as the expected (or average) squared deviation of its values from the mean. \\[\\begin{align} var(X) &amp; = E\\{(X - \\mu_x)^2\\}\\nonumber \\\\ &amp; = E(X^2 - 2\\mu_xX + \\mu_{X}^{2})\\nonumber \\\\ &amp; = E(X^2) + E(-2\\mu_xX) + E(\\mu_{X}^{2})\\nonumber \\\\ &amp; = E(X^2) - 2\\mu_xE(X) + \\mu_{X}^{2}\\nonumber \\\\ &amp; = E(X^2) - 2\\mu_x\\mu_x + \\mu_{X}^{2}\\nonumber \\\\ &amp; = E(X^2) - \\mu_x^{2}\\nonumber \\end{align}\\] We can also think of a random variable \\(X\\) composed of two entities, the population mean \\(\\mu_x\\) and a disturbance term or random component \\(u\\). \\[\\begin{align} X = \\mu_x + u \\hspace{1cm}\\text{from which it follows that}\\hspace{1cm} u = X - \\mu_x \\end{align}\\] We can show that the expected value of \\(u\\) is zero and that the variance of X is the same as the variance of \\(u\\). In other words, the variance of X solely depends on the variance of \\(u\\) and not of its mean (!). \\[\\begin{align} E(u) = E(X - \\mu_x) = E(X) + E(- \\mu_x) = \\mu_x - \\mu_x = 0 \\end{align}\\] We know that, \\[\\begin{align} var(X) = E\\{(X - \\mu_x)^2\\} = E(u^2) \\end{align}\\] and, \\[\\begin{align} var(u) = E\\{(u - \\text{mean of}\\hspace{0.1cm} u)^2 \\} = E\\{(u - 0)^2 \\} = E(u^2) \\end{align}\\] therefore, It is also useful and pretty straight forward to note that the variance of a random variable \\(X\\) can be thought of the covariance of \\(X\\) with itself: \\[\\begin{align} var(X) &amp; = E\\{(X - \\mu_x)^2\\}\\nonumber \\\\ &amp; = E\\{(X - \\mu_x)(X - \\mu_x)\\}\\nonumber \\\\ &amp; = cov(X, X)\\nonumber \\end{align}\\] Rule 1 If \\(Y = V + W\\), then, \\[\\begin{align} var(Y) = var(V) + var(W) + 2cov(V, W) \\end{align}\\] PROOF for Rule 1: \\[\\begin{align} var(Y) &amp; = cov(Y,Y) \\nonumber \\\\ &amp; = cov(Y,[V + W]) \\nonumber \\\\ &amp; = cov(Y,V) + cov(Y,W) \\nonumber \\\\ &amp; = cov(V,[V + W]) + cov([V + W],W) \\nonumber \\\\ &amp; = cov(V,V) + cov(W,V) + cov(V,W) + cov(W,W) \\nonumber \\\\ &amp; = var(V) + var(W) + 2cov(V, W) \\nonumber \\\\ \\end{align}\\] Rule 2 If \\(Y = bZ\\), where \\(b\\) is a constant, then, \\[\\begin{align} var(Y) = b^2var(Z) \\end{align}\\] PROOF for Rule 2: \\[\\begin{align} var(Y) &amp; = cov(Y,Y)\\nonumber \\\\ &amp; = cov(bZ,Y)\\nonumber \\\\ &amp; = bcov(Z,Y)\\nonumber \\\\ &amp; = bcov(Z,bZ)\\nonumber \\\\ &amp; = b^2cov(Z,Z)\\nonumber \\\\ &amp; = b^2var(Z)\\nonumber \\end{align}\\] Rule 3 If \\(Y = b\\), where \\(b\\) is a constant, then, \\[\\begin{align} var(Y) = 0 \\end{align}\\] PROOF for Rule 3: \\[\\begin{align} var(Y) &amp; = cov(b,b)\\nonumber \\\\ &amp; = 0 \\nonumber \\end{align}\\] Rule 4 If \\(Y = V + b\\), where \\(b\\) is a constant, then, \\[\\begin{align} var(Y) = var(V) \\end{align}\\] PROOF for Rule 4: \\[\\begin{align} var(Y) &amp; = var(V + b) \\nonumber \\\\ &amp; = var(V) + var(b) + 2cov(V,b)\\nonumber \\\\ &amp; = var(V) \\nonumber \\\\ \\end{align}\\] "],["chapter6.html", "Chapter 6 Inference", " Chapter 6 Inference The basics of hypothesis testing1 In descriptive statistics, we work with a sample of data obtained from a larger population and we are interested in understanding the characteristics of the sample. In inferential statistics, we use the sample to try to obtain conclusions (more precisely said, to make inference) regarding the characteristics of the population from which the sample is obtained. For example, having a sample available we might be interested in inferring whether the sample mean (sample statistics, \\(\\bar{x}\\)) is representative of the population mean (population parameter, \\(\\mu\\)). That is, we are interested in generalising the information obtained from the sample to the entire population. Of course it is easy to imagine that our conclusion will be subject to error as we only have one sample available. With only one sample available, it is very unlikely that the sample mean will be identical to the population mean. So in our inference process we have to take this sampling error into account. Hypothesis testing is a widely used statistical procedure for making inference from a sample to a population.2 The test starts with two statements. The null hypothesis, denoted \\(H_0\\). The null hypothesis describes the condition that is assumed to be true at the time. It is often compared to the situation in court where an accused person is assumed to be innocent until there is enough evidence to find him guilty. The second statement is the alternative hypothesis, denote \\(H_1\\). The null hypothesis and the alternative hypothesis are mutually exclusive. The alternative hypothesis is the one that is favoured if enough evidence is found. Many times we have a theory that suggests which values to specify for the null hypothesis and the alternative hypothesis. Hypothesis testing is a means of testing the statistical (not practical) validity of our theory. \\[H_0: \\mu = \\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\] How do we decide between the null hypothesis and the alternative hypothesis? We assume that we have a normally distributed random variable \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Let us further assume that the population mean \\(\\mu\\) (which we do not know) is a value equal to \\(\\mu_0\\) (maybe because theory or experience tells us so). In a first sample obtained from our random variable \\(X\\), we compute the mean (\\(\\bar{x}\\)) and obtain a value close to the hypothesized value \\(\\mu_0\\). Since we know that data are obtained from a normally distributed \\(X\\), we know that the probability of obtaining a value close to the mean is actually quite high. In a second sample, a much larger value of \\(\\bar{x}\\) is obtained. In this case, we know that obtaining values much larger than the mean is less probable (in other words, we are in the tales of the distribution). What should we conclude with such a high value now? Does the sample results confirm or contradict the null hypothesis, i.e. the hypothesized value for the mean of the population? We can continue to believe that the null hypothesis is correct and that we have just an unlucky sample from which we have obtained a very high mean but we are somehow convinced that the hypothesized value is still correct (\\(H_0: \\mu = \\mu_0\\)) and the sample was just a bit off. Or we can convince ourselves that the sample result contradicts the assumed value \\(\\mu_0\\) and that we were wrong with the null hypothesis (therefore, \\(H_1: \\mu \\neq \\mu_0\\)). In the rule, the null hypothesis is rejected if the probability of obtaining such an extreme value is smaller than an arbitrarily defined probability. There are two kinds of mistakes that can be made. A Type I error and a Type II error. When we reject a hypothesis that is actually true, we are committing a Type I error. When we are not rejecting (or accepting, but this is not the right terminology) a hypothesis that should be rejected, we are committing a Type II error. Since we are basing our decision on a sample, and there is always uncertainty in the sampling process, we are more or less sure that there is some uncertainty attached to our test conclusion. In hypothesis testing we decide in advance what kind of error we want to make. The procedure is to decide in advance to commit to a certain Type I error, e.g. we decide in advance to tolerate a Type I error in 5% of the cases. This value is called significance level and is traditionally indicated with the greek letter \\(\\alpha\\).3 The discussion presented here is based on Wooldridge, J. Introductory Econometrics: A Modern Approach (Chapter 4 and Appendix C) and Dougherty, C. Introduction to Econometrics (Review chapter). This procedure is part of the branch of statistics called parametric statistics. In fact, it is assumed that the population from which the sample is obtained follows some kind of distribution (\\(t\\) distribution, normal distribution, \\(F\\) distribution) to which reference will be made to compare the plausibility of the sample statistics. Another branch of statistics is called non-parametric and does not assume any kind of distribution. Again, \\(\\alpha\\) is the probability of rejecting a true null hypothesis. Traditionally in econometrics, we use three levels of significance 10%, 5% and 1%. "],["example.html", "Chapter 7 Example 7.1 The normal distribution6", " Chapter 7 Example The mean lifetime of a sample of 100 light bulbs is computed to be 1570 hours with a standard deviation of 120 hours. If \\(\\mu\\) is the mean lifetime of all the light bulbs produced, test the hypothesis that the population mean is \\(\\mu = 1600\\) hours against the alternative that \\(\\mu \\neq 1600\\) using a significance level of 5%. Find the p-value of the test and build a 95% confidence interval. \\[H_0: \\mu = 1600\\] \\[H_1: \\mu \\neq \\space 1600\\] We need to construct our test statistic to perform the test. In practice, wee need to transform the computed mean obtained from the sample of light bulbs into a statistic that follow a standard normal distribution with zero mean and unit variance. \\[z_{test} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{1570 - 1600}{12} = -2.5\\] ## [1] 2.5 Since in this exercise we are interested in performing a two-sided test (look a the formulation of the null and alternative hypothesis), we take the absolute value of our \\(z\\) test. We can reject \\(H_0\\) at the 5% significance level (\\(\\alpha\\) = 5%) if, \\[|z_{test}| &gt; z_{crit}\\] ## [1] TRUE We can reject \\(H_0\\) at the 5% significance level. In the following graph, in green we can see the rejection region (the two green shaded areas add up to 5%) while with the yellow line it is indicated the value of our \\(z\\) test in both tails of the distribution. As we said, since the value of our \\(z\\) test falls within the rejection region, we can reject \\(H_0\\) at the 5% significance level. What do we see highlighted in black in the graph above? The sum of the two black areas represent our p-value for the z score that we have just calculated. We can think of the p-value as the smallest significance level at which we still reject the null hypothesis (or the largest significance level at which the null hypothesis cannot be rejected). How large is the p-value for our z test? ## [1] 1.241933 How can we interpret the p-value? In our exercise, the p-value of 1.24% represents the probability that a mean lifetime of less than 1570 or more than 1630 hours would occur by chance if \\(H_0\\) were actually true. A rather small probability. How did we obtain the values 1570 and 1630? We have used (half) of the p-value to calculate the quantiles of a normal distribution with mean \\(mu = 1600\\) and standard deviation \\(120/\\sqrt{100}\\). ## [1] 1630 ## [1] 1570 ## [1] 1.241933 Since we see that the p-value is larger than 1%, we already know that we will fail to reject the null hypothesis at the 1% significance level (\\(\\alpha\\) = 1%) . Shouldnt we have used a t test rather than a \\(z\\) test as suggested by professional statisticians? Probably yes. Since we do not know the population standard deviation (the standard deviation of all light bulbs), a t test sounds more appropriate. However, since our sample is relatively large (\\(n = 100\\)), much larger than the commonly suggested rule of thumb (\\(n &gt; 30\\)), we will practically obtain (almost) the same result using the normal distribution and the t distribution.4 To conclude, we need to construct the 95% confidence interval (\\(100 - \\alpha\\), where \\(\\alpha\\) = 5%). \\[(\\bar{x} + z_{\\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] We must be careful with the signs in the formula above. Once we look up the critical value \\(z_{\\alpha / 2}\\) (or we compute it using R) we will see that the quantity is actually negative. If we include the negative sign in the formula for the confidence interval, it would not be wrong, but it can create some confusion. This is why it is better to use the formula below. In this case, we have to include the negative sign in the formula because \\(z_{1 - \\alpha / 2}\\) is going to be a positive number and this is the version that we are going to implement in R. \\[(\\bar{x} - z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] ## [1] 1546.48 1593.52 The hypothesized value for the mean lifetime of all the bulbs produced was 1600 working hours. Since this particular value falls outside the confidence interval constructed around the sample mean, we can reject \\(H_0\\). With the confidence interval test the null hypothesis is rejected if and only if the hypothesized value falls outside the confidence interval. The \\(z\\) test (or \\(t\\) test) and the confidence interval test are basically an elaboration of one another and provides always the same test decision.5 7.1 The normal distribution6 The normal distribution is a common probability distribution in statistics and econometrics (it is just one of many distributions). The normal distribution fits a number of natural and social phenomena. When a phenomenon (a random variable) has a normal distribution, its probability density function (for short, PDF) assumes the well-known bell-shaped curve. The normal distribution is sometimes called the Gaussian distribution or the Gauss curve in honor of the famous mathematician Carl-Friedrich Gauss.7 Shape and position of the normal distribution are entirely determined by mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) of the normally distributed random variable. This is written as: \\[X \\sim Normal(\\mu, \\sigma)\\] For example, we see that the two normal distributions shown in the following graph have the same mean but different standard deviations. The mean determines the location of the normal distribution in the horizontal axis. The majority of the body is located around the mean to which correspond the peak in the distribution. The standard deviation determines the shape of the curve. In practice, it determines how far the values of the variable are from the mean. This means that a higher mean shifts the curve to the right without changing its shape. Similarly, a higher standard deviation widens the body of the curve without shifting its position on the horizontal axis. The normal distribution has a number of interesting and useful properties. First of all, it is symmetrical with respect to the mean, from which it follows that half of the values are distributed half to the right and half to the left of the mean. Knowing the mean and standard deviation of a certain event or random variable, the normal distribution allows us to calculate the probability that the event will assume a certain value or range of values. Roughly speaking, this correspond to the area below the curve. In reality, this is done using the cumulative distribution function (CDF) which is nothing more than the integral of the PDF. The following figure shows the relationship between PDF (left) and CDF (right) of a normally distributed random variable with mean 0 and standard deviation 1. If we want to see why this is the case, take a look here. For common misunderstandings about the confidence interval, see here. The discussion presented here and in particular the proof of mean and standard deviation of the the standardized random variable rely on Wooldridge, J. Introductory Econometrics: A Modern Approach (Appendix C). At first glance, many phenomena do not appear to follow a normal distribution. However, after a logarithmic transformation they assume a (log)normal distribution. "],["the-standard-normal-distribution.html", "Chapter 8 The standard normal distribution", " Chapter 8 The standard normal distribution A special case of normal distribution is the standard normal distribution where the mean is equal to 0 and the standard deviation is equal to 1 (this is actually what we used in the previous exercise but we had not yet used this term). Let us now see how it is possible to standardise a variable. This is a very important procedure that we will see again later when we talk about hypothesis testing. In the following section we will instead use it to calculate the so-called \\(z\\) scores. \\[Z = \\frac{X - \\mu}{\\sigma}\\] Rewriting \\(Z\\) as \\(aX + b\\), where \\(a = (1/\\sigma)\\) and \\(b = -(\\mu/\\sigma)\\) and using the properties of expectation and variance we can see that: \\[E(Z) = aE(X) + b = (\\mu/\\sigma) - (\\mu/\\sigma) = 0\\] \\[Var(Z) = a^2Var(X) = (\\sigma^2/\\sigma^2) = 1\\] What does that mean? It means that if we subtract the mean from a variable (\\(X\\)) and divide it by the standard deviation we will have a standardised variable (\\(Z\\)) that has a mean of zero and standard deviation of 1. "],["exercises.html", "Chapter 9 Exercises", " Chapter 9 Exercises We are given the following set of numbers: \\(6, 2 , 8, 7, 5\\). Transform the set into standard scores and check that mean and standard deviation of the transformed set are respectively 0 and 1. ## [1] 1.387779e-16 ## [1] 1 Let us assume that the random variable \\(X\\) is a normally distributed random variable with mean (\\(\\mu\\)) equal to 5 and population standard deviation (\\(\\sigma\\)) equal to 4. In short, \\(Normal ~ (5,4)\\). Calculate the probabilities that our random variable \\(X\\) assume a value smaller than 6, \\(P(X \\leq 6)\\), using the table of the standard normal probabilities or R (much better!). If we did not have R available we would have to find the \\(z\\) score corresponding to the value of interest, 6 in this case, and look in the table of standard normal probabilities (the area below the curve) the probability that our random variable assumes a value smaller than that.8 ## [1] 0.25 \\[z = \\frac{6 - 5}{4} = 0.25\\] Our \\(z\\) value of interest is 0.25. The probability that the variable \\(X\\) takes on a value less than 6 is given by the area under the normal curve to the left of \\(z = 0.25\\). This value is equal to: ## [1] 59.87063 We can achieve the same result by using the lower.tail = FALSE option. In this case we get the white area in the graph below and will have to subtract this quantity from 1 or 100%, i.e. the whole area under the curve. ## [1] 59.87063 In the graph below, the area marked in green indicates the probability that the independent variable \\(X\\) takes on a value less than 6 given mean and population standard deviation of 5 and 4, respectively. If we have software at our disposal we do not have to use tables. In this case there is no need to calculate the \\(z\\) score. The result (and the graph) will be exactly the same with the important difference that now the values reported in the horizontal axis will be the values of \\(X\\) and not the standardized scores. ## [1] 59.87063 The test scores for a class of students (this is the population) are normally distributed with mean (\\(\\mu\\)) equal to 75 points and standard deviation (\\(\\sigma\\)) equal to 10 points. What is the probability that a students scores above 80 points? Calculate the following probabilities: Given \\(X \\sim Normal(3,4)\\), find \\(P(X \\leq 1)\\) Given \\(X \\sim Normal(4,0)\\), find \\(P(2 &lt; X \\leq 6)\\) Since the normal distribution is continuous, \\(P(Z &lt; z) = P(Z \\leq z)\\). "],["chapter7.html", "Chapter 10 Econometrics 1", " Chapter 10 Econometrics 1 The linear regression model - Part 19 We are given the following set of values: We want to find the line that minimizes the sum of squared residuals, i.e. the squared distance between the observed value and the line. In a simple bivariate case as this one, we have to find a slope (\\(\\hat{\\beta_1}\\)) and an intercept (\\(\\hat{\\beta_0}\\)) for the line so that the sum of squared residuals is as small as possible. \\[\\begin{align} y_i &amp; = \\hat{y_i} + \\hat{u_i} \\\\ \\hat{y_i} &amp; = \\hat{\\beta_0} + \\hat{\\beta_1}x_i \\\\ y_i &amp; = \\hat{\\beta_0} + \\hat{\\beta_1}x_i + \\hat{u_i} \\\\ \\hat{u_i} &amp; = y_i - \\hat{y_i} \\\\ \\hat{u_i} &amp; = y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}x_i) \\end{align}\\] We have now a function that we want to minimize with respect to \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). \\[\\begin{align} f(\\hat{\\beta_0}, \\hat{\\beta_1}) = \\sum_{n=1}^{3} \\hat{u_i}^2 = \\sum_{n=1}^{3} (y_i - \\hat{y_i})^2 = \\sum_{n=1}^{3} (y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}x_i))^2 = \\sum_{n=1}^{3} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)^2 \\end{align}\\] We now take the derivative of (6) with respect to \\(\\hat{\\beta_0}\\) and set it equal to 0. We then do the same thing for \\(\\hat{\\beta_1}\\). These are the so-called . \\[\\begin{align} \\dfrac{\\partial f(\\hat{\\beta_0}, \\hat{\\beta_1})}{\\partial \\hat{\\beta_0}} &amp; = -2\\sum_{n=1}^{3} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0 \\\\ \\dfrac{\\partial f(\\hat{\\beta_0}, \\hat{\\beta_1})}{\\partial \\hat{\\beta_1}} &amp; = -2\\sum_{n=1}^{3} x_i (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0 \\end{align}\\] ## [1] 5.5 4.0 3.5 ## [1] -0.5 2.0 -1.5 ## [1] 6.5 ## [1] 2.166667 ## [1] 8.666667 ## [1] 8.666667 ## [1] 0.25 ## [1] 0.25 ## [1] 2.54951 ## ## =============================================== ## Dependent variable: ## --------------------------- ## y ## ----------------------------------------------- ## x -0.250 ## (0.433) ## ## Constant 4.750 ## (1.639) ## ## ----------------------------------------------- ## Observations 3 ## R2 0.250 ## Residual Std. Error 2.550 (df = 1) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Coefficients: ## (Intercept) x I(x^2) ## 7.9375 0.1667 -0.2708 Which model has the largest \\(R^2\\)? ## ## =================================================== ## Dependent variable: ## ------------------------------- ## vary ## (1) (2) ## --------------------------------------------------- ## varx 0.162 0.162* ## (0.360) (0.080) ## ## I(varx2) -0.284*** ## (0.015) ## ## Constant -2.140 8.281*** ## (2.182) (0.724) ## ## --------------------------------------------------- ## Observations 21 21 ## R2 0.010 0.954 ## Residual Std. Error 9.999 (df = 19) 2.208 (df = 18) ## =================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Text and the derivation above rely on the suggested literature: Wooldridge, J. Introductory Econometrics: A Modern Approach (Chapter 2) and Dougherty, C. Introduction to Econometrics (Chapter 1). "],["chapter8.html", "Chapter 11 Econometrics 2 11.1 The basics of hypothesis testing10", " Chapter 11 Econometrics 2 11.1 The basics of hypothesis testing10 In descriptive statistics, we work with a sample of data obtained from a larger population and we are interested in understanding the characteristics of the sample. In inferential statistics, we use the sample to try to obtain conclusions (more precisely said, to make inference) regarding the characteristics of the population from which the sample is obtained. For example, having a sample available we might be interested in inferring whether the sample mean (sample statistics, \\(\\bar{x}\\)) is representative of the population mean (population parameter, \\(\\mu\\)). That is, we are interested in generalising the information obtained from the sample to the entire population. Of course it is easy to imagine that our conclusion will be subject to error as we only have one sample available. With only one sample available, it is very unlikely that the sample mean will be identical to the population mean. So in our inference process we have to take this sampling error into account. Hypothesis testing is a widely used statistical procedure for making inference from a sample to a population.11 The test starts with two statements. The null hypothesis, denoted \\(H_0\\). The null hypothesis describes the condition that is assumed to be true at the time. It is often compared to the situation in court where an accused person is assumed to be innocent until there is enough evidence to find him guilty. The second statement is the alternative hypothesis, denote \\(H_1\\). The null hypothesis and the alternative hypothesis are mutually exclusive. The alternative hypothesis is the one that is favoured if enough evidence is found. Many times we have a theory that suggests which values to specify for the null hypothesis and the alternative hypothesis. Hypothesis testing is a means of testing the statistical (not practical) validity of our theory. \\[H_0: \\mu = \\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\] How do we decide between the null hypothesis and the alternative hypothesis? We assume that we have a normally distributed random variable \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Let us further assume that the population mean \\(\\mu\\) (which we do not know) is a value equal to \\(\\mu_0\\) (maybe because theory or experience tells us so). In a first sample obtained from our random variable \\(X\\), we compute the mean (\\(\\bar{x}\\)) and obtain a value close to the hypothesized value \\(\\mu_0\\). Since we know that data are obtained from a normally distributed \\(X\\), we know that the probability of obtaining a value close to the mean is actually quite high. In a second sample, a much larger value of \\(\\bar{x}\\) is obtained. In this case, we know that obtaining values much larger than the mean is less probable (in other words, we are in the tales of the distribution). What should we conclude with such a high value now? Does the sample results confirm or contradict the null hypothesis, i.e. the hypothesized value for the mean of the population? We can continue to believe that the null hypothesis is correct and that we have just an unlucky sample from which we have obtained a very high mean but we are somehow convinced that the hypothesized value is still correct (\\(H_0: \\mu = \\mu_0\\)) and the sample was just a bit off. Or we can convince ourselves that the sample result contradicts the assumed value \\(\\mu_0\\) and that we were wrong with the null hypothesis (therefore, \\(H_1: \\mu \\neq \\mu_0\\)). In the rule, the null hypothesis is rejected if the probability of obtaining such an extreme value is smaller than an arbitrarily defined probability. There are two kinds of mistakes that can be made. A Type I error and a Type II error. When we reject a hypothesis that is actually true, we are committing a Type I error. When we are not rejecting (or accepting, but this is not the right terminology) a hypothesis that should be rejected, we are committing a Type II error. Since we are basing our decision on a sample, and there is always uncertainty in the sampling process, we are more or less sure that there is some uncertainty attached to our test conclusion. In hypothesis testing we decide in advance what kind of error we want to make. The procedure is to decide in advance to commit to a certain Type I error, e.g. we decide in advance to tolerate a Type I error in 5% of the cases. This value is called significance level and is traditionally indicated with the greek letter \\(\\alpha\\).12 The discussion presented here is based on Wooldridge, J. Introductory Econometrics: A Modern Approach (Chapter 4 and Appendix C) and Dougherty, C. Introduction to Econometrics (Review chapter). This procedure is part of the branch of statistics called parametric statistics. In fact, it is assumed that the population from which the sample is obtained follows some kind of distribution (\\(t\\) distribution, normal distribution, \\(F\\) distribution) to which reference will be made to compare the plausibility of the sample statistics. Another branch of statistics is called non-parametric and does not assume any kind of distribution. Again, \\(\\alpha\\) is the probability of rejecting a true null hypothesis. Traditionally in econometrics, we use three levels of significance 10%, 5% and 1%. "],["example-1.html", "Chapter 12 Example", " Chapter 12 Example The mean lifetime of a sample of 100 light bulbs is computed to be 1570 hours with a standard deviation of 120 hours. If \\(\\mu\\) is the mean lifetime of all the light bulbs produced, test the hypothesis that the population mean is \\(\\mu = 1600\\) hours against the alternative that \\(\\mu \\neq 1600\\) using a significance level of 5%. Find the p-value of the test and build a 95% confidence interval. \\[H_0: \\mu = 1600\\] \\[H_1: \\mu \\neq \\space 1600\\] We need to construct our test statistic to perform the test. In practice, wee need to transform the computed mean obtained from the sample of light bulbs into a statistic that follow a standard normal distribution with zero mean and unit variance. \\[z_{test} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{1570 - 1600}{12} = -2.5\\] ## [1] 2.5 Since in this exercise we are interested in performing a two-sided test (look a the formulation of the null and alternative hypothesis), we take the absolute value of our \\(z\\) test. We can reject \\(H_0\\) at the 5% significance level (\\(\\alpha\\) = 5%) if, \\[|z_{test}| &gt; z_{crit}\\] ## [1] TRUE We can reject \\(H_0\\) at the 5% significance level. In the following graph, in green we can see the rejection region (the two green shaded areas add up to 5%) while with the yellow line it is indicated the value of our \\(z\\) test in both tails of the distribution. As we said, since the value of our \\(z\\) test falls within the rejection region, we can reject \\(H_0\\) at the 5% significance level. What do we see highlighted in black in the graph above? The sum of the two black areas represent our p-value for the z score that we have just calculated. We can think of the p-value as the smallest significance level at which we still reject the null hypothesis (or the largest significance level at which the null hypothesis cannot be rejected). How large is the p-value for our z test? ## [1] 1.241933 How can we interpret the p-value? In our exercise, the p-value of 1.24% represents the probability that a mean lifetime of less than 1570 or more than 1630 hours would occur by chance if \\(H_0\\) were actually true. A rather small probability. How did we obtain the values 1570 and 1630? We have used (half) of the p-value to calculate the quantiles of a normal distribution with mean \\(mu = 1600\\) and standard deviation \\(120/\\sqrt{100}\\). ## [1] 1630 ## [1] 1570 ## [1] 1.241933 Since we see that the p-value is larger than 1%, we already know that we will fail to reject the null hypothesis at the 1% significance level (\\(\\alpha\\) = 1%) . Shouldnt we have used a t test rather than a \\(z\\) test as suggested by professional statisticians? Probably yes. Since we do not know the population standard deviation (the standard deviation of all light bulbs), a t test sounds more appropriate. However, since our sample is relatively large (\\(n = 100\\)), much larger than the commonly suggested rule of thumb (\\(n &gt; 30\\)), we will practically obtain (almost) the same result using the normal distribution and the t distribution.13 To conclude, we need to construct the 95% confidence interval (\\(100 - \\alpha\\), where \\(\\alpha\\) = 5%). \\[(\\bar{x} + z_{\\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] We must be careful with the signs in the formula above. Once we look up the critical value \\(z_{\\alpha / 2}\\) (or we compute it using R) we will see that the quantity is actually negative. If we include the negative sign in the formula for the confidence interval, it would not be wrong, but it can create some confusion. This is why it is better to use the formula below. In this case, we have to include the negative sign in the formula because \\(z_{1 - \\alpha / 2}\\) is going to be a positive number and this is the version that we are going to implement in R. \\[(\\bar{x} - z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}}; \\bar{x} + z_{1 - \\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\] ## [1] 1546.48 1593.52 The hypothesized value for the mean lifetime of all the bulbs produced was 1600 working hours. Since this particular value falls outside the confidence interval constructed around the sample mean, we can reject \\(H_0\\). With the confidence interval test the null hypothesis is rejected if and only if the hypothesized value falls outside the confidence interval. The \\(z\\) test (or \\(t\\) test) and the confidence interval test are basically an elaboration of one another and provides always the same test decision.14 If we want to see why this is the case, take a look here. For common misunderstandings about the confidence interval, see here. "],["the-t-test-in-the-regression.html", "Chapter 13 The t test in the regression 13.1 F statistic for overall significance of a regression 13.2 The F test", " Chapter 13 The t test in the regression We are now going to perform the test of the hypothesis on the population parameters of a simple bivariate linear regression model. The good news is that this procedure is done automatically by all statistical packages that run regressions. In our case, we are going to replicate the results provided with the output of the \\(lm\\) function in R. First, we perform a two-sided t test for the intercept parameter (\\(\\beta_0\\)). We want to test the null hypothesis (\\(H_0\\)) that \\(\\beta_0 = 0\\) against the alternative hypothesis (\\(H_1\\)) that \\(\\beta_0 \\neq 0\\). This is the kind of hypothesis test that is usually performed by linear regression function in software packages. Since we are interested in a two-sided test, we will take the absolute value of our \\(t\\) statistic. We want to perform the test at the 5% significance level. We need to find the critical value for a 5% significance level with 524 degrees of freedom. We reject \\(H_0\\) if \\(|t_{\\hat{\\beta}_0}| &gt; t_{crit}\\) otherwise we will fail to reject \\(H_0\\). ## [1] FALSE We fail to reject \\(H_0\\) at the 5% significance level. Consequently, we will fail to reject the \\(H_0\\) for every significance level smaller than 5%. Which one is the smallest significance level at which we still reject \\(H_0\\)? We need to calculate the (in)famous p-value. In this case, it is given by \\(P(|T| &gt; |t_{\\hat{\\beta}_0}|)\\).15 We can use the function pt to calculate the area below the t distribution at the right of our \\(t_{\\hat{\\beta}_0}\\). Dont forget to multiply this quantity by two as we need the p-value for a two-sided test. ## [1] 18.70735 We have a relatively large sample. We know that the t distribution converges to the standard normal distribution as the number of observation in the sample approaches infinity (meaning that the quantiles of the t distribution and the standard normal distribution are the same). Therefore, we can obtain (almost) the same result using the function pnorm. ## [1] 18.64969 We can check that we will actually reject \\(H_0\\) for a significance level larger than our p-value. Lets try to see whats happening when the significance level is 20%.16 As before, we reject \\(H_0\\) if \\(|t_{\\hat{\\beta}_0}| &gt; t_{crit}\\) otherwise we will fail to reject \\(H_0\\). ## [1] TRUE Given the result of the test, we reject \\(H_0\\) at the 20% significance level. We can also build a 95% confidence interval for the intercept:17 ## [1] -2.250472 ## [1] 0.4407687 Since the value of 0 is contained in the confidence interval, we fail to reject \\(H_0\\) at 5% significance level. The \\(H_0\\) is rejected if and only if the value 0 is not contained in the 95% confidence interval. There is no need to repeat all these steps above every time. The confint function allows us to quickly calculate the confidence interval. ## 2.5 % 97.5 % ## (Intercept) -2.250472 0.4407687 Now, we perform a two-sided t test for the slope parameter. We want to test the null hypothesis (\\(H_0\\)) that \\(\\beta_1 = 0\\) against the alternative hypothesis (\\(H_1\\)) \\(\\beta_1 \\neq 0\\). Since we are interested in a two-sided test, we will take the absolute value of our t statistic. As we know, we reject \\(H_0\\) if \\(|t_{\\hat{\\beta}_1}| &gt; t_{crit}\\), otherwise, we fail to reject \\(H_0\\). ## [1] TRUE What is the conclusion of the test? We can reject \\(H_0\\) that \\(\\beta_1 = 0\\) at the 5% significance level. Can we reject \\(H_0\\) also at the 1% significance level? We must first calculate the critical value for a 1% significance level with 524 degrees of freedom. ## [1] TRUE We can reject \\(H_0\\) that \\(\\beta_1 = 0\\) also at the 1% significance level. We can also build a 95% confidence interval for the slope parameter. ## [1] 0.4367534 ## [1] 0.6459651 Since the value of 0 is not contained in the confidence interval, we reject \\(H_0\\) at 5% significance level. Also here we can check our result using the confint function. ## 2.5 % 97.5 % ## educ 0.4367534 0.6459651 13.1 F statistic for overall significance of a regression 13.2 The F test The F test is used to test whether a group of variables has no effect on the dependent variable. In this sense, the test allows to test if the parameters of a set (or at the limit all) the independent variables are jointly significance. Obviously it is the theory or intuition that tells us to operate such a test on a given group of variables. It is often the case that the F test is performed on all independent variables in a model. It is then said that the test is for the overall significance of the regression. In this exercise, to understand how the F test works in practice, we will replicate the F test provided by the regression function in R (as by any other statistical software packages). This is precisely a test for overall joint significance of the regression. We estimate the following model: \\[wage = \\beta_0 + \\beta_1educ + \\beta_2exper + \\beta_3tenure + u\\] We formulate the following joint null hypothesis (\\(H_0\\)) stating that the regressors have jointly no effect on the dependent variable: \\[H_0 : \\beta_1 = \\beta_2 = \\beta_3 = 0\\] The alternative hypothesis (\\(H_1\\)) is: \\[H_1 : H_0 \\space \\text{is not true}\\] The formula for the F statistic (or F ratio), where \\(q\\) is the number of restrictions (in this example we are imposing three restrictions), and \\(n-k-1\\) is the number of degrees of freedom of the unrestricted model, is defined by:18 \\[F = \\frac{SSR_r - SSR_{ur}}{SSR_{ur}} * \\frac{(n-k-1)}{q}\\] First, we estimate the unrestricted model. With this term, we mean the entire or complete model: We can now calculate the sum of squared residual (SSR) of the unrestricted model: We then estimate the restricted model. The restricted model has clearly less parameters than the unrestricted model. Since we are performing an F test for the overall significance of the regression, we must regress the dependent variable wage on just an intercept. In R, this is done by including only a 1 after the tilde sign in the lm function. We can now calculate the SSR of the restricted model. We report the results in a single table created using the stargazer package. Finally, we can calculate the F statistic and its corresponding p-value. We compare the value of our F statistic (and its p-value) with the value provided by R (see the last row of the first column in the table above). ## [1] 76.87317 ## [1] 3.405862e-41 We choose a significance level (\\(\\alpha\\)) of 1% and calculate the corresponding critical value in the F distribution. ## [1] 3.819327 What is the conclusion of the test? We can observe that our F value is clearly larger the critical value for the chosen significance level of 1%. Our p-value is also very very small, certainly smaller than the significance level of 1%. We can therefore soundly reject the null hypothesis that the variables are not jointly significant. We can also create the graph of the F distribution. In green we mark the rejection region for the significance level that we have choosen. See Wooldridge p. 126. Such a large significance level is never used in practice! See Wooldridge p. 130. The number of degrees of freedom of the unrestricted model is given by \\(n-k-1\\) where \\(n\\) is the number of observations, \\(k\\) is the number of independent variables and \\(1\\) stands for the coefficient of the intercept. "],["chapter9.html", "Chapter 14 Econometrics 3", " Chapter 14 Econometrics 3 In this brief exercise we are going to estimate a Keynesian aggregate consumption function for the United States from 1960 to 2019. We will regress private consumption (at constant prices) on real GDP. The slope of the regression will be what in the economic literature is called the marginal propensity to consume (MPC for short). What is MPC? MPC is that value that tells us how much consumption will increase as income increases. Theory tells us that the MPC is a value between 0 and 1. When income increases by 1$, consumption will increase by a value less than the initial increase of 1$ but greater than 0. From our Macroeconomics lessons we remember that the Keynesian consumption function is represented by the following equation, where \\(C\\) indicates the aggregate household consumption, \\(Y\\) the national income (both expressed in billions of constant dollars), \\(c_Y\\) is the MPC and \\(c_{aut}\\), autonomous consumption, i.e. the part of consumption that does not depend on income. \\[C = c_{aut} + c_Y Y\\] We first get the data using the rdbnomics package and then we will produce a graph of the time series from 1960 to 2019 with the ggplot2 package. Both time series are expressed in billions of 2015 dollars. We are now ready to perform the linear regression. We will also produce the scatter plot of the data to get the graphical intuition of the regression. From the linear regression, we obtained the MPC. If income increases by 1$, consumption will increase by approximately 70 cents on average. Once the MPC is obtained, we can calculate the value of the Keynesian multiplier (\\(m\\)) as learned in Macroeconomics classes. \\[m = \\frac{1}{1 - c_Y} = \\frac{1}{1 - 0.7} = 3.33\\] However, this regression has a number of problems from an econometric perspective.19 The two series have a tendency to grow over time and the variables might seem highly correlated to us because they both have the same tendency to increase with time. This could lead to the conclusion that there is a correlation when in fact there is not (not the case here). This problem is known as spurious regression. We need to see if the independent variable is correlated with the error term. This is certainly an undesirable behavior. The residuals must be completely random and contain no predictive power. We can observe a clear pattern between residuals and income. Our coefficients are biased. If we were in a cross-sectional context, we would have to figure out which variable is missing from the model and where possible include it. Or it could be that the model is misspecified. To get to the point, econometricians would say that our two series follow a unit root process. The series are highly persistent over time and contain a, in our case positive, time trend.20 Wanting to simplify a lot we can say that our time series are not stationary and must be transformed before being used in a regression. A quick fix that works is to use logarithmic differences.21 We can see that the estimate in the slope has changed slightly. This time it is somewhat higher at 0.77. However, the interpretation of the model has changed. This time we estimated an elasticity. How can we calculate the MPC having estimated the elasticity of consumption with respect to income? From courses in Microeconomics we remind that elasticity is nothing more than the ratio of two percentage rates and that on the demand curve elasticity varies depending on where you measure it.22 In our case, our elasticity parameter (just called \\(\\epsilon_Y\\) for simplicity) is given by: \\[ \\epsilon_Y = \\frac{\\frac{\\partial C}{C}}{\\frac{\\partial Y}{Y}} = \\frac{\\partial C}{\\partial Y} \\frac{Y}{C}\\] Rearranging the terms, we get: \\[\\frac{\\partial C}{\\partial Y} = \\epsilon_Y \\frac{C}{Y} \\] The elasticity parameter is equal to 0.77 while the C/Y term that we will calculate as the average over the entire period from 1960 to 2019 is equal to 0.64. The marginal effect, the MPC, calculated on the avare over the entire period is therefore 0.4928. How do the residuals perform this time? There are actually a whole host of econometric issues that we have left out but that need to be properly addressed when estimating a model. Is there serial correlation in the residuals and if so what is the consequence? Is there heteroschedasticity in the residuals and if so what problems could it cause? There are tests appropriately developed by statisticians to identify these problems and solutions to fix them. All of these things will be the subject of the course in the following semester. The value of \\(R^2\\) almost equal to one must immediately raise doubts. The two concepts, trending behaviour and persistent behaviour, should not be confused. Please refer to the Wooldridge textbook (Chapter 11). Differencing will removes the linear trend. For a review of the elasticity concept in economics, see here. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
